<!doctype html><html><title>Search - wiki</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name=apple-mobile-web-app-capable content="yes"><meta name=description content="Content under 'Search'."><link rel=stylesheet href=https://wiki.cont.run/css/main.min.479810c3a6a3fbb1591e386f69d36401f3871ceb0a3424a7e58bcdaeeb192f5d.css><body><header><a href=../ id=logo><svg id="Capa_1" enable-background="new 0 0 511.992 511.992" height="512" viewBox="0 0 511.992 511.992" width="512" xmlns="http://www.w3.org/2000/svg"><g><g><g><path d="m256 420.826c0 38.345-11.844 68.545-49.991 68.014-27.744-.385-51.481-15.31-61.853-39.46-1.239-2.887-4.024-4.734-7.154-4.725-.07.0-.135.0-.201.0-47.474.0-75.537-26.171-75.537-73.882.0-5.633.542-11.138 1.568-16.468.62-3.229-.825-6.489-3.671-8.125C23.668 325.748 10 300.053 10 255.997c0-44.057 13.668-69.757 49.161-90.185 2.846-1.636 4.291-4.896 3.671-8.13-1.026-5.33-1.568-10.83-1.568-16.463.0-47.711 28.064-73.882 75.537-73.882h.201c3.13.009 5.915-1.837 7.154-4.729 10.372-24.145 34.109-39.069 61.853-39.455C244.159 22.621 256 52.821 256 91.166" fill="#ff9eb1"/></g><g><g><g><path d="m256 91.166c0-38.344 11.844-68.545 49.991-68.014 27.744.385 51.481 15.31 61.853 39.46 1.239 2.887 4.024 4.734 7.154 4.724h.201c47.474.0 75.537 26.171 75.537 73.882.0 5.633-.542 11.138-1.568 16.468-.62 3.229.825 6.489 3.671 8.125C488.332 186.245 502 211.939 502 255.996s-13.668 69.756-49.161 90.185c-2.846 1.636-4.291 4.896-3.671 8.13 1.026 5.33 1.568 10.83 1.568 16.463.0 47.711-28.064 73.882-75.537 73.882h-.201c-3.13-.009-5.915 1.837-7.154 4.729-10.372 24.145-34.109 39.069-61.853 39.455-38.15.531-49.991-29.669-49.991-68.014" fill="#ff7d97"/></g></g><g><g><path d="m502 265.996c-4.193.0-7.984-2.713-9.407-6.636-1.419-3.912-.16-8.459 3.063-11.092 3.291-2.689 8.009-2.99 11.621-.758 3.568 2.205 5.404 6.578 4.478 10.669-1.02 4.501-5.126 7.817-9.755 7.817z"/></g></g></g><g><path d="m340.83 229.18h-58.013v-58.014h-53.634v58.014H171.17v53.633h58.013v58.013h53.634v-58.013h58.013z" fill="#faf7f5"/></g></g><g><g><path d="m498.468 291.859c-5.141-2.02-10.945.508-12.965 5.648-6.442 16.389-18.055 28.727-37.649 40.005-6.513 3.746-9.932 11.253-8.505 18.689.921 4.783 1.388 9.686 1.388 14.572.0 41.792-22.662 63.882-65.537 63.882h-.225c-.938.0-1.864.074-2.771.217-15.031-4.92-23.796-20.93-19.661-36.479 1.42-5.337-1.757-10.815-7.094-12.234-5.333-1.418-10.814 1.756-12.234 7.094-5.958 22.405 4.241 45.396 23.384 56.443-9.583 17.791-28.602 28.836-50.748 29.145-11.303.146-19.802-2.743-26.011-8.867-9.184-9.057-13.84-25.592-13.84-49.148v-70h16.816c5.522.0 10-4.477 10-10v-48.014h48.014c5.522.0 10-4.477 10-10V229.18c0-5.523-4.478-10-10-10h-48.014v-48.014c0-5.523-4.478-10-10-10H266v-70c0-23.555 4.657-40.09 13.841-49.148 6.21-6.123 14.696-9.022 26.011-8.867 23.862.332 44.096 13.132 52.803 33.405.218.509.458 1.003.719 1.483-3.225 8.243-9.084 15.093-16.833 19.574-8.993 5.2-19.465 6.581-29.485 3.89-5.337-1.434-10.819 1.73-12.251 7.064-1.433 5.334 1.729 10.819 7.063 12.252 5.074 1.363 10.221 2.037 15.338 2.037 10.2.0 20.272-2.681 29.346-7.928 11.083-6.408 19.607-16.017 24.616-27.575 41.6.67 63.569 22.718 63.569 63.866.0 4.89-.467 9.794-1.389 14.582-1.427 7.426 1.991 14.933 8.502 18.678 19.572 11.267 31.176 23.584 37.625 39.936 1.552 3.934 5.318 6.334 9.306 6.333 1.221.0 2.462-.225 3.666-.7 5.138-2.026 7.66-7.834 5.634-12.972-7.943-20.141-22.201-35.773-44.801-49.086.967-5.521 1.457-11.155 1.457-16.772.0-52.114-31.48-83.391-84.288-83.876-12.157-26.863-38.963-43.753-70.318-44.189-16.67-.232-30.255 4.688-40.332 14.625-3.813 3.76-7.08 8.226-9.797 13.382-2.717-5.156-5.984-9.622-9.797-13.382-10.075-9.937-23.668-14.852-40.333-14.625-31.353.437-58.158 17.325-70.32 44.189-52.807.485-84.286 31.762-84.286 83.876.0 5.617.49 11.253 1.458 16.771-37.422 22.031-52.724 50.552-52.724 98.008.0 47.451 15.299 75.969 52.721 98.006-.967 5.521-1.457 11.154-1.457 16.772.0 52.114 31.48 83.391 84.288 83.876 12.157 26.863 38.963 43.753 70.318 44.189.377.005.751.008 1.125.008 16.172.0 29.358-4.92 39.207-14.632 3.813-3.76 7.08-8.226 9.797-13.382 2.717 5.156 5.984 9.622 9.797 13.382 9.849 9.713 23.034 14.633 39.208 14.632.373.0.749-.003 1.125-.008 31.352-.436 58.158-17.325 70.32-44.189 52.807-.485 84.286-31.762 84.286-83.876.0-5.617-.49-11.253-1.458-16.772 22.634-13.329 36.901-28.989 44.838-49.178 2.022-5.14-.507-10.945-5.647-12.966zM282.816 239.18h48.014v33.633h-48.014c-5.522.0-10 4.477-10 10v48.014h-33.633v-48.014c0-5.523-4.477-10-10-10H181.17V239.18h48.014c5.523.0 10-4.477 10-10v-48.014h33.633v48.014c-.001 5.523 4.477 10 9.999 10zm-50.657 230.794c-6.21 6.124-14.717 9.018-26.011 8.867-23.862-.331-44.096-13.132-52.803-33.405-.218-.509-.458-1.003-.719-1.483 3.225-8.243 9.085-15.093 16.833-19.574 8.992-5.2 19.463-6.581 29.485-3.89 5.337 1.434 10.819-1.73 12.251-7.064 1.433-5.333-1.73-10.819-7.064-12.251-15.188-4.08-31.059-1.988-44.684 5.891-11.083 6.408-19.607 16.017-24.616 27.575-41.6-.67-63.569-22.718-63.569-63.866.0-4.89.467-9.794 1.389-14.582 1.427-7.427-1.991-14.934-8.502-18.678-32.183-18.528-44.149-40.621-44.149-81.517.0-40.9 11.966-62.994 44.146-81.517 6.513-3.746 9.932-11.253 8.505-18.689-.921-4.783-1.388-9.686-1.388-14.572.0-41.792 22.662-63.882 65.537-63.882h.225c.938.0 1.864-.074 2.771-.217 15.031 4.92 23.796 20.93 19.661 36.479-1.42 5.337 1.757 10.815 7.094 12.234.861.229 1.726.338 2.577.338 4.422.0 8.467-2.956 9.657-7.432 5.958-22.405-4.241-45.396-23.384-56.443 9.583-17.791 28.602-28.836 50.748-29.145 11.267-.15 19.801 2.743 26.011 8.867 9.184 9.057 13.84 25.592 13.84 49.148v70h-16.816c-5.522.0-10 4.477-10 10v48.014H171.17c-5.522.0-10 4.477-10 10v53.633c0 5.523 4.478 10 10 10h48.014v48.014c0 5.523 4.478 10 10 10H246v70c0 23.554-4.657 40.09-13.841 49.147z"/><path d="m139.699 228.227c-6.766.0-13.186 1.514-18.907 4.31-3.049-8.65-8.286-16.485-15.336-22.673-4.151-3.643-10.469-3.233-14.113.918-3.643 4.15-3.232 10.469.918 14.112 6.711 5.891 10.816 14.143 11.498 22.953-1.213 1.914-2.293 3.946-3.225 6.088-1.145 2.633-2.015 5.316-2.615 8.019-13.414 11.422-33.601 10.834-46.225-1.792-3.906-3.904-10.236-3.904-14.143.0-3.905 3.905-3.905 10.237.0 14.143 10.524 10.524 24.354 15.784 38.193 15.784 8.04.0 16.083-1.775 23.484-5.325 1.904 5.443 4.967 10.557 9.136 15.03.596.639 1.204 1.269 1.826 1.891 1.953 1.953 4.512 2.929 7.071 2.929 2.56.0 5.118-.976 7.071-2.929 3.905-3.905 3.905-10.237.0-14.143-.458-.458-.906-.922-1.342-1.389-6.26-6.716-7.799-15.778-4.118-24.241 2.878-6.616 9.86-13.686 20.826-13.686 5.522.0 10-4.477 10-10 .001-5.522-4.476-9.999-9.999-9.999z"/><path d="m387.667 287.543c-3.905 3.905-3.905 10.237.0 14.143 1.953 1.953 4.512 2.929 7.071 2.929s5.118-.976 7.071-2.929c.622-.622 1.23-1.253 1.83-1.896 4.167-4.471 7.229-9.583 9.133-15.025 7.401 3.549 15.444 5.324 23.484 5.324 13.839.0 27.67-5.261 38.193-15.784 3.905-3.905 3.905-10.237.0-14.143-3.906-3.904-10.236-3.904-14.143.0-12.624 12.625-32.811 13.214-46.225 1.792-.6-2.702-1.47-5.386-2.615-8.019-.932-2.142-2.012-4.175-3.225-6.088.682-8.81 4.787-17.062 11.498-22.953 4.15-3.644 4.561-9.962.918-14.112-3.646-4.151-9.964-4.563-14.113-.918-7.05 6.189-12.287 14.023-15.336 22.673-5.721-2.796-12.141-4.31-18.907-4.31-5.523.0-10 4.477-10 10s4.477 10 10 10c10.966.0 17.948 7.07 20.826 13.686 3.681 8.463 2.142 17.525-4.114 24.237-.44.47-.888.935-1.346 1.393z"/></g></g></g></svg></a><h3 class=site-title>wiki</h3><form id=search action=https://wiki.cont.run/search/ method=get><label hidden for=search-input>Search site</label>
<input type=text id=search-input name=query placeholder="Type here to search">
<input type=submit value=search></form></header><div class=grid-container><div class=grid><div class="page wide"><ul id=results><li>Enter a keyword above to search this site.</li></ul></div></div></div><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script><script>window.store={"https:\/\/wiki.cont.run\/":{title:"",tags:[],content:"Table of Contents ",url:"https://wiki.cont.run/"},"https:\/\/wiki.cont.run\/search\/":{title:"Search",tags:[],content:"",url:"https://wiki.cont.run/search/"},"https:\/\/wiki.cont.run\/categories\/":{title:"Categories",tags:[],content:"",url:"https://wiki.cont.run/categories/"},"https:\/\/wiki.cont.run\/chrome-download-shelf\/":{title:"Chrome download shelf",tags:[],content:"Chrome download shelf is the widget shown in the buttom when you initiate a new download. This document summarizes how the download shelf is implemented. It is sketchy on the details. For more information, click on the source code or documentation links.\nHow to inspect chrome browser ui components In short, run chrome with chromium --enable-ui-devtools=1234 and open url chrome://inspect/#native-ui. See also UI DevTools Overview and Chromium Desktop UI Debugging Tools and Tips.\nLayout in browser window How to layout a new component The elements of chromium UI are views. A View is a UI element, similar to an HTML DOM element. In order to create new UI componenets, we need to create corresponding views. Take download shelf as an example, the entrypoint to register new browser views is browser_view_layout.cc. Here is how chromium layout download shelf in the main window, where the download_shelf_ is a view created here.\nDownload shelf Implementations There are currently two implementations for the download shelf. One is implemented in c++ with MVC architecture, while the other is implemented with webui using web technologies.\nMVC Architecture Chromium uses the well-known MVC design pattern to layout UI components, although sometimes model, view and controller are not strictly separated.\nHere is a diagram which illustrates how mvc architecture works.\n The download shelf view is the UI that is visiable to the user. The user may interact with the download shelf, which then leverages the controller functions to update the download items.\nFor example, when the user click on the context menu (shown by DownloadShelfWebView::ShowDownloadContextMenu) of a download item on the download shelf. The download shelf view executes this download command by calling DownloadCommands::ExecuteCommand, which in turn calls DownloadUIModel::ExecuteCommand of the DownloadUIModel.\nWhen the download items are modified, the views may be updated according by the controllers. The download item models notify controller for updates using the observer design pattern.\nFor example, DownloadUIController observes download item modifications, the notifications are sent by the download manager core services (DownloadNotificationManager). When a new download item is ready, download core service calls DownloadNotificationManager::OnNewDownloadReady, which then calls DownloadShelfUIControllerDelegate::OnNewDownloadReady, which in turn calls browser-\u0026gt;window()-\u0026gt;GetDownloadShelf()-\u0026gt;AddDownload(std::move(model)) to make the download item show up on the download shelf.\nDownload items There are three data models for the download items.\nDownload UI model DownloadUIModel is an interface to define methods to operate on download items (e.g. pasue, resume downloads), show download item status (e.g. saved file name, downloading status).\nDownload item model DownloadItemModel is just a wrapper around download::DownloadItem, which implements the download UI model interface.\nDownload item download::DownloadItem is the underlying download item implementation. It defines what represents a download item, how to serialize download items, how to store their states, etc.\nMenu and commands To show a simple context menu, we need to inherit ui::SimpleMenuModel::Delegate. A SimpleMenuModel::Delegate subclass needs to define some commands and their actions. Commands correspond to menu items. When a menu item is clicked, the corresponding action is executed.\nWebui download shelf “WebUI” is a term used to loosely describe parts of Chrome\u0026rsquo;s UI implemented with web technologies (i.e. HTML, CSS, JavaScript).\nCommunication Between the Browser and WebUI web page Bi-direction communication by mojo (a high level IPC abstraction).\nData definition and RPC The data structures and rpc interfaces are defined in download_shelf.mojom. It contains what structures represent a download when passing from browser to webpage, and vice versa, and what remote procedure calling does the browser provide. Note that the download items defined here is different from download::DownloadItem. We may need to convert from one to another.\nBrowser side handler Browser side handlers are normally called PageHandler. The download shelf PageHandler defines methods to accept rpc request from the website, execute the relevant methods, and return the results. Examples are opening the downloaded item, removing a download. The interface is here. It is implemented here in the browser. The method calls are delegated to the underlying download models. The webui client calls these methods by the generated mojo apis.\nWeb page side handler Web page side handlers are normally called Handler. They are used by the web page to handled native RPC requests from the browser. When the browser did something, it may want to notify the browser for its effects, e.g. there is a new download, the webui may want to show it on its interface. Here are the download shelf web page handlers. There are implmented in javascript here.\nUI embedding The webui have low barriers to entry, but it may lack some desirable effects. For example, we may need to show a native menu. We need to define a UI embedder to show native context menu. The download shelf define a DownloadShelfUIEmbedder here. When the user right clicks on a download item, he actually sees the context menu of the DownloadShelfContextMenuView which is able to display native context menu.\nWebUI registration To register a chrome:// url for a webui, we need to register it in the GetWebUIFactoryFunction. Here is how download shelf is registered.\n",url:"https://wiki.cont.run/chrome-download-shelf/"},"https:\/\/wiki.cont.run\/identity-aware-proxy\/":{title:"identity-aware proxy",tags:[],content:"An identity-aware proxy is a reverse proxy which forwards normal payload and attaches the identity information to the backend servers.\nIdentity-aware proxy is an essential part of Google\u0026rsquo;s beyondcorp.\nBeyondcorp To me, there\u0026rsquo;re three key ingredients for beyondcorp.\n Collecting all collectible information about the device and user. Proxying all traffic through an access proxy, instead of setting up a trusted perimeter. Specifying security policy through a unified configuration center and a powerful dsl (cel).  How to obtain identity information? This is implementation-specific. For http, there is a more or less generic method to obtain identity information, i.e. nginx http_auth_request_module and its equivalents. For other protocols, there is no such thing?\nHow to pass identity information?   HTTP is simple. We can easily attach any metadata in the HTTP world just by adding additional header. How do we do that securely? Http header is easy to fabricate. Luckily, there is a standard way to do this, json web token (jwt). JWT consists of three parts, metadata, payload and signature. The payload can be any valid json data. The integrity is ensured by the signature, which can be verified with json web keys (jwks).\n  Life is harder for other tcp/udp services. We have mutual ssl authentication. All we need is a common, trusted certificate authority, and certificates signed by this CA is trusted by all services. We attach the necessary metadata to the certificates.\n  What about L3 solutions? In the brave new Kubernetes world, we have different ip addresses for different pods (using services are more appropriate), i.e. we can identify pods with the ip addresses assigned to them. Some container network interface (cni) (like cilium) implementer did implement identity-awareness in the ip level. I expect they are implemented with something like tailscale to do network traffic, with a full-featured control plane. One thing I am curious is that how do they pass the identity information. Applications normally do interact with stuff that low level.\n  Some implementations Teleport Let\u0026rsquo;s have a look at the architecture of teleport, an open source unified access plane which supports a wide range of applications, including kubernetes, postgresql and http applications.\n  Initiate Client Connection Authenticate Client Connect to Node Authorize Client Access to Node  A notable difference is that instead of connecting directly to the backend service. Teleport has a pool of nodes which acts on the user\u0026rsquo;s behaviour to connect to backend servers. This has a few benefits.\n This evades the problem of backend services not being directly routable from teleport proxy. Loosing coupling between proxy and backend services. Different kind of backend services have different kind of nodes, which can have different kind of auditing logic.  Also note teleport proxy does not push information directly to node service. Node service actively pull information from teleport auth.\nTODO Pomerium TODO Cilium ",url:"https://wiki.cont.run/identity-aware-proxy/"},"https:\/\/wiki.cont.run\/kernel-development-with-nix\/":{title:"kernel development with nix",tags:[],content:"See also Kernel Debugging with QEMU.\nThis nix file should be placed in the root directory of kernel source code.\n{ system ? builtins.currentSystem, configuration ? null , nixpkgs ? import \u0026lt;nixpkgs\u0026gt; { }, extraConfigFile ? \u0026#34;config\u0026#34;, ... }@args: with nixpkgs.pkgs; let buildLinuxArgs = builtins.removeAttrs args [ \u0026#34;system\u0026#34; \u0026#34;configuration\u0026#34; \u0026#34;nixpkgs\u0026#34; \u0026#34;extraConfigFile\u0026#34; ]; makeKernelVersion = src: stdenvNoCC.mkDerivation { name = \u0026#34;my-kernel-version\u0026#34;; inherit src; phases = \u0026#34;installPhase\u0026#34;; # make kernelversion also works. installPhase = \u0026#39;\u0026#39; set -x s=\u0026#34;$(\u0026lt; \u0026#34;$src/Makefile\u0026#34;)\u0026#34; get() { awk \u0026#34;/^$1 = / \u0026#34;\u0026#39;{print $3}\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$s\u0026#34; } printf \u0026#39;%s.%s.%s%s\u0026#39; \u0026#34;$(get VERSION)\u0026#34; \u0026#34;$(get PATCHLEVEL)\u0026#34; \u0026#34;$(get SUBLEVEL)\u0026#34; \u0026#34;$(get EXTRAVERSION)\u0026#34; | tee $out \u0026#39;\u0026#39;; }; getKernelVersion = src: builtins.readFile \u0026#34;${makeKernelVersion src}\u0026#34;; kernelSrc = let filter = name: type: let baseName = builtins.baseNameOf (builtins.toString name); in lib.cleanSourceFilter name type \u0026amp;\u0026amp; !(baseName == \u0026#34;.ccls-cache\u0026#34; || baseName == extraConfigFile || lib.hasSuffix \u0026#34;.nix\u0026#34; baseName); in lib.cleanSourceWith { inherit filter; src = ./.; }; kernelVersion = getKernelVersion kernelSrc; latestConfigFile = linuxPackages_latest.kernel.configfile; defaultConfigFile = (linuxConfig { src = kernelSrc; version = kernelVersion; }).overrideAttrs ({ prePatch ? \u0026#34;\u0026#34;, ... }: { prePatch = linuxPackages_latest.kernel.prePatch + prePatch; }); # We need to merge some `CONFIG_` to make qemu happy. allConfigFiles = let p = \u0026#34;${builtins.toPath ./.}/${extraConfigFile}\u0026#34;; extraConfig = lib.optionals (builtins.pathExists p) [ \u0026#34;${builtins.path { name = \u0026#34;extra-kernel-config\u0026#34;; path = p; }}\u0026#34; ]; in [ defaultConfigFile latestConfigFile ] ++ extraConfig; mergedConfigFile = (stdenv.mkDerivation { name = \u0026#34;merged-kernel-config\u0026#34;; src = kernelSrc; phases = \u0026#34;unpackPhase prePatchPhase installPhase\u0026#34;; prePatchPhase = linuxPackages_latest.kernel.prePatch; # make qemu happy with `CONFIG_EXPERIMENTAL=y`. installPhase = \u0026#39;\u0026#39; set -x KCONFIG_CONFIG=$out RUNMAKE=false \u0026#34;$src/scripts/kconfig/merge_config.sh\u0026#34; ${ builtins.concatStringsSep \u0026#34; \u0026#34; allConfigFiles }grep -q \u0026#39;^CONFIG_EXPERIMENTAL=\u0026#39; $out \u0026amp;\u0026amp; sed -i \u0026#39;s/^CONFIG_EXPERIMENTAL=.*/CONFIG_EXPERIMENTAL=y/\u0026#39; $out || echo \u0026#39;CONFIG_EXPERIMENTAL=y\u0026#39; \u0026gt;\u0026gt; $out \u0026#39;\u0026#39;; }).overrideAttrs ({ prePatch ? \u0026#34;\u0026#34;, ... }: { prePatch = linuxPackages_latest.kernel.prePatch + prePatch; }); nixosConfiguration = { config, pkgs, ... }: { imports = [ ] ++ lib.optionals (configuration != null) [ configuration ]; boot.kernelPackages = # TODO: Does not work yet. # buildLinux { # src = kernelSrc; # version = kernelVersion; # }; linuxPackages_custom { src = kernelSrc; version = kernelVersion; configfile = \u0026#34;${mergedConfigFile}\u0026#34;; }; environment = { enableDebugInfo = true; etc = let getHome = x: builtins.elemAt (builtins.split \u0026#34;:\u0026#34; x) 10; entries = builtins.filter (x: x != \u0026#34;\u0026#34; \u0026amp;\u0026amp; x != [ ]) (builtins.split \u0026#34;\\n\u0026#34; (builtins.readFile /etc/passwd)); homes = builtins.map getHome entries; currentFile = \u0026#34;${builtins.toPath ./.}\u0026#34;; possibleUserHomes = builtins.filter (x: lib.hasPrefix x currentFile) homes; keyFiles = builtins.filter (x: builtins.pathExists x) (builtins.map (x: \u0026#34;${x}/.ssh/authorized_keys\u0026#34;) possibleUserHomes); keys = builtins.concatStringsSep \u0026#34;\\n\u0026#34; (builtins.map (x: builtins.readFile x) keyFiles); in lib.optionalAttrs (keys != \u0026#34;\u0026#34;) { \u0026#34;ssh/authorized_keys.d/root\u0026#34; = { text = builtins.trace \u0026#34;Added the following keys for ssh access.\\n${keys}\\n\u0026#34; keys; mode = \u0026#34;0444\u0026#34;; }; }; }; # Generate with mkpasswd -m sha-512 pwFuerRoot users.users.root.initialHashedPassword = \u0026#34;$6$3zAawH1uhs$dlOiT.ckvpbBQ21tax1J4RI1EGm/1j1HDBoe5u1jy.gHw0QXCKA1dVEwKF.LD0bvzqBu4co.eaZCIK7b2E17k1\u0026#34;; services.sshd.enable = true; }; nixos = import (nixpkgs.path + \u0026#34;/nixos/\u0026#34;) { inherit system; configuration = nixosConfiguration; }; in nixos // { inherit allConfigFiles defaultConfigFile latestConfigFile mergedConfigFile kernelSrc; } To build a vm image, run it and connect to it with ssh\nnix-build . -A vmWithBootLoader QEMU_OPTS=-nographic QEMU_NET_OPTS=\u0026#34;hostfwd=tcp::2222-:22\u0026#34; ./result/bin/run-*-vm # password is pwFuerRoot ssh -p 2222 root@localhost TODO:\n [] Incremental build  ",url:"https://wiki.cont.run/kernel-development-with-nix/"},"https:\/\/wiki.cont.run\/posts\/":{title:"Posts",tags:[],content:"",url:"https://wiki.cont.run/posts/"},"https:\/\/wiki.cont.run\/quine-in-haskell\/":{title:"quine in haskell",tags:[],content:"See How to make compressed file quines, step by step\n#!/usr/bin/env stack -- stack --resolver lts-15.01 script module Main where import Data.List (intercalate) main :: IO () main = putStr $ (unlines prefix) ++ (getPrefixDef prefix) where getPrefixDef list = (\u0026#34; prefix =\\n[\\n\u0026#34;) ++ (intercalate \u0026#34;,\\n\u0026#34; (map show list)) ++ \u0026#34;\\n]\u0026#34; prefix = [ \u0026#34;#!/usr/bin/env stack\u0026#34;, \u0026#34;-- stack --resolver lts-15.01 script\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;module Main where\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;import Data.List (intercalate)\u0026#34;, \u0026#34;\u0026#34;, \u0026#34;main :: IO ()\u0026#34;, \u0026#34;main = putStr $ (unlines prefix) ++ (getPrefixDef prefix) where\u0026#34;, \u0026#34; getPrefixDef list = (\\\u0026#34;prefix =\\\\n [\\\\n \\\u0026#34;) ++ (intercalate \\\u0026#34;,\\\\n \\\u0026#34;(map show list)) ++ \\\u0026#34;\\\\n ]\\\u0026#34;\u0026#34; ] ",url:"https://wiki.cont.run/quine-in-haskell/"},"https:\/\/wiki.cont.run\/self-hosted-infrastructure\/":{title:"self-hosted infrastructure",tags:[],content:"TLDR: I use tailscale/zerotier to establish a smallish mesh network. I use envoy as an edge router to forward L4 traffic. I mainly provision and manage services with nix, docker and sops. When it is absolutely required, I use k3s to deploy Kubernetes services. Traefik is used for routing, and authelia is used for blocking unauthorized access. To multiplexing protocols with a single port, I use aioproxy. I use restic to back up my inevitably accumulated state. For CI/CD, I use github actions, hashicorp vault, depoly-rs and cachix. I use the grafana stack (prometheus, AlertManager, grafana and loki) for observablility.\nPrinciples My principles can be best described as cloud nativeness. Cloud-native is an all-encompassing and vague term. I have a few concrete points on my mind.\n Software-defined everything Declarative Infrastructure as code Minimal state maintenance Self-organization Single source of truth  Networking The first obstacle to self-host everything is that you don\u0026rsquo;t have a stable public accessible IP. There are a few solutions.\nThe cloud  I am paranoid enough to not trust the cloud, aka other people\u0026rsquo;s computer. This approach is not cost-efficient. Even my Raspberry PI can beat many VPSes in terms of computing power. Not to mention I can easily insert a 256G SD card. Locality. There is no place like LAN. I see no benefit in downloading youtube video to another VPS.  DDNS This is simplest. I don\u0026rsquo;t use this mainly because it is not reliable in my setup. To name a few problems of DDNS,\n 80, 443, 8080 blocked Not portable router configurations. You need to set up port mapping or DMZ host in your router, which is hard to codify, if not impossible CGNAT ipv6 is still yet to come  Port Forwarding There are many port-forwarding software. To name a few, autossh (my favorite), ngrok, frp, nps. You may also combine DDNS with port forwarding of your router. Here was my attempt to do this. The biggest problem of port forwarding is that it is not scalable and there is no generic inter-node connectivity. Port forwarding has the following weaknesses.\n star topology, single point of failure no inter-node connectivity number of ports are limited, you only have one 443 hard to set up (authorization) no hairpinning support most port forwarding only supports TCP  Overlay Networks Overlay networks are magic. What I meant is not container network interface kind of overlay network, but solutions like zerotier, tailscale, innernet, nebula, n2n. There all have interesting aspects. But none of they are self-organizing. They all require a centralized coordination server. What I have on my mind is something like matrix pinecone. I have been thinking on implementing a pinecone-like overlay network for a while, self-organizing, and tunneling traffic with libp2p. I currently rely on tailscale and zerotier to establish peer-to-peer connectivity. This works great in the following perspectives.\n inter-node connectivity all ports are belong to you easy to set up (implementation-dependent) transparent hole punching transparent multi-path  Routing L3 L3 routing is provided by the overlay network solutions.\nL4 Considerations For L4 routing, I care about transparency, protocol multiplexing and configuration-complexity.\n   Transparency\nThis means that the backend service does not need to know there is a middle man do the heavy lifting. In particular, it means that the origin requester\u0026rsquo;s address is preserved. Typical HTTP reverse proxies are not transparent. They pass the original requester\u0026rsquo;s information by injecting an X-Forwarded-For header.\n     Protocol Multiplexing\nL4 protocol multiplexing means that we can use the same TCP port for HTTP, TlS and SSH. An example is sslh. It normally works by peeking into a few first bytes and determine which protocol this packet is, and then handing off the connection to another application which is listening on some other port.\n     Configuration Complexity\nDo we have to configure both the proxy and backend services? What if we change a user-fronting proxy address? Do the backend server need to adjust for this change? Any special configuration for different user-fronting proxies? What if an upstream server is down? Must I manually edit the configuration to reflect this change?\n  Solutions    iptables\nThis is just like NAT. It is transparent. I believe you can multiplex port with some iptables extensions. It is not super pretty. A lethal problem is that the user-fronting proxy must be in the return path of the connection (usually the proxy is the default gateway). To circumvent this problem, we need some modifications to the routing table and routing policies. When there are two proxies which are connected to the same interface, there are multiple return paths, to select the correct one, we need policy based routing.\n     ipvs\nCompared with iptables, ipvs is much more manageable and scalable. Yet it still is too complicated.\n     usespace L4 proxy\nenvoy/haproxy/nginx etc. can be used as L4 proxy. They accept incoming downstream connection and establish a new upstream connection, just like a pipe. This is much more manageable, the downside is that the original client\u0026rsquo;s information is lost in translation. To ease this problem, haproxy designed a protocol called PROXY (I can haz a more searchable name?). In short, it appends original request\u0026rsquo;s source and destination addresses to the TCP connection or UDP stream. As stated in the above document, this will solve the multiple return paths because we are initiating another TCP connection/UDP stream. Unfortunately, this solution is invasive as it requires the backend service to support PROXY protocol explicitly. Fortunately we have mmproxy. It accepts PROXY protocol packets, unwraps them and then forwards them to upstream. Moreover, it does so transparently. The original mmproxy does not support UDP, while this go implementation go-mmproxy supports.\n     aioproxy\nmmproxy is great when working with envoy. But it does not multiplex port like sslh, is not transparent, and does not work with non-PROXY protocol traffic. Non-transparent proxy is useful when we are trying to proxy a connection whose original requester, proxy and the backend server are all the same host (see below).\n   How transparent proxy works\nLet cip be the client ip, pip be the proxy ip and sip be the backend server IP.\n Client connection: cip:45678 -\u0026gt; sip:22, client tries to connect to sip:22, but it actually connects to transparent proxy Transparent proxy downstream connection: cip:45678 -\u0026gt; pip:44443, transparent proxy accepts traffic from cip:45678, the traffic originally targeted sip:22 is redirected to pip:44443 by netfilter. Transparent proxy upstream connection: pip:45678 -\u0026gt; sip:22, transparent proxy establish a new connection to sip:22, it changes the socket source address to cip:45678 with the help of IP_TRANSPARENT. Backend server connection: cip:45678 -\u0026gt; sip:22, backend server is fooled by the connection socket address, this connection is actually started from the transparent proxy. If the transparent proxy stands right in the middle of the return path from the backend server to the client, then the proxy can get the return packet from its upstream connection and send it to the client on behalf of backend server by its downstream connection.       What could go wrong when client and transparent proxy are on the same host\nIf client and transparent proxy are on the same host 127.0.0.1, both of them will try to bind 127.0.0.1:45678, which would fail with Address Already in Use.\n     What could go wrong when we chain more than one transparent proxy\nOn the other hand, if we use the scheme client \u0026lt;-\u0026gt; envoy \u0026lt;-\u0026gt; mmproxy \u0026lt;-\u0026gt; sslh \u0026lt;-\u0026gt; ssh, and when both mmproxy and sslh are configured to proxy transparently, the same bind error would occur (I have not tried it, I expect it to fail).\nSo it is sometimes useful to proxy non-transparently, and it would be great if we can have an all-in-one proxy which can intelligently unwrap PROXY protocol traffic (when it fails to do so, just treats it as normal traffic and forwards it), supports transparent proxy to upstream and multiplexes port for different protocols.\nHere is my take on this problem. Aioproxy has rudimentary solutions for all above problems. There are a few things I intended to add. First, more protocol support for multiplexing. Most outstandingly, peeking into SNI, and forwarding connection accordingly. Second, as discussed above, it could go wrong when client and transparent proxy is on the same host. We need intelligent transparent forwarding, i.e. when client and transparent proxy is on the same host, do not use the same client address tuple. At this point, the aioproxy is abandoned in favor of caddy-l4. Caddy-l4 is not mature enough currently, but it has much greater potential, as we can use anything caddy already provided.\n       envoy+traefik+aioproxy\nThis is my current setup. Envoy, traefik and aioproxy are a great match. Client connection to my edge proxy is wrapped with PROXY protocol by envoy and forwarded to traefik. Depending on the packet format, traefik would forward it to HTTP traffic to docker or Kubernetes, other TCP traffic to aioproxy (this works by setting SNI to rules to match Host(\u0026quot;*\u0026quot;), see here), the PROXY protocol header is automatically peeled off when possible. It is not transparent to aioproxy. I don\u0026rsquo;t intend to optimize it for now. In fact, it would be better if I insert aioproxy in front of traefik, as this way every service is now ignorant of the proxy. But I didn\u0026rsquo;t implement intelligent transparent proxy mentioned above yet (this is fairly easy, and I am fairly lazy currently). It\u0026rsquo;s now done. There will be some problem when client and transparent proxy are on the same host, which is a frequent user case for me.\n  Intermission: Split Horizon DNS I have a few ways to access my services. When I use my own devices, I can just access my services by overlay networks. My devices are part of the overlay network. I can access services via a stable address within 10.144.0.0/16. Overlay networks are magic. They automatically select paths for me, e.g. when my two devices are in the same network, they connect each other using LAN address, otherwise, they connect each other over WAN. Overlay networks can transparently do NAT-PMP/UPNP, punch holes. When one device is behind an impenetrable NAT, they automatically select a relay. I may want to make part of my services available outside the overlay network. In that case, access to the services is proxied by two public accessible VPSes. They forward traffic as described above. The problem is that my VPSes live in Far Far Away. I don\u0026rsquo;t want to travel around the world when I am in the overlay network. Can my device be intelligent enough to just try the overlay network first, when it fails to do so, use the backup VPSes? This is a well-known problem of split horizon dns. I have a stable domain name service-a.example.com, I want it to be resolved as 10.2.3.4 when I am in the corporate network (or I was using a VPN), otherwise please resolve it to 1.2.3.4. Here is a few solutions. By the way, this is a great read on this problem.\nHosts The easiest and the most abominable solution. The downsides are\n no wildcard support for Windows, Linux no flexibility. You can not graceful fallback to another host or easily add another entry  Nsswitch If you ever use mdns, you may wonder how abc.local resolve to the host abc. The secret sauce lies in the following stanza of /etc/nsswitch.conf.\nhosts: files mdns_minimal [NOTFOUND=return] mymachines resolve [!UNAVAIL=return] dns mdns myhostname Here, mdns_minimal and mymachine are dynamic libraries used by NSS to resolve hosts. They provide the functionality of resolving mdns hosts and machinectl hosts. Theoretically, I can just write another plugin for nsswitch like mdns_minimal, but nsswitch is also an abomination. It is glibc only, thus musl-linked and statically linked binaries would fail. As a matter of fact, supporting mdns on musl is a future idea, while golang fallbacks to glibc to resolve hostname when the hosts entry in nsswitch is too conflicted. So it does not worth the effort to fiddle with nsswitch.\nCoredns I found salvation in coredns. Here is how I resolve a domain name with coredns enriched by coredns-mdns and coredns-alternate. The source code to this coredns instance is here.\n.:5355 { template IN A mydomain.tld { match ^(|[.])(?P\u0026lt;p\u0026gt;.*)\\.(?P\u0026lt;s\u0026gt;(?P\u0026lt;h\u0026gt;.*?)\\.(?P\u0026lt;d\u0026gt;mydomain.tld)[.])$ answer \u0026quot;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.\u0026quot; fallthrough } template IN AAAA mydomain.tld { match ^(|[.])(?P\u0026lt;p\u0026gt;.*)\\.(?P\u0026lt;s\u0026gt;(?P\u0026lt;h\u0026gt;.*?)\\.(?P\u0026lt;d\u0026gt;mydomain.tld)[.])$ answer \u0026quot;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.\u0026quot; fallthrough } mdns mydomain.tld alternate original NXDOMAIN,SERVFAIL,REFUSED . 1.0.0.1 8.8.4.4 9.9.9.9 180.76.76.76 223.5.5.5 } The Corefile above does the following things.\n cname *.hostname.mydomain.tld to hostname.mydomain.tld Let hostname.mydomain.tld be resolved to hostname.local by coredns-mdns Anything not matched or not resolved here is forwarded to real world DNS servers  To resolve hostname.local, I use avahi to announce the workstation hostname. This solution is particular elegant, in the sense that all hosts need only to configure themselves. To use this DNS server for all applications, I configured systemd-resolved here. It is also possible to make other devices in the overlay network to use this DNS server. I haven\u0026rsquo;t done it yet.\nL7 Now that we can resolve domains to desirable hosts, we can access services directly in the browser.\nTLS Certificates and Termination I use acme with dns-chanlledge. My DNS service provider is cloudflare. From letsencrypt, I got free wildcard certificates for *.hostname.mydomain.tld, *.local.mydomain.tld, optionally also some alias domains like *.hub.mydomain.tld. The certificates are obtained by setting NixOS options security.acme, and are shared between multiple applications. Currently, TLS is terminated by traefik using above certificates.\nService and Routing Registration Service and router registration is done in a self-organizing way. I don\u0026rsquo;t use subpath routing rules, as it may require extra work of rewriting paths. Routing is only matched by Host. All my services have dedicated domains. Cloudflare provides wildcard DNS resolution. My coredns configuration above also resolves domain names in a wildcard-matching fashion.\n   Fixed Services and Routings\nGenerated from nix expressions. It is a obligation for me to praise how easily nix (a real programming language, albeit a weak one) eliminates boilerplate. Why is everyone trying to use some half-baked configuration format? Can we have a good language for general configurations? Spoiler alert: dhall-lang.\n     Docker\nThis is managed by traefik with docker provider. All I need to do is add a label to the container. Traefik will automatically pick up the label and set up a routing rule according to the defaultRule. My rule is to use domainprefix label when applicable, otherwise fall back to container name.\nproviders = { docker = { defaultRule = getRule \u0026#39;\u0026#39;{{ (or (index .Labels \u0026#34;domainprefix\u0026#34;) .Name) | normalize }}\u0026#39;\u0026#39;; }; }      Kubernetes\nJust the usual Kubernetes ingress. I passed k3s kubeconfig to traefik by systemd environment variable here. Traefik will automatically apply Kubernetes ingress rules.\n  Deployment I currently use nix to manage all my personal devices, ansible to manage all the cloud resources. Most services are managed by nix. When nix becomes too unwieldy, I resort to Kubernetes. An ideal setup would be using terraform to provision cloud resources, using nix to manage all services including Kubernetes ones. This is currently not possible for me because firstly, many resources I used does not have terraform provider. Secondly, nix currently does not support ad hoc variable assignment like terraform and ansible. It is possible to pass variables from the command line, but it is not pleasant to use. Thirdly, Kubernetes requires a lot of dedication. Currently nix, can\u0026rsquo;t manage Kubernetes efficiently.\nNix Nix is a much more declarative, reliable and reproducible way to build infrastructure. Here is a short introduction. In short, building NixOS profiles is like building docker image. You build a new container image and run a container with that image as base. The container image itself is immutable. When you change your code, you need to build a new image. When you need some new operating system configuration, you build a new NixOS profile and switch to it. The best thing about NixOS is that nearly every aspect of the OS is tunable by NixOS options. The knobs are formed by the purely functional, lazy language nix.\nDocker I manage docker containers declaritively with nix. A typical docker container configuration is\nmkContainer \u0026#34;wallabag\u0026#34; prefs.ociContainers.enableWallabag { dependsOn = [ \u0026#34;postgresql\u0026#34; ]; environment = { \u0026#34;SYMFONY__ENV__DOMAIN_NAME\u0026#34; = \u0026#34;https://${prefs.getFullDomainName \u0026#34;wallabag\u0026#34;}\u0026#34;; }; traefikForwardingPort = 8978; middlewares = [ \u0026#34;authelia\u0026#34; ]; volumes = [ \u0026#34;/var/data/wallabag/data:/var/www/wallabag/data\u0026#34; \u0026#34;/var/data/wallabag/images:/var/www/wallabag/web/assets/images\u0026#34; ]; environmentFiles = [ \u0026#34;/run/secrets/wallabag-env\u0026#34; ]; } mkContainer is a function to make a new container. If prefs.ociContainers.enableWallabag is true, nix would make a container named wallabag which depends on the postgresql container and has such such volumes and such such environment variables. The environmentFiles is also read to set up environment variables. The file /run/secrets/wallabag-env is managed by sops-nix and is version-controlled. I also specified the middleware authelia for traefik, which means that not everyone is allowed to access this service.\nService Discovery This is easy. Docker container within the same bridge network can access each other by the container name.\nConfigmaps and Secrets I use docker command line flag --env and --env-file to pass my configurations as container environment variable. To mount secrets like Kubernetes, I use docker volume. The secrets are managed by sops-nix, which generate secret files according to my sops.yaml file.\nInit Containers and Jobs Kubernetes init containers are sometimes used to manage pods/services dependencies. For this specific use case, init containers are ugly hacks. Using systemd to manage container dependency is much more elegant. I only need to specify dependsOn in my nix file, e.g. dependsOn = [\u0026quot;postgresql\u0026quot;]; above. I override the ExecStartPost option for systemd units to do initialization job. Kubernetes jobs are just more containers, while cronjobs are just containers with systemd timer.\nIngress See routing.\nAnsible As much as I love NixOS, I don\u0026rsquo;t use nix for everything. Nix does not work along with some technologies. I use ansible for two purposes, first setting up cloud resources (like setting up tailscale and envoy), second managing Kubernetes. Kubernetes is declarative, but using command line to manage Kubernetes is imperative. I use community.kubernetes. A pleasant side effect of using ansible to manage Kubernetes is what I did and what I need to do are well-documented.\nKubernetes My Kubernetes distribution is k3s (provisioned by nix). Each Kubernetes cluster includes exactly one node for the time being. There are a few edge cases where I can\u0026rsquo;t simply use nix and docker. Jupyterhub and eclipse che are major ones, as they need to provision cluster resources dynamically, e.g. they need to spawn new containers on user request. This is doable with vanilla docker spawner for jupyter hub. I don\u0026rsquo;t think Che support this natively. Using Kubernetes is much preferable.\nSecurity Authentication and Authorization Setup I use authelia for authentication and authorization. I created an ForwardAuth middleware for traefik, which works like nginx auth_request. Upon receiving a client request, depending on the routing, traefik may initiate a subrequest to authelia possibly with necessary client crendentials, if authelia is able to authenticate the user and authorize the request, the client request will be forwarded to the backend service with some extra headers containing client user information. There is not such thing as authorization yet. It\u0026rsquo;s only me using my services.\nWeakness Authelia is not satisfactory in many aspects. First, its policy engine is not flexible enough. Second, it requires a lot of boilerplate in the configuration, e.g. I need to specify many hard-coded base domain hostname-a.mydomain.tld instead of hostname-a. This is not desirable as I have many different postfixes, and the configuration is shared.\nStrength What I really like about authelia is its simplicity and easy integration with traefik.\nFuture I want to use a beyondcorp style identity-aware proxy with open policy agent support some other day. The last time I checked pomerium, I found envoy was hard to pack and pomerium was too oidc-centric, most of all it did not support ldap or other local user database.\nSSO Authelia just landed openid connect support. I haven\u0026rsquo;t tried it yet. One more thing about authelia is that I currently use a single text file as account backend. I have set up openldap on my machines, but I haven\u0026rsquo;t tried it on authelia yet. I intend to use freeipa instead, which is much more versatile.\nIntrusion Prevention Because of my distrust to other people\u0026rsquo;s computer, I intentionally made my edge proxy to be as dumb as possible. There ain\u0026rsquo;t such thing as intrusion detection system yet. Setting up fail2ban is easy, but I need to integrate it with traefik and aioproxy.\nBackup Of all the incremental backup tools, there are two distinctive features about restic. First, it supports all rclone backends, second, I can back up different directories from different hosts to the same endpoint. Here is my nix configuration.\nrestic = { backups = let go = name: conf: backend: { \u0026#34;${name}-${backend}\u0026#34; = { initialize = true; passwordFile = \u0026#34;/run/secrets/restic-password\u0026#34;; repository = \u0026#34;rclone:${backend}:restic\u0026#34;; rcloneConfigFile = \u0026#34;/run/secrets/rclone-config\u0026#34;; timerConfig = { OnCalendar = \u0026#34;00:05\u0026#34;; RandomizedDelaySec = \u0026#34;5h\u0026#34;; }; pruneOpts = [ \u0026#34;--keep-daily 7 --keep-weekly 5 --keep-monthly 12 --keep-yearly 75\u0026#34; ]; } // conf; }; mkBackup = name: conf: go name conf \u0026#34;backup-primary\u0026#34; // go name conf \u0026#34;backup-secondary\u0026#34;; in mkBackup \u0026#34;vardata\u0026#34; { extraBackupArgs = [ \u0026#34;--exclude=postgresql\u0026#34; ]; paths = [ \u0026#34;/var/data\u0026#34; ]; }; }; I back up my data every day to two backend storage.\nObservablility I use grafana, loki, prometheus for observablility. I can\u0026rsquo;t praise enough this squad for its simplicity to set up. I basically just set up the components separately. They just work. Also, it is a share-nothing architecture, so in order to acheive high availbility, all I need to do is add a new remote write target. For that, I use grafana cloud.\nMetrics Prometheus is pull based. It is quite easy to obtain nodes data from node exporter. Besides, almost all services now expose prometheus metrics.\nLogs Loki lives up to its promise \u0026ndash; like prometheus, for logs. It is harder for loki to search things, but it is tolerable for me.\nVisualization Grafana.\nTODO Alerts Alert manager.\nContinuous Integration/Continuous Delivery Worker Github is quite generous for the offer of github actions. The free machines' performance is quite good. It is no wonder that there are many miners trying to abuse them. As good as github actions, there are two nuisances for my usage.\n disk size. The closure size of my toplevel system profile easily exceeds the size limit. I need to clean up some packages to get more free disk space.  Some of my machines' profile can be as large as 70G. There is no way for github actions to build a profile that large.\n running time limit. Nix channel updates can invaildate many binary caches. I need to build so many packages that github actions workflow frequently times out.  I need to manually rerun it. I have to cache my build artifacts with cachix.\nArtifacts store Most of nix\u0026rsquo;s builds are reproducible. The nix derivation output path depends on the hashes of the build inputs. Given the same inputs, we can easily check if there are valid binary cache for the output. I use cachix to cache my builds. Think cachix as a docker container registry. It is quite straightforward to use cachix action. I also setting up cachix in my local machines, so that I can use the building results of github actions worker. It greatly reduces the building time on my local machines.\nDeployment I use deploy-rs to deploy my nixos configuration to the target machine. deploy-rs reads my flake.nix, builds the profile on the machine running deploy-rs command. It then copies the profile to target machine via ssh. Depending on my configuration, it may choose to download binary caches from subsitutes firstly (thus reduces time by avoid possible slow ssh connection). It should be noted that deploy build the profile on local machine. This is important for me as many of my machines are not powerful enough to build a profile quickly. deploy-rs also has elementary sanity check, e.g. automatically rollback to previous generation of profile if ssh connection didn\u0026rsquo;t come back after switch to the new profile. The only remaining complication is ssh connectivity.\nNode Connectivity To establish connectivity from github actions runner to my server, I use wstunnel. Well, this time I use port-mapping solution. Note that wstunnel dig tunnels over websocket. And I have described a lot about how I can access my services over http above. So this is quite a no-brainer for me to set up a tunnel. All I need to do is running wstunnel in server mode, set up a routing for it, and then ssh -o ProxyCommand=\u0026quot;wstunnel --upgradePathPrefix=some-superb-secret-path -L stdio:%h:%p wss://wstunnel.example.com\u0026quot; hub.localhost I kept the routing path some-superb-secret-path secret so that it would be impossible for other people to arbitrarily establish a tunnel to my machine.\nSecrets management One more thing, how to make github actions runner\u0026rsquo;s ssh connection to my machines more secure. I fully agree the sentimental of this article. We should use ssh certificates as more as possible. The question is now how to securely use ssh certificates. I need a system to automatically issue short-lived certificates. This system must be fully programmable. Smallstep certificates is not good in terms of programmability. I use Hashicorp Vault ssh secret engine for this. Here is how I use valut to issue short-lived ssh certificates.\nProxy It is a mandate to use a proxy on my machines, as too many websites are blocked in China. I can\u0026rsquo;t tolerate my wallabag instance is unable to access articles on Wikipedia. I use clash and iptables for transparent proxy. Here is the script, and here is the systemd unit to run the script and update clash configuration. The source of truth for my clash configuration lies in cloudflare workers kv. All my machines use the same proxy configuration by periodically downloading a subscription from cloudflare worker. Although it is straightforward to set up transparent proxy on Linux, There are two complications when I want to proxy docker container traffic transparently.\nTransparent proxy does not work with docker container in bridge network mode This is a first world problem. Docker/Kubernetes wants sysctl net.bridge.bridge-nf-call-iptables=1, while libvirt wants sysctl net.bridge.bridge-nf-call-iptables=0. More explanations can be found here, here and here. The following scenery illustrates why docker/Kubernetes insists on enabling bridge-netfilter.\ndocker run -it --rm -p 8081:8081 nicolaka/netshoot socat -v -v -d -d tcp-listen:8081,fork exec:cat HOST_IP=\u0026#34;$(ip -4 -json addr | jq -r \u0026#39;.[] | .addr_info[] | select(.scope == \u0026#34;global\u0026#34;) | .local\u0026#39; | head -n 1)\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081,bind=\\$(ip -4 -json addr show dev eth0 | jq -r \u0026#39;.[].addr_info[].local\u0026#39;):8082\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081,bind=127.1.0.1:8082\u0026#34; When bridge-netfilter is disabled, the last command would time out, while the other two commands will not. This kind of hairpinning support is seldom needed on my machine.\nsysctl net.bridge.bridge-nf-call-iptables=0 net.bridge.bridge-nf-call-ip6tables=0 net.bridge.bridge-nf-call-arptables=0 So I disable bridge-netfilter. A further complication is that k3s and docker is so smart as to enable bridge-netfilter on startup. I added a ExecStartPost to disable it.\nTransparent proxy does not work with docker container when on-ip is missing To be more precise, sometimes it does not work. I don\u0026rsquo;t know why. I just banged my head for a few hundreds times and find out --on-ip is a must.\niptables -t mangle -A CLASH_EXTERNAL -p tcp -j TPROXY --on-port 7893 --on-ip 127.0.0.1 --tproxy-mark 0x4242/0xffffffff Alternatives Oh, dear god, iptables is hard. I wish there is an easier way to transparent proxy.\n TUN macvlan virtual machine  Server Management wstunnel ttyd aioproxy Next Step Kubernetes after All? I abandoned my plan of using Kubernetes for all. Currently, I refrain my usage of Kubernetes because first I didn\u0026rsquo;t find a satisfactory workflow for nix and Kubernetes, second I begin to feel Kubernetes is the new c++. I sincerely hope I can declaratively manage Kubernetes with nix the way I manage docker and traefik with nix. I find integrating kustomize and kubenix interesting, but it is not there yet. Both nix and Kubernetes are too overwhelming. They require you to go all-in. Nix is my daily driver. It is definitely here to stay. I need some Kubernetes features like node affinity (jupyter hub requires a faster node) and proxying traffic received from any node. As I said, Kubernetes is like c++. It is extremely powerful, but it is also extremely complex and can be easily misused. I partially agree “Let’s use Kubernetes!” Now you have 8 problems. I find also find kubevela to be interesting. I haven\u0026rsquo;t tried it yet. I hope it lives up to its promise. Also, Nomad looks interesting, it may well suit che and jupyter hub, but they do not support nomad.\nConfiguration Database Nix is great. But it is hard for outside world to learn my nix configuration.\nSecurity Hardening Federated Storage Grand Unification Personal Data Warehouse ",url:"https://wiki.cont.run/self-hosted-infrastructure/"},"https:\/\/wiki.cont.run\/self-hosted-services\/":{title:"self-hosted services",tags:[],content:"TODO. For now, See self-hosted infrastructure.\n",url:"https://wiki.cont.run/self-hosted-services/"},"https:\/\/wiki.cont.run\/sigma-bullets\/":{title:"sigma bullets",tags:[],content:"Introduction This document summarize how a zero-knowledge confidential transaction scheme works and how to implement it. A confidential transaction scheme is a transaction scheme with which the transaction value and the balance of the sender and receiver are encrypted. The zero-knowledge part means that outsider can effectively learn nothing about the values, although he can verify the transaction is not fabricated. The main references are Bulletproofs and Zether. We use Zether to homomorphically encrypt the transaction value so that the we can directly add/subtract ciphertext of the encrypted balance which can then be decrypted into the correct balance after the transaction. We use bulletproofs to check the transaction value is valid, i.e. it is a non-negative number within the range \\([0, 2^n)\\), and after the transaction the sender must still have a non-negative balance. The vanilla bulletproofs do not apply to the scenario of zether as Elgamal commitments are not fully homomorphic. We need to tweak bulletproofs to support \\(\\Sigma\\)-protocols, i.e. interactive proof of the values commited in Bulletproofs are truly the values involved in zether, whence we obtain a complete and sound proof of a confidential transaction.\nEncryption and Decryption of Balance From now no, Let \\(G\\) be a group where the discrete logarithm problem is assumed to be hard to solve. Let \\(g, \\overrightarrow{g}=(g_1, g_2, \\cdots), h, \\overrightarrow{h}=(h_1, h_2, \\cdots)\\) be base points of \\(G\\) whose logarithm relationship is unclear.\nLet \\(y_1 = sk_1 * g\\) (resp. \\(y_2 = sk_2 * g\\)) be public key of secret key \\(sk_1\\) (resp. \\(sk_2\\)). Assume the balance of every account is in the interval \\([0, 2^n)\\), where \\(n\\) is a small integer like 32. The ciphertexts obtained from encrypting balance \\(b_1\\) (resp. \\(b_2\\)) with public key \\(y_1\\) (resp. \\(y_2\\)) is \\(c_1 = (b_1 * g + r_1 *y_1, r_1 * g)\\) (resp. \\(c_2 = (b_2 * g + r_2 * y_2, r_2 * g)\\)), where \\(r_1\\) and \\(r_2\\) are random scalars. This is also called ElGamal Commitment.\nUsing the usual ElGamal decryption, we can obtain \\(b_1 * g\\) (resp. \\(b_2 * g\\)) from ciphertext \\(c_1\\) (resp. \\(c_2\\)), i.e. we calculate \\((b_1 * g + r_1 * y_1) - (sk_1 * r_1 * g)\\) which equals \\(b_1 * g\\) by definition of the public key \\(y_1 = sk_1 * g\\). We then obtain \\(b_1\\) (resp. \\(b_2\\)) with brute force. This is feasible as \\(b_1, b_2\\) are small enough.\nConfidential Transfer For the same public key \\(y\\), we define the addition/subtraction of two ciphertexts \\(c_1 = (b_1 * g + r_1 * y, r_1 * g)\\) and \\(c_2 = (b_2 * g + r_2 * y, r_2 * g)\\) as the multiplication/division in the group \\(G^2\\), for example define \\(c_1 + c_2 = ((b_1 + b_2) * g + (r_1+r_2) * y, (r_1+r_2) * g)\\). It is easy to verify the decryption of the resulting ciphertext is indeed the addition/subtraction of corresponding balance.\nThat is to say, the mapping from balance interval to ciphertext homomorphic, we can do the math on ciphertexts which corresponds exactly to the math on balances.\nWe want to make a transaction from account \\(Y\\) to account \\(\\bar{Y}\\), we assume \\(Y\\) initially has balance \\(b\\), he/she wants to transfer \\(b^\\star\\) to \\(\\bar{Y}\\). In the good old bitcoin world. We need only check, it is indeed \\(Y\\) made the transaction, and \\(Y\\) didn\u0026rsquo;t transfer more than what he/she has, i.e. \\(b\\).\nIn the brave new world of cryptopia, we have no way to know what \\(b\\) and \\(b^\\star\\) are, as they are both encrypted.\nProof of knowledge of discrete logarithm Let\u0026rsquo;s summarize what we need to do.\nSuppose \\(Y\\), whose public key is \\(y\\), secret key is \\(sk\\), wants to transfer \\(b^\\star\\) to \\(\\bar{Y}\\), whose public key is \\(\\bar{y}\\), in the end, X has only \\(b^\\prime\\) left in his/her wallet. Our goal is then to prove the following statements.\n The ciphertext \\((C, D)\\) of \\(b^\\star\\) under public key \\(y\\) and random number \\(r\\) is \\((b^\\star * g + r * y, r * g)\\). The ciphertext \\((\\bar{C}, D)\\) of \\(b^\\star\\) under public key \\(\\bar{y}\\) and random number \\(r\\) is \\((b^\\star * g + r * \\bar{y}, r * g)\\). Note we also enforce \\(b^\\star\\) is encrypted under the same random number \\(r\\). \\(C_{n}, D_{n}\\), the amount of money of \\(Y\\) left after the transaction is the ElGamal encryption of \\(b^\\prime\\) under public key \\(y\\), i.e. \\(C_n = b^\\star * g + sk * D_n\\) and \\(sk * g = y\\). Both \\(b^\\star\\) and \\(b^\\prime\\) are within the range \\([0, 2^n)\\).  We can make use of Schnorr\u0026rsquo;s protocol to prove the first three statements.\nIn Schnorr\u0026rsquo;s protocol, the prover wants to prove that he knows \\(x\\) such that \\((x, h)\\) satisfies relationship \\(h = x * g\\) where \\(g\\) is a known element of group \\(G\\), and \\(x\\) is hidden. Schnorr\u0026rsquo;s protocol. First the prover randomly choose a scalar \\(r\\) and send \\(u = r * g\\) to the verifier. The verifier send the randomly chosen challenge \\(c\\) to the prover. The honest prover send \\(v = (c * x + r) * g\\) to the verifier. The verifier outputs \\( v == c*h + u \\).\nWith little changes, we can extend Schnorr\u0026rsquo;s protocol to prove statements like \\(C = b^\\star * g + r * y\\). Now We need only a proof which proves both \\(b^\\star\\) and \\(b^\\prime\\) are within the range \\([0, 2^n)\\), without ever revealing the actual values. In order to do so, we will first commit the value, and then prove properties concerning the commitment.\nPederson Commitment We now provide another way to hide balance which also allow us prove to statements about the hidden balance. Given a value \\(v\\) in the message space, we can commit this value and obtain a output \\(c\\) in the commitment space. \\(c\\) is called the commitment of \\(v\\).\nThere are two properties concerning the security of a commitment scheme, binding and hiding. Informally, a commitment is said to be binding if we can not find two values whose commitment are equal, a commitment is said to be hiding is we can not discern two values from each other. If in addition to this two properties, the commitment scheme is homomorphic, then we translate statements from message space to commitment space. This is quite useful for us to prove properties of hidden values.\nOne of the hiding, binding and homomorphic commitment schemes is Pederson commitment. The Pederson commitment of \\((b, r) \\in (\\mathbb{Z}, \\mathbb{Z})\\) is defined to be the function \\(PC: (b, r) \\mapsto b*g + r*h\\) where \\(g\\) and \\(h\\) are fixed base points, \\(r\\) is called the blinding factor of \\(b\\). Note that in Pederson Commitment \\(h\\) is fixed. We can easily verify Pedenson commitment is a homomorphic commitment scheme, i.e. \\(\\forall b_1, r_1, b_2, r_2\\), we have \\(PC(b_1+b_2, r_1+r_2) = PC(b_1, r_1) + PC(b_2, r_2)\\).\nWe generalize Pederson Commitment to vectors. Let \\((\\overrightarrow{a_L}, \\overrightarrow{a_R}) \\in (\\mathbb{Z}^n, \\mathbb{Z}^n)\\), we define the Pederson commitment to be the function \\(PC: (\\overrightarrow{a_L}, \\overrightarrow{a_R}, r) \\mapsto \\sum a_{L_i} * g_i + \\sum a_{R_i} * h_i + rh\\) where \\(h\\), \\(g_i\\) and \\(h_i\\) are fixed base points, \\(r\\) is called the blinding factor of \\((\\overrightarrow{a_L}, \\overrightarrow{a_R})\\).\nZen of Range Checking Instead of proving \\(a\\) is within the range \\([0, 2^n)\\) directly. We prove the following equivalent equation.\n\n\\[a - \\sum_{i=1}^{n} a_{L_i} \\times 2^{i-1} = 0 \\text{ (eqn:1)}\\]\n\n\\[a_{L_i} - 1 - a_{R_i} = 0, \\forall i = 1, \\cdots, n \\text{ (eqn:2)}\\]\n\n\\[a_{R_i} * a_{L_i} = 0, \\forall i = 1, \\cdots, n \\text{ (eqn:3)}\\]\nCombining 1 and 1, we have \\((a_{L_i} - 1) * a_{L_i} = 0\\), i.e. \\(a_{L_i} = 1\\) or \\(a_{L_i} = 0\\). Together with the first equation, we can see that \\(a_{L_i}\\) is the binary representation of \\(a\\). As we have only \\(n\\) \\(a_{L_i}\\), \\(a\\) is indeed within the range \\([0, 2^n)\\).\nDenote \\(\\overrightarrow{y_n}\\), or simply \\(\\overrightarrow{y}\\) when \\(n\\) is clear, \\((1, y, \\cdots, y^{n-1})\\), \\(overrightarrow{a_L} = (a_{L_1}, a_{L_2}, \\cdots, a_{L_n})\\), \\(\\overrightarrow{a_R} = (a_{R_1}, a_{R_2}, \\cdots, a_{R_n})\\). Let \\(X\\), \\(Y\\) be two vector in \\(\\mathbb{Z}^n\\), we denote the Euclid inner product \\(X\\cdot Y\\), the Hermitian product \\(X \\circ Y\\).\nTo verify the second equation, the verifier makes a challenge, a random scalar \\(y\\), to the prove. The prover proves that,\n\n\\[ \\sum_{i=1}^{i=n} (a_{L_i} - 1 - a_{R_i}) * y^{i-1} = 0 \\text{ (eqn:4)}\\]\nThe left side of the above equation is a polynomial of degree at most \\(n-1\\), so it has at most \\(n-1\\) roots. \\(y\\) is highly unlikely to be a root of the polynomial unless all coefficients are zero.\nUsing the same argument, verifying the following equation is enough for the third equation.\n\n\\[ \\sum_{i=1}^{i=n} (a_{L_i} * a_{R_i}) * y^{i-1} = 0 \\text{ (eqn:5)}\\]\nRewrite equation 1 as \\((\\overrightarrow{a_L} - \\overrightarrow{1_n} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_n} = 0\\), rewrite equation 1 as \\((\\overrightarrow{a_L}) \\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_n}) = 0\\), rewrite 1 as \\(a - \\overrightarrow{a_L} \\cdot \\overrightarrow{2_n} = 0\\). Using the trick as above, we combine this equations to a single equation\n\n\\[(\\overrightarrow{a_L} - \\overrightarrow{1_n} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_n} + \\overrightarrow{a_L}\\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_n}) * z + (a - \\overrightarrow{a_L} \\cdot \\overrightarrow{2_n})* z^2 = 0 \\text{ (eqn:6)}\\]\nThis equation is equivalent to\n\n\\[(\\overrightarrow{a_L} - z\\overrightarrow{1_n}) \\cdot (\\overrightarrow{a_R}\\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n}) = z^2 v + \\delta(y, z) \\text{ (eqn:7)}\\]\nwhere \\(\\delta(y, z) = (z - z^2)(\\overrightarrow{1_n} \\cdot \\overrightarrow{y_n}) - z^3 (\\overrightarrow{1_n} \\cdot \\overrightarrow{2_n})\\) is a term involves only \\(y\\) and \\(z\\).\nIn order to make the range proof zero knowledge, we will add additional term \\(\\overrightarrow{s_L}x\\) (resp. \\(\\overrightarrow{s_R}x\\)) to \\(\\overrightarrow{a_L}\\) (resp. \\(\\overrightarrow{a_R}\\)), where \\(\\overrightarrow{s_L}, \\overrightarrow{s_R} \\in \\mathbb{Z}^n\\) are random vectors, \\(x\\) is unknown variable in \\(\\mathbb{Z}\\). Thus the left-hand side of equation 1 is now a polynomial in \\(x\\) of degree 2. Adjust the right-hand side to a polynomial in \\(x\\) of degree 2, Then we have a equation of the following form\n\n\\[\\overrightarrow{l(x)} \\cdot \\overrightarrow{r(x)} = t(x) \\text{ (eqn:8)}\\]\nwhere\n\n\\[\\overrightarrow{l(x)} = \\overrightarrow{a_L} + \\overrightarrow{s_L}x - z\\overrightarrow{1_n} \\text{ (eqn:9)}\\]\n\n\\[\\overrightarrow{r(x)} = (\\overrightarrow{a_R} + \\overrightarrow{s_R}x) \\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n} \\text{ (eqn:10)}\\]\n\n\\[t(x) = t_0 + t_1 x + t_2 x^2 = z^2 v + \\delta(y, z) + t_1 x + t_2 x^2 \\text{ (eqn:11)}\\]\nRange Proof We now view \\(x\\) as a chosen random scalar. Let \\(V\\) be the Pederson Commitment of \\(v\\), \\(T_1\\) be the Pederson Commitment of \\(t_1\\), \\(T_2\\) be the Pederson Commitment of \\(t_2\\), \\(A = PC(\\overrightarrow{a_L}, \\overrightarrow{a_R}, \\tilde{a})\\), \\(S = PC(\\overrightarrow{s_L}, \\overrightarrow{s_R}, \\tilde{s})\\), \\(P = PC(\\overrightarrow{l(x)}, \\overrightarrow{r(x)}, \\tilde{p})\\).\nThe range proof consists of \\((V, A, S, T_1, T_2, \\tilde{t}(x), t(x), \\tilde{p})\\) and a proof which proves that \\(t(x)\\) is indeed the inner product of \\(\\overrightarrow{l(x)}\\) and \\(\\overrightarrow{r(x)}\\), i.e. 1 holds.\nTo verify 1 and 1, we note that knowing the blinding factor, the Pederson commitment of \\((\\overrightarrow{a_L} + \\overrightarrow{s_L}x - z\\overrightarrow{1_n}, (\\overrightarrow{a_R} + \\overrightarrow{s_R}x) \\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n})\\) can be calculated from \\(A, S\\). Given the blinding factor of the Pedenson commitment of \\(\\overrightarrow{l(x)}, \\overrightarrow{r(x)}\\), we can calculate the commitment of \\((\\overrightarrow{l(x)}, \\overrightarrow{r(x)})\\) in the inner product proof. Thus we can only verify the given inner product proof with the commitment calculated from above.\nTo verify 1, we compare the commitment of \\(t(x)\\) with the commitment of \\(z^2 v + \\delta(y, z) + t_1 x + t_2 x^2\\). The first term can be calculated directly with \\(\\tilde{t}(x), t(x)\\), and the second term can be calculated with \\(V, T_1, T_2\\).\nAggregated Range Proof In our use case, we want to aggregate two range proofs. To aggregate range proofs of terms \\(a^{(k)}, k = 1, \\cdots, m\\) are within the range \\([0, 2^n)\\), we have the following equations\n\n\\[ a^{(k)} - \\sum_{i=1}^{n} a^{(k)}_{L_i} \\times 2^{i-1} = 0, \\forall k = 1, \\cdots, m \\text{ (eqn:12)}\\]\n\n\\[a^{(k)}_{L_i} - 1 - a^{(k)}_{R_i} = 0, \\forall i = 1, \\cdots, n, \\forall k = 1, \\cdots, m \\text{ (eqn:13)}\\]\n\n\\[a^{(k)}_{R_i} * a^{(k)}_{L_i} = 0, \\forall i = 1, \\cdots, n, \\forall k = 1, \\cdots, m \\text{ (eqn:14)}\\]\nNote when we concatenate all the binary representation of \\(\\overrightarrow{a^{(k)}_{L}}\\) (resp. \\(\\overrightarrow{a^{(k)}_{R}}\\)) into \\(\\overrightarrow{a_{L}}\\) (resp. \\(\\overrightarrow{a_{R}}\\)), we can condense 1 (resp. 1) into the 1 (resp. 1). We use the same trick as before to compress equations in 1, then we have \\[(\\overrightarrow{a_L} - \\overrightarrow{1_{mn}} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_{mn}} + \\overrightarrow{a_L}\\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_{mn}}) * z + \\sum_k(a^{(k)} - \\overrightarrow{a^{(k)}_{L}} \\cdot \\overrightarrow{2_n}) * z^{2+k} = 0\\]\nAccordingly, we adjust terms in 1, 1 and 1. After that, we can verify the proof in the same way. Now we have got rid of all the roadblocks. A zero-knowledge confidential transaction scheme is here to stay.\n",url:"https://wiki.cont.run/sigma-bullets/"},"https:\/\/wiki.cont.run\/tags\/":{title:"Tags",tags:[],content:"",url:"https://wiki.cont.run/tags/"}}</script><script src=https://wiki.cont.run/js/lunr.min.js type=text/javascript></script><script src=https://wiki.cont.run/js/search.js type=text/javascript></script></body></html>