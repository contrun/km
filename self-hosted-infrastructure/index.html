<!doctype html><html><title>self-hosted infrastructure - wiki</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name=apple-mobile-web-app-capable content="yes"><meta name=description content="TLDR: I use tailscale/zerotier to establish a smallish mesh network. I use envoy (not anymore, I now use nginx) as an edge router to forward L4 traffic. I mainly provision and manage services with nix, docker and sops. When it is absolutely required, I use k3s to deploy Kubernetes services. Traefik is used for routing, and authelia is used for blocking unauthorized access. To multiplexing protocols with a single port, I use aioproxy."><link rel=stylesheet href=https://wiki.cont.run/css/main.min.8d43d6513a337a78474b04be2b746e8294979e8c860cf809a6f432cc9e08ab50.css><body><header><a href=../ id=logo><svg id="Capa_1" enable-background="new 0 0 511.992 511.992" height="512" viewBox="0 0 511.992 511.992" width="512" xmlns="http://www.w3.org/2000/svg"><g><g><g><path d="m256 420.826c0 38.345-11.844 68.545-49.991 68.014-27.744-.385-51.481-15.31-61.853-39.46-1.239-2.887-4.024-4.734-7.154-4.725-.07.0-.135.0-.201.0-47.474.0-75.537-26.171-75.537-73.882.0-5.633.542-11.138 1.568-16.468.62-3.229-.825-6.489-3.671-8.125C23.668 325.748 10 300.053 10 255.997c0-44.057 13.668-69.757 49.161-90.185 2.846-1.636 4.291-4.896 3.671-8.13-1.026-5.33-1.568-10.83-1.568-16.463.0-47.711 28.064-73.882 75.537-73.882h.201c3.13.009 5.915-1.837 7.154-4.729 10.372-24.145 34.109-39.069 61.853-39.455C244.159 22.621 256 52.821 256 91.166" fill="#ff9eb1"/></g><g><g><g><path d="m256 91.166c0-38.344 11.844-68.545 49.991-68.014 27.744.385 51.481 15.31 61.853 39.46 1.239 2.887 4.024 4.734 7.154 4.724h.201c47.474.0 75.537 26.171 75.537 73.882.0 5.633-.542 11.138-1.568 16.468-.62 3.229.825 6.489 3.671 8.125C488.332 186.245 502 211.939 502 255.996s-13.668 69.756-49.161 90.185c-2.846 1.636-4.291 4.896-3.671 8.13 1.026 5.33 1.568 10.83 1.568 16.463.0 47.711-28.064 73.882-75.537 73.882h-.201c-3.13-.009-5.915 1.837-7.154 4.729-10.372 24.145-34.109 39.069-61.853 39.455-38.15.531-49.991-29.669-49.991-68.014" fill="#ff7d97"/></g></g><g><g><path d="m502 265.996c-4.193.0-7.984-2.713-9.407-6.636-1.419-3.912-.16-8.459 3.063-11.092 3.291-2.689 8.009-2.99 11.621-.758 3.568 2.205 5.404 6.578 4.478 10.669-1.02 4.501-5.126 7.817-9.755 7.817z"/></g></g></g><g><path d="m340.83 229.18h-58.013v-58.014h-53.634v58.014H171.17v53.633h58.013v58.013h53.634v-58.013h58.013z" fill="#faf7f5"/></g></g><g><g><path d="m498.468 291.859c-5.141-2.02-10.945.508-12.965 5.648-6.442 16.389-18.055 28.727-37.649 40.005-6.513 3.746-9.932 11.253-8.505 18.689.921 4.783 1.388 9.686 1.388 14.572.0 41.792-22.662 63.882-65.537 63.882h-.225c-.938.0-1.864.074-2.771.217-15.031-4.92-23.796-20.93-19.661-36.479 1.42-5.337-1.757-10.815-7.094-12.234-5.333-1.418-10.814 1.756-12.234 7.094-5.958 22.405 4.241 45.396 23.384 56.443-9.583 17.791-28.602 28.836-50.748 29.145-11.303.146-19.802-2.743-26.011-8.867-9.184-9.057-13.84-25.592-13.84-49.148v-70h16.816c5.522.0 10-4.477 10-10v-48.014h48.014c5.522.0 10-4.477 10-10V229.18c0-5.523-4.478-10-10-10h-48.014v-48.014c0-5.523-4.478-10-10-10H266v-70c0-23.555 4.657-40.09 13.841-49.148 6.21-6.123 14.696-9.022 26.011-8.867 23.862.332 44.096 13.132 52.803 33.405.218.509.458 1.003.719 1.483-3.225 8.243-9.084 15.093-16.833 19.574-8.993 5.2-19.465 6.581-29.485 3.89-5.337-1.434-10.819 1.73-12.251 7.064-1.433 5.334 1.729 10.819 7.063 12.252 5.074 1.363 10.221 2.037 15.338 2.037 10.2.0 20.272-2.681 29.346-7.928 11.083-6.408 19.607-16.017 24.616-27.575 41.6.67 63.569 22.718 63.569 63.866.0 4.89-.467 9.794-1.389 14.582-1.427 7.426 1.991 14.933 8.502 18.678 19.572 11.267 31.176 23.584 37.625 39.936 1.552 3.934 5.318 6.334 9.306 6.333 1.221.0 2.462-.225 3.666-.7 5.138-2.026 7.66-7.834 5.634-12.972-7.943-20.141-22.201-35.773-44.801-49.086.967-5.521 1.457-11.155 1.457-16.772.0-52.114-31.48-83.391-84.288-83.876-12.157-26.863-38.963-43.753-70.318-44.189-16.67-.232-30.255 4.688-40.332 14.625-3.813 3.76-7.08 8.226-9.797 13.382-2.717-5.156-5.984-9.622-9.797-13.382-10.075-9.937-23.668-14.852-40.333-14.625-31.353.437-58.158 17.325-70.32 44.189-52.807.485-84.286 31.762-84.286 83.876.0 5.617.49 11.253 1.458 16.771-37.422 22.031-52.724 50.552-52.724 98.008.0 47.451 15.299 75.969 52.721 98.006-.967 5.521-1.457 11.154-1.457 16.772.0 52.114 31.48 83.391 84.288 83.876 12.157 26.863 38.963 43.753 70.318 44.189.377.005.751.008 1.125.008 16.172.0 29.358-4.92 39.207-14.632 3.813-3.76 7.08-8.226 9.797-13.382 2.717 5.156 5.984 9.622 9.797 13.382 9.849 9.713 23.034 14.633 39.208 14.632.373.0.749-.003 1.125-.008 31.352-.436 58.158-17.325 70.32-44.189 52.807-.485 84.286-31.762 84.286-83.876.0-5.617-.49-11.253-1.458-16.772 22.634-13.329 36.901-28.989 44.838-49.178 2.022-5.14-.507-10.945-5.647-12.966zM282.816 239.18h48.014v33.633h-48.014c-5.522.0-10 4.477-10 10v48.014h-33.633v-48.014c0-5.523-4.477-10-10-10H181.17V239.18h48.014c5.523.0 10-4.477 10-10v-48.014h33.633v48.014c-.001 5.523 4.477 10 9.999 10zm-50.657 230.794c-6.21 6.124-14.717 9.018-26.011 8.867-23.862-.331-44.096-13.132-52.803-33.405-.218-.509-.458-1.003-.719-1.483 3.225-8.243 9.085-15.093 16.833-19.574 8.992-5.2 19.463-6.581 29.485-3.89 5.337 1.434 10.819-1.73 12.251-7.064 1.433-5.333-1.73-10.819-7.064-12.251-15.188-4.08-31.059-1.988-44.684 5.891-11.083 6.408-19.607 16.017-24.616 27.575-41.6-.67-63.569-22.718-63.569-63.866.0-4.89.467-9.794 1.389-14.582 1.427-7.427-1.991-14.934-8.502-18.678-32.183-18.528-44.149-40.621-44.149-81.517.0-40.9 11.966-62.994 44.146-81.517 6.513-3.746 9.932-11.253 8.505-18.689-.921-4.783-1.388-9.686-1.388-14.572.0-41.792 22.662-63.882 65.537-63.882h.225c.938.0 1.864-.074 2.771-.217 15.031 4.92 23.796 20.93 19.661 36.479-1.42 5.337 1.757 10.815 7.094 12.234.861.229 1.726.338 2.577.338 4.422.0 8.467-2.956 9.657-7.432 5.958-22.405-4.241-45.396-23.384-56.443 9.583-17.791 28.602-28.836 50.748-29.145 11.267-.15 19.801 2.743 26.011 8.867 9.184 9.057 13.84 25.592 13.84 49.148v70h-16.816c-5.522.0-10 4.477-10 10v48.014H171.17c-5.522.0-10 4.477-10 10v53.633c0 5.523 4.478 10 10 10h48.014v48.014c0 5.523 4.478 10 10 10H246v70c0 23.554-4.657 40.09-13.841 49.147z"/><path d="m139.699 228.227c-6.766.0-13.186 1.514-18.907 4.31-3.049-8.65-8.286-16.485-15.336-22.673-4.151-3.643-10.469-3.233-14.113.918-3.643 4.15-3.232 10.469.918 14.112 6.711 5.891 10.816 14.143 11.498 22.953-1.213 1.914-2.293 3.946-3.225 6.088-1.145 2.633-2.015 5.316-2.615 8.019-13.414 11.422-33.601 10.834-46.225-1.792-3.906-3.904-10.236-3.904-14.143.0-3.905 3.905-3.905 10.237.0 14.143 10.524 10.524 24.354 15.784 38.193 15.784 8.04.0 16.083-1.775 23.484-5.325 1.904 5.443 4.967 10.557 9.136 15.03.596.639 1.204 1.269 1.826 1.891 1.953 1.953 4.512 2.929 7.071 2.929 2.56.0 5.118-.976 7.071-2.929 3.905-3.905 3.905-10.237.0-14.143-.458-.458-.906-.922-1.342-1.389-6.26-6.716-7.799-15.778-4.118-24.241 2.878-6.616 9.86-13.686 20.826-13.686 5.522.0 10-4.477 10-10 .001-5.522-4.476-9.999-9.999-9.999z"/><path d="m387.667 287.543c-3.905 3.905-3.905 10.237.0 14.143 1.953 1.953 4.512 2.929 7.071 2.929s5.118-.976 7.071-2.929c.622-.622 1.23-1.253 1.83-1.896 4.167-4.471 7.229-9.583 9.133-15.025 7.401 3.549 15.444 5.324 23.484 5.324 13.839.0 27.67-5.261 38.193-15.784 3.905-3.905 3.905-10.237.0-14.143-3.906-3.904-10.236-3.904-14.143.0-12.624 12.625-32.811 13.214-46.225 1.792-.6-2.702-1.47-5.386-2.615-8.019-.932-2.142-2.012-4.175-3.225-6.088.682-8.81 4.787-17.062 11.498-22.953 4.15-3.644 4.561-9.962.918-14.112-3.646-4.151-9.964-4.563-14.113-.918-7.05 6.189-12.287 14.023-15.336 22.673-5.721-2.796-12.141-4.31-18.907-4.31-5.523.0-10 4.477-10 10s4.477 10 10 10c10.966.0 17.948 7.07 20.826 13.686 3.681 8.463 2.142 17.525-4.114 24.237-.44.47-.888.935-1.346 1.393z"/></g></g></g></svg></a><h3 class=site-title>wiki</h3><form id=search action=https://wiki.cont.run/search/ method=get><label hidden for=search-input>Search site</label>
<input type=text id=search-input name=query placeholder="Type here to search">
<input type=submit value=search></form></header><div class=grid-container><div class=grid><div class=page data-level=1><div class=content><h1>self-hosted infrastructure</h1><p>TLDR: I use <a href=https://tailscale.com/>tailscale</a>/<a href=https://www.zerotier.com/>zerotier</a> to establish a smallish mesh network. I use <a href=https://www.envoyproxy.io/>envoy</a> (not anymore, I now use nginx) as an edge router to forward L4 traffic.
I mainly provision and manage services with <a href=https://nixos.org/>nix</a>, <a href=https://www.docker.com/>docker</a> and <a href=https://github.com/mozilla/sops>sops</a>. When it is absolutely required, I use <a href=https://k3s.io/>k3s</a> to deploy <a href=https://kubernetes.io/>Kubernetes</a> services.
<a href=https://traefik.io/>Traefik</a> is used for routing, and <a href=https://github.com/authelia/authelia>authelia</a> is used for blocking unauthorized access.
To multiplexing protocols with a single port, I use <a href=https://github.com/contrun/aioproxy/>aioproxy</a>. I use <a href=https://github.com/jimsalterjrs/sanoid/>syncoid</a> and <a href=https://restic.net/>restic</a> to back up my inevitably accumulated state. For CI/CD,
I use <a href=https://github.com/features/actions>github actions</a>, <a href=https://www.vaultproject.io/>hashicorp vault</a>, <a href=https://github.com/serokell/deploy-rs>depoly-rs</a> and <a href=https://www.cachix.org/>cachix</a>. I use the grafana stack (<a href=https://prometheus.io/>prometheus</a>, <a href=https://prometheus.io/docs/alerting/latest/alertmanager/>AlertManager</a>, <a href=https://grafana.com/>grafana</a> and <a href=https://grafana.com/oss/loki/>loki</a>)
for observablility.</p><h2 id=life-of-a-request>Life of a Request</h2><figure><img src=../ox-hugo/life-of-a-request.svg></figure><h2 id=principles>Principles</h2><p>My principles can be best described as <a href=https://github.com/cncf/toc/blob/main/DEFINITION.md>cloud nativeness</a>. Cloud-native is an all-encompassing and vague term.
I have a few concrete points on my mind.</p><ul><li>Software-defined everything</li><li>Declarative</li><li>Infrastructure as code</li><li>Minimal state maintenance</li><li>Self-organization</li><li>Single source of truth</li></ul><h2 id=networking>Networking</h2><p>The first obstacle to self-host everything is that you don&rsquo;t have a stable public accessible IP. There are a few solutions.</p><h3 id=the-cloud>The cloud</h3><ul><li>I am paranoid enough to not trust the cloud, aka other people&rsquo;s computer.</li><li>This approach is not cost-efficient. Even my Raspberry PI can beat many VPSes in terms of computing power. Not to mention I can easily insert a 256G SD card.</li><li>Locality. There is no place like LAN. I see no benefit in downloading youtube video to another VPS.</li></ul><h3 id=ddns>DDNS</h3><p>This is simplest. I don&rsquo;t use this mainly because it is not reliable in my setup. To name a few problems of DDNS,</p><ul><li>80, 443, 8080 blocked</li><li>Not portable router configurations. You need to set up port mapping or DMZ host in your router, which is hard to codify, if not impossible</li><li><a href=https://en.wikipedia.org/wiki/Carrier-grade%5FNAT>CGNAT</a></li><li>ipv6 is still yet to come</li></ul><h3 id=port-forwarding>Port Forwarding</h3><p>There are many port-forwarding software. To name a few, <a href=https://www.harding.motd.ca/autossh/>autossh</a> (my favorite), <a href=https://ngrok.com/>ngrok</a>, <a href=https://github.com/fatedier/frp>frp</a>, <a href=https://github.com/ehang-io/nps>nps</a>.
You may also combine DDNS with port forwarding of your router. <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L2189-L2234>Here</a> was my attempt to do this.
The biggest problem of port forwarding is that it is not scalable and there is no generic inter-node connectivity.
Port forwarding has the following weaknesses.</p><ul><li>star topology, single point of failure</li><li>no inter-node connectivity</li><li>number of ports are limited, you only have one 443</li><li>hard to set up (authorization)</li><li>no <a href=https://en.wikipedia.org/wiki/Hairpinning>hairpinning</a> support</li><li>most port forwarding only supports TCP</li></ul><h3 id=overlay-networks>Overlay Networks</h3><p><a href=https://en.wikipedia.org/wiki/Overlay%5Fnetwork>Overlay networks</a> are magic. What I meant is not <a href=https://github.com/containernetworking/cni>container network interface</a> kind of overlay network, but solutions like <a href=https://www.zerotier.com/>zerotier</a>, <a href=https://tailscale.com/>tailscale</a>, <a href=https://github.com/tonarino/innernet>innernet</a>, <a href=https://github.com/slackhq/nebula>nebula</a>, <a href=https://github.com/ntop/n2n>n2n</a>.
There all have interesting aspects. But none of they are self-organizing. They all require a centralized coordination server.
What I have on my mind is something like <a href=https://matrix.org/blog/2021/05/06/introducing-the-pinecone-overlay-network/>matrix pinecone</a>. I have been thinking on implementing a pinecone-like overlay network for a while, self-organizing, and tunneling traffic with <a href=https://libp2p.io/>libp2p</a>.
I currently rely on tailscale and zerotier to establish peer-to-peer connectivity. This works great in the following perspectives.</p><ul><li>inter-node connectivity</li><li>all ports are belong to you</li><li>easy to set up (implementation-dependent)</li><li>transparent <a href=https://en.wikipedia.org/wiki/Hole%5Fpunching%5F(networking)>hole punching</a></li><li>transparent multi-path</li></ul><h2 id=routing>Routing</h2><h3 id=l3>L3</h3><p>L3 routing is provided by the overlay network solutions.</p><h3 id=l4>L4</h3><h4 id=considerations>Considerations</h4><p>For L4 routing, I care about transparency, protocol multiplexing and configuration-complexity.</p><ul><li><p>Transparency</p><p>This means that the backend service does not need to know there is a middle man do the heavy lifting.
In particular, it means that the origin requester&rsquo;s address is preserved. Typical HTTP reverse proxies are not transparent.
They pass the original requester&rsquo;s information by injecting an <code>X-Forwarded-For</code> header.</p></li></ul><ul><li><p>Protocol Multiplexing</p><p>L4 protocol multiplexing means that we can use the same TCP port for HTTP, TLS and SSH. An example is <a href=https://github.com/yrutschle/sslh>sslh</a>.
It normally works by peeking into a few first bytes and determine which
protocol this packet is, and then handing off the connection to another application which is listening on some other port.</p></li></ul><ul><li><p>Configuration Complexity</p><p>Do we have to configure both the proxy and backend services? What if we change a user-fronting proxy address?
Do the backend server need to adjust for this change? Any special configuration for different user-fronting proxies?
What if an upstream server is down? Must I manually edit the configuration to reflect this change?</p></li></ul><h4 id=solutions>Solutions</h4><ul><li><p>iptables</p><p>This is just like NAT. It is transparent. I believe you can multiplex port with some <a href=https://ipset.netfilter.org/iptables-extensions.man.html>iptables extensions</a>. It is not super pretty.
A lethal problem is that the user-fronting proxy must be in the return path of the connection (usually the proxy is the default gateway).
To circumvent this problem, we need <a href=https://unix.stackexchange.com/questions/4420/reply-on-same-interface-as-incoming>some modifications to the routing table and routing policies</a>.
When there are two proxies which are connected to the same interface, there are multiple return paths, to select the correct one,
we need policy based routing.</p></li></ul><ul><li><p>ipvs</p><p>Compared with iptables, ipvs is much more manageable and scalable. Yet it still is too complicated.</p></li></ul><ul><li><p>usespace L4 proxy</p><p>envoy/haproxy/nginx etc. can be used as L4 proxy. They accept incoming downstream connection and establish a new upstream connection, just like a pipe.
This is much more manageable, the downside is that the original client&rsquo;s information is lost in translation.
To ease this problem, haproxy designed a protocol called <a href=https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt>PROXY</a> (I can haz a more searchable name?).
In short, it appends original request&rsquo;s source and destination addresses to the TCP connection or UDP stream.
As stated in the above document, this will solve the multiple return paths because we are initiating another TCP connection/UDP stream.
Unfortunately, this solution is invasive as it requires the backend service to support PROXY protocol explicitly.
Fortunately we have <a href=https://github.com/cloudflare/mmproxy>mmproxy</a>. It accepts PROXY protocol packets, unwraps them and then forwards them to upstream. Moreover, it does so transparently.
The original mmproxy does not support UDP, while this go implementation <a href=https://github.com/path-network/go-mmproxy>go-mmproxy</a> supports.</p></li></ul><ul><li><p>aioproxy</p><p>mmproxy is great when working with envoy. But it does not multiplex port like sslh, is not transparent, and does not work with non-PROXY protocol traffic.
Non-transparent proxy is useful when we are trying to proxy a connection whose original requester, proxy and the backend server are all the same host (see below).</p><ul><li><p>How transparent proxy works</p><p>Let <code>cip</code> be the client ip, <code>pip</code> be the proxy ip and <code>sip</code> be the backend server IP.</p><ul><li>Client connection: cip:45678 -> sip:22,
client tries to connect to sip:22, but it actually connects to transparent proxy</li><li>Transparent proxy downstream connection: cip:45678 -> pip:44443,
transparent proxy accepts traffic from cip:45678, the traffic originally targeted sip:22 is redirected to pip:44443 by netfilter.</li><li>Transparent proxy upstream connection: pip:45678 -> sip:22,
transparent proxy establish a new connection to sip:22, it changes the socket source address to cip:45678 with the help of IP_TRANSPARENT.</li><li>Backend server connection: cip:45678 -> sip:22,
backend server is fooled by the connection socket address, this connection is actually started from the transparent proxy.
If the transparent proxy stands right in the middle of the return path from the backend server to the client, then the proxy can get the return packet from its upstream connection
and send it to the client on behalf of backend server by its downstream connection.</li></ul></li></ul><ul><li><p>What could go wrong when client and transparent proxy are on the same host</p><p>If client and transparent proxy are on the same host <code>127.0.0.1</code>,
both of them will try to bind <code>127.0.0.1:45678</code>, which would fail with <code>Address Already in Use</code>.</p></li></ul><ul><li><p>What could go wrong when we chain more than one transparent proxy</p><p>On the other hand, if we use the scheme client &lt;-> envoy &lt;-> mmproxy &lt;-> sslh &lt;-> ssh, and when both mmproxy and sslh are configured to proxy
transparently, the same bind error would occur (I have not tried it, I expect it to fail).</p><p>So it is sometimes useful to proxy non-transparently, and it would be great if we can have an all-in-one proxy which can intelligently unwrap PROXY protocol
traffic (when it fails to do so, just treats it as normal traffic and forwards it), supports transparent proxy to upstream and multiplexes port for different protocols.</p><p><a href=https://github.com/contrun/aioproxy>Here</a> is my take on this problem. Aioproxy has rudimentary solutions for all above problems.
There are a few things I intended to add. First, more protocol support for multiplexing. Most outstandingly, peeking into SNI, and forwarding connection accordingly.
Second, as discussed above, it could go wrong when client and transparent proxy is on the same host. We need intelligent transparent forwarding, i.e.
when client and transparent proxy is on the same host, do not use the same client address tuple.
At this point, the aioproxy is abandoned in favor of <a href=https://github.com/mholt/caddy-l4>caddy-l4</a>. Caddy-l4 is not mature enough currently, but it has much greater potential,
as we can use anything caddy already provided.</p></li></ul></li></ul><ul><li><p>envoy+traefik+aioproxy</p><p>This is my current setup. Envoy, traefik and aioproxy are a great match. Client connection to my edge proxy <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/templates/envoy.yaml.j2>is wrapped with PROXY protocol</a> by envoy
and forwarded to traefik. Depending on the packet format, traefik would forward it to HTTP traffic to docker or Kubernetes, other TCP traffic to aioproxy
(this works by setting SNI to rules to match <code>Host("*")</code>, see <a href=https://community.traefik.io/t/routing-ssh-traffic-with-traefik-v2/717>here</a>), the PROXY protocol header is automatically peeled off when possible.
It is not transparent to aioproxy. I don&rsquo;t intend to optimize it for now. In fact, it would be better if I insert aioproxy
in front of traefik, as this way every service is now ignorant of the proxy.
<del>But I didn&rsquo;t implement intelligent transparent proxy mentioned above yet (this is fairly easy, and I am fairly lazy currently).</del> It&rsquo;s now done.
There will be some problem when client and transparent proxy are on the same host, which is a frequent use case for me.</p></li></ul><h3 id=intermission-split-horizon-dns>Intermission: Split Horizon DNS</h3><p>I have a few ways to access my services. When I use my own devices, I can just access my services by overlay networks.
My devices are part of the overlay network. I can access services via a stable address within <code>10.144.0.0/16</code>.
Overlay networks are magic. They automatically select paths for me, e.g. when my two devices are in the same network, they connect each other
using LAN address, otherwise, they connect each other over WAN. Overlay networks can transparently do NAT-PMP/UPNP, punch holes. When one device is behind an impenetrable NAT,
they automatically select a relay. I may want to make part of my services available outside the overlay network. In that case,
access to the services is proxied by two public accessible VPSes. They forward traffic as described above.
The problem is that my VPSes live in Far Far Away. I don&rsquo;t want to travel around the world when I am in the overlay network.
Can my device be intelligent enough to just try the overlay network first, when it fails to do so, use the backup VPSes?
This is a well-known problem of <a href=https://en.wikipedia.org/wiki/Split-horizon%5FDNS>split horizon dns</a>. I have a stable domain name <code>service-a.example.com</code>, I want it to be resolved as <code>10.2.3.4</code> when I am
in the corporate network (or I was using a VPN), otherwise please resolve it to <code>1.2.3.4</code>. Here is a few solutions.
By the way, <a href=https://tailscale.com/blog/sisyphean-dns-client-linux/>this</a> is a great read on this problem.</p><h4 id=hosts>Hosts</h4><p>The easiest and the most abominable solution. The downsides are</p><ul><li>no wildcard support for <a href=https://superuser.com/questions/135595/using-wildcards-in-names-in-windows-hosts-file>Windows</a>, <a href=https://stackoverflow.com/questions/20446930/how-to-put-wildcard-entry-into-etc-hosts>Linux</a></li><li>no flexibility. You can not graceful fallback to another host or easily add another entry</li></ul><h4 id=nsswitch>Nsswitch</h4><p>If you ever use mdns, you may wonder how is <code>abc.local</code> resolved to the host <code>abc</code>. The secret sauce lies in
the following stanza of <code>/etc/nsswitch.conf</code>.</p><pre tabindex=0><code class=language-nil data-lang=nil>hosts:     files mdns_minimal [NOTFOUND=return] mymachines resolve [!UNAVAIL=return] dns mdns myhostname
</code></pre><p>Here, <code>mdns_minimal</code> and <code>mymachine</code> are dynamic libraries used by <a href=https://wiki.archlinux.org/title/Domain%5Fname%5Fresolution#Name%5FService%5FSwitch>NSS</a> to resolve hosts.
They provide the functionality of resolving mdns hosts and machinectl hosts. Theoretically, I can just
write another plugin for nsswitch like <code>mdns_minimal</code>, but nsswitch is also an abomination.
It is glibc only, thus musl-linked and statically linked binaries would fail.
As a matter of fact, <a href=https://wiki.musl-libc.org/future-ideas.html>supporting mdns on musl is a future idea</a>, while <a href=https://github.com/golang/go/issues/10485>golang fallbacks to glibc to resolve hostname</a> when
the hosts entry in nsswitch is too complicated. So it does not worth the effort to fiddle with nsswitch.</p><h4 id=coredns>Coredns</h4><p>I found salvation in coredns. Here is how I resolve a domain name with coredns enriched by <a href=https://github.com/openshift/coredns-mdns>coredns-mdns</a> and <a href=https://github.com/coredns/alternate>coredns-alternate</a>.
The source code to this coredns instance is <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/coredns/main.go>here</a>.</p><pre tabindex=0><code class=language-nil data-lang=nil>.:5355 {
    template IN A mydomain.tld {
      match ^(|[.])(?P&lt;p&gt;.*)\.(?P&lt;s&gt;(?P&lt;h&gt;.*?)\.(?P&lt;d&gt;mydomain.tld)[.])$
      answer &#34;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.&#34;
      fallthrough
    }
    template IN AAAA mydomain.tld {
      match ^(|[.])(?P&lt;p&gt;.*)\.(?P&lt;s&gt;(?P&lt;h&gt;.*?)\.(?P&lt;d&gt;mydomain.tld)[.])$
      answer &#34;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.&#34;
      fallthrough
    }
    mdns mydomain.tld
    alternate original NXDOMAIN,SERVFAIL,REFUSED . 1.0.0.1 8.8.4.4 9.9.9.9 180.76.76.76 223.5.5.5
}
</code></pre><p>The Corefile above does the following things.</p><ul><li>cname <code>*.hostname.mydomain.tld</code> to <code>hostname.mydomain.tld</code></li><li>Let <code>hostname.mydomain.tld</code> be resolved to <code>hostname.local</code> by coredns-mdns</li><li>Anything not matched or not resolved here is forwarded to real world DNS servers</li></ul><p>To resolve <code>hostname.local</code>, I use <a href=https://www.avahi.org/>avahi</a> to <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L824-L863>announce the workstation</a> <code>hostname</code>. This solution is particular elegant,
in the sense that all hosts need only to configure themselves. To use this DNS server for all applications,
I configured systemd-resolved <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L773-L780>here</a>. It is also possible to make other devices in the overlay network
to use this DNS server. I haven&rsquo;t done it yet.</p><h4 id=multicasting>Multicasting</h4><p><a href=https://en.wikipedia.org/wiki/Link-Local%5FMulticast%5FName%5FResolution>LLMNR</a> and <a href=https://en.wikipedia.org/wiki/Multicast%5FDNS>mDNS</a> can be leveraged to resolve hosts, if your VPN support multicasting
(which zerotier supports, while tailscale doesn&rsquo;t support yet). The downside is that,
most resolvers only support single label for LLMNR, and `.local` postfix is required for mDNS.
So you can not easily resolve usual domain like `test.example.com` to host `test`.
The solution is to use coredns as described above.</p><h3 id=l7>L7</h3><p>Now that we can resolve domains to desirable hosts, we can access services directly in the browser.</p><h4 id=tls-certificates-and-termination>TLS Certificates and Termination</h4><p>I use acme with dns-chanlledge. My DNS service provider is cloudflare. From <a href=https://letsencrypt.org/>letsencrypt</a>, I got free wildcard certificates for
<code>*.hostname.mydomain.tld</code>, <code>*.local.mydomain.tld</code>, optionally also some alias domains like <code>*.hub.mydomain.tld</code>.
The certificates are obtained by setting <a href="https://search.nixos.org/options?channel=20.09&from=0&size=50&sort=relevance&query=security.acme">NixOS options security.acme</a>, and are shared between multiple applications.
Currently, TLS is terminated by traefik using above certificates.</p><h4 id=edge-routing>Edge Routing</h4><p>Given TLS termination is not handled by the edge routers, we can only do intelligent routing based <a href=https://en.wikipedia.org/wiki/Server%5FName%5FIndication>SNI</a>.
As far as I can tell, <a href=https://www.envoyproxy.io/docs/envoy/latest/configuration/listeners/network%5Ffilters/sni%5Fdynamic%5Fforward%5Fproxy%5Ffilter>SNI dynamic forward proxy</a> of envoy relies heavily on the DNS server to find out
which backend server to forward traffic. This is less than ideal in my use case, because with the help of systemd-resolved&rsquo;s LLMNR support
(mDNS must be manually enabled for each interface, LLMNR seems to be easier to use), I can use easily resolve hostnames.
All I need is obtaining a new hostname from the original hostname by a simple regex. I choose nginx over envoy to do that.
Here is my nginx configuration.</p><pre tabindex=0><code class=language-nil data-lang=nil>user  nginx;
worker_processes  auto;

error_log  /var/log/nginx/error.log notice;
pid        /var/run/nginx.pid;


events {
    worker_connections  1024;
}

stream {
    log_format format &#39;$remote_addr [$time_iso8601] &#39;
		     &#39;$protocol $status $bytes_sent $bytes_received &#39;
		     &#39;$session_time &#34;$upstream_addr&#34; &#39;
		     &#39;&#34;$upstream_bytes_sent&#34; &#34;$upstream_bytes_received&#34; &#34;$upstream_connect_time&#34;&#39;;
    access_log  /var/log/nginx/access.log format;

    map $ssl_preread_server_name $ssl_backend {
	~^([^.]+\.)*alias\.[^.]+\.[^.]+$   real-server:$server_port;
	~^([^.]+\.)*(?P&lt;my_hostname&gt;[^.]+)\.[^.]+\.[^.]+$   $my_hostname:$server_port;
	default default-server:$server_port;
    }

    map $hostname $backend {
	~^([^.]+\.)*alias\.[^.]+\.[^.]+$   real-server:$server_port;
	~^([^.]+\.)*(?P&lt;my_hostname&gt;[^.]+)\.[^.]+\.[^.]+$   $my_hostname:$server_port;
	default $ssl_backend;
    }

    resolver 127.0.0.53 ipv6=off;

    server {
	listen 0.0.0.0:80 reuseport;
	listen 0.0.0.0:2022 reuseport;
	listen 0.0.0.0:2122 reuseport;
	listen 0.0.0.0:2222 reuseport;
	listen 0.0.0.0:443 reuseport;
	listen 0.0.0.0:4443 reuseport;
	listen 0.0.0.0:4000 reuseport;
	listen 0.0.0.0:5678 reuseport;
	listen 0.0.0.0:8080 reuseport;
	listen 0.0.0.0:80 udp reuseport;
	listen 0.0.0.0:2022 udp reuseport;
	listen 0.0.0.0:2122 udp reuseport;
	listen 0.0.0.0:2222 udp reuseport;
	listen 0.0.0.0:443 udp reuseport;
	listen 0.0.0.0:4443 udp reuseport;
	listen 0.0.0.0:4000 udp reuseport;
	listen 0.0.0.0:5678 udp reuseport;
	listen 0.0.0.0:8080 udp reuseport;
	proxy_pass $backend;
	proxy_protocol on;
	ssl_preread on;
    }
}
</code></pre><p>With this configuration, nginx can both forward http and https traffic based on TLS SNI and http hostname.
<code>$my_hostname</code> here is resolved by systemd-resolved.
I also added some aliases to simplify management of domain name prefixes which do not have a backing hostname.
This is needed as I find no easy way to add an alias to an existing domain (c.f. <a href=https://github.com/systemd/systemd/issues/11976>this issue</a>).
Besides, I didn&rsquo;t find any good LLMNR responder with customizable aliases.
Note this is particularly easy to manage as nginx need not know its serving domain.
I can add more edge proxies as needed. They work the same way. I can also add more backend servers as needed.
All they need to respond to LLMNR requests.</p><h4 id=service-and-routing-registration>Service and Routing Registration</h4><p>Service and router registration is done in a self-organizing way.
I don&rsquo;t use subpath routing rules, as it may require extra work of rewriting paths.
Routing is only matched by <code>Host</code>. All my services have dedicated domains.
Cloudflare provides wildcard DNS resolution. My coredns configuration above also resolves domain names in a wildcard-matching fashion.</p><ul><li><p>Fixed Services and Routings</p><p><a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L933-L1102>Generated from nix expressions</a>. It is a obligation for me to praise how easily nix (a real programming language, albeit a weak one) eliminates boilerplate.
Why is everyone trying to use some half-baked configuration format? Can we have a good language for general configurations? Spoiler alert: <a href=https://dhall-lang.org/>dhall-lang</a>.</p></li></ul><ul><li><p>Docker</p><p>This is managed by traefik with <a href=https://doc.traefik.io/traefik/providers/docker/>docker provider</a>. All I need to do is add a label to the container. Traefik will automatically pick up the label
and set up a routing rule according to the <code>defaultRule</code>. My rule is to use <code>domainprefix</code> label when applicable, otherwise fall back to container name.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix><span style=display:flex><span>providers <span>=</span> {
</span></span><span style=display:flex><span>  docker <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>    defaultRule <span style=color:#666>=</span> getRule
</span></span><span style=display:flex><span>      <span style=color:#4070a0>&#39;&#39;{{ (or (index .Labels &#34;domainprefix&#34;) .Name) | normalize }}&#39;&#39;</span>;
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>}
</span></span></code></pre></div></li></ul><ul><li><p>Kubernetes</p><p>Just <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>the usual Kubernetes ingress</a>. I passed k3s kubeconfig to traefik by systemd environment variable <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1897-L1903>here</a>.
Traefik will automatically apply Kubernetes ingress rules.</p></li></ul><h2 id=deployment>Deployment</h2><p>I currently use nix to manage all my personal devices, <a href=https://www.ansible.com/>ansible</a> to manage all the cloud resources. Most services are managed by nix.
When nix becomes too unwieldy, I resort to Kubernetes.
An ideal setup would be using <a href=https://www.terraform.io/>terraform</a> to provision cloud resources, using nix to manage all services including Kubernetes ones.
This is currently not possible for me because firstly, many resources I used does not have terraform provider. Secondly,
nix currently does not support ad hoc variable assignment like terraform and ansible. It is possible to pass variables from the command line, but it is not pleasant to use.
Thirdly, Kubernetes requires a lot of dedication. Currently nix, can&rsquo;t manage Kubernetes efficiently.</p><h3 id=nix>Nix</h3><p>Nix is a much more declarative, reliable and reproducible way to build infrastructure. <a href=https://talks.cont.run/the-hitchhiker-s-guide-to-nixos/>Here</a> is a short introduction.
In short, building NixOS profiles is like building docker image.
You build a new container image and run a container with that image as base. The container image itself is immutable. When you change your code,
you need to build a new image. When you need some new operating system configuration, you build a new NixOS profile and switch to it.
The best thing about NixOS is that nearly every aspect of the OS is tunable by NixOS options. The knobs are formed by the purely functional, lazy language nix.</p><h3 id=docker>Docker</h3><p>I <a href=https://www.breakds.org/post/declarative-docker-in-NixOS/>manage docker containers declaritively with nix</a>. A typical <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1509-L1712>docker container configuration</a> is</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix><span style=display:flex><span>mkContainer <span style=color:#4070a0>&#34;wallabag&#34;</span> prefs<span style=color:#666>.</span>ociContainers<span style=color:#666>.</span>enableWallabag {
</span></span><span style=display:flex><span>  dependsOn <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;postgresql&#34;</span> ];
</span></span><span style=display:flex><span>  environment <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;SYMFONY__ENV__DOMAIN_NAME&#34;</span> <span style=color:#666>=</span>
</span></span><span style=display:flex><span>      <span style=color:#4070a0>&#34;https://</span><span style=color:#70a0d0;font-style:italic>${</span>prefs<span style=color:#666>.</span>getFullDomainName <span style=color:#4070a0>&#34;wallabag&#34;</span><span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>&#34;</span>;
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>  traefikForwardingPort <span style=color:#666>=</span> <span style=color:#40a070>8978</span>;
</span></span><span style=display:flex><span>  middlewares <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;authelia&#34;</span> ];
</span></span><span style=display:flex><span>  volumes <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;/var/data/wallabag/data:/var/www/wallabag/data&#34;</span>
</span></span><span style=display:flex><span>    <span style=color:#4070a0>&#34;/var/data/wallabag/images:/var/www/wallabag/web/assets/images&#34;</span>
</span></span><span style=display:flex><span>  ];
</span></span><span style=display:flex><span>  environmentFiles <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;/run/secrets/wallabag-env&#34;</span> ];
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p><code>mkContainer</code> is a function to make a new container. If <code>prefs.ociContainers.enableWallabag</code> is true, nix would make a container named
<code>wallabag</code> which depends on the <code>postgresql</code> container and has such such volumes and such such environment variables. The environmentFiles is also
read to set up environment variables. The file <code>/run/secrets/wallabag-env</code> is managed by <a href=https://github.com/Mic92/sops-nix>sops-nix</a> and is version-controlled. I also specified
the middleware <code>authelia</code> for traefik, which means that not everyone is allowed to access this service.</p><h4 id=service-discovery>Service Discovery</h4><p>This is easy. Docker container within the same bridge network can access each other by the container name.</p><h4 id=configmaps-and-secrets>Configmaps and Secrets</h4><p>I use docker command line flag <code>--env</code> and <code>--env-file</code> to pass my configurations as container environment variable.
To mount secrets like Kubernetes, I use docker volume. The secrets are managed by sops-nix, which generate secret files
according to my <code>sops.yaml</code> file.</p><h4 id=init-containers-and-jobs>Init Containers and Jobs</h4><p><a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>Kubernetes init containers</a> are sometimes used to manage pods/services dependencies. For this specific use case, init containers are ugly hacks.
Using systemd to manage container dependency is much more elegant. I only need to specify <code>dependsOn</code> in my nix file, e.g. <code>dependsOn = ["postgresql"];</code> above.
I override the <code>ExecStartPost</code> option for systemd units to do initialization job. Kubernetes jobs are just more containers,
while cronjobs are just containers with systemd timer.</p><h4 id=ingress>Ingress</h4><p>See routing.</p><h3 id=ansible>Ansible</h3><p>As much as I love NixOS, I don&rsquo;t use nix for everything. Nix does not work along with some technologies.
I use ansible for two purposes, first setting up cloud resources (like setting up <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/site.yaml#L19-L41>tailscale</a> and <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/site.yaml#L43-L84>envoy</a>), second managing Kubernetes.
Kubernetes is declarative, but using command line to manage Kubernetes is imperative. I use <a href=https://docs.ansible.com/ansible/latest/collections/community/kubernetes/>community.kubernetes</a>.
One pleasant side effect of using ansible to manage Kubernetes is what I did and what I need to do are well-documented.</p><h3 id=kubernetes>Kubernetes</h3><p>My Kubernetes distribution is k3s (provisioned by nix). Each Kubernetes cluster includes exactly one node for the time being.
There are a few edge cases where I can&rsquo;t simply use nix and docker. <a href=https://jupyterhub.readthedocs.io/en/stable/>Jupyterhub</a> and <a href=https://www.eclipse.org/che/>eclipse che</a> are major ones, as they need to provision cluster resources dynamically,
e.g. they need to spawn new containers on user request. This is doable with vanilla docker spawner for jupyter hub. I don&rsquo;t think Che support this natively.
Using Kubernetes is much preferable.</p><h2 id=security>Security</h2><h3 id=authentication-and-authorization>Authentication and Authorization</h3><h4 id=setup>Setup</h4><p>I use <a href=https://github.com/authelia/authelia>authelia</a> for authentication and authorization. I <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1619>created</a> an <a href=https://doc.traefik.io/traefik/middlewares/forwardauth/>ForwardAuth</a> middleware for traefik, which works like nginx <a href=http://nginx.org/en/docs/http/ngx%5Fhttp%5Fauth%5Frequest%5Fmodule.html>auth_request</a>.
Upon receiving a client request, depending on the routing, traefik may initiate a subrequest to authelia possibly with necessary client credentials,
if authelia is able to authenticate the user and authorize the request, the client request will be forwarded
to the backend service with some extra headers containing client user information.
There is not such thing as authorization yet. It&rsquo;s only me using my services.</p><h4 id=weakness>Weakness</h4><p>Authelia is not satisfactory in many aspects. First, its policy engine is not flexible enough. Second, it requires a lot of boilerplate in
the configuration, e.g. I need to specify many hard-coded base domain <code>hostname-a.mydomain.tld</code> instead of <code>hostname-a</code>. This is not desirable as I have many postfixes, and the configuration is shared.</p><h4 id=strength>Strength</h4><p>What I really like about authelia is its simplicity and easy integration with traefik.</p><h4 id=future>Future</h4><p>I want to use a <a href=https://cloud.google.com/beyondcorp>beyondcorp</a> style <a href=../identity-aware-proxy/>identity-aware proxy</a> with <a href=https://www.openpolicyagent.org/>open policy agent</a> support some other day. The last time I checked <a href=https://www.pomerium.com/>pomerium</a>,
I found envoy was hard to pack and pomerium was too oidc-centric, most of all it did not support ldap or other local user database.</p><h3 id=sso>SSO</h3><p>Authelia just landed <a href=https://github.com/authelia/authelia/issues/189>openid connect support</a>. I haven&rsquo;t tried it yet. One more thing about authelia is that I currently use a single text file as account backend.
I have set up <a href=https://www.openldap.org/>openldap</a> on my machines, but I haven&rsquo;t tried it on authelia yet. I intend to use <a href=https://www.freeipa.org/page/Main%5FPage>freeipa</a> instead (tried container, systemd within the container didn&rsquo;t work), which is much more versatile.</p><h3 id=intrusion-prevention>Intrusion Prevention</h3><p>Because of my distrust to other people&rsquo;s computer, I intentionally made my edge proxy to be as dumb as possible.
There ain&rsquo;t such thing as intrusion detection system yet. Setting up fail2ban is easy, but I need to integrate it with traefik and aioproxy.</p><h2 id=backup>Backup</h2><h3 id=syncoid>syncoid</h3><p>I use <a href=https://github.com/jimsalterjrs/sanoid/>syncoid</a> for on-site backup. Syncoid is basically a
<code>zfs send | zfs receive</code> wrapper. With naive <code>rsync -avzh --process</code>, I can easily encounter database corruption. Thanks to Zfs&rsquo;s hard work, I don&rsquo;t have to worry about this consistency.
Syncoid also works incrementally. Another advantage of this method is that I can easily restore an entire zpool. But it requires a lot of free space, and it may take a while to finish.
I attached an external disk to my main computer.</p><h3 id=restic>restic</h3><p>I use <a href=https://restic.net/>restic</a> for off-site backup.
Of all the incremental backup tools, there are two distinctive features about restic.
First, it supports all <a href=https://rclone.org/>rclone</a> backends, second, I can back up different directories from different hosts to the same endpoint.
Here is my nix configuration.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix><span style=display:flex><span>restic <span>=</span> {
</span></span><span style=display:flex><span>  backups <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>let</span>
</span></span><span style=display:flex><span>    go <span style=color:#666>=</span> name: conf: backend: {
</span></span><span style=display:flex><span>      <span style=color:#4070a0>&#34;</span><span style=color:#70a0d0;font-style:italic>${</span>name<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>-</span><span style=color:#70a0d0;font-style:italic>${</span>backend<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>&#34;</span> <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>	initialize <span style=color:#666>=</span> <span style=color:#60add5>true</span>;
</span></span><span style=display:flex><span>	passwordFile <span style=color:#666>=</span> <span style=color:#4070a0>&#34;/run/secrets/restic-password&#34;</span>;
</span></span><span style=display:flex><span>	repository <span style=color:#666>=</span> <span style=color:#4070a0>&#34;rclone:</span><span style=color:#70a0d0;font-style:italic>${</span>backend<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>:restic&#34;</span>;
</span></span><span style=display:flex><span>	rcloneConfigFile <span style=color:#666>=</span> <span style=color:#4070a0>&#34;/run/secrets/rclone-config&#34;</span>;
</span></span><span style=display:flex><span>	timerConfig <span style=color:#666>=</span> {
</span></span><span style=display:flex><span>	  OnCalendar <span style=color:#666>=</span> <span style=color:#4070a0>&#34;00:05&#34;</span>;
</span></span><span style=display:flex><span>	  RandomizedDelaySec <span style=color:#666>=</span> <span style=color:#4070a0>&#34;5h&#34;</span>;
</span></span><span style=display:flex><span>	};
</span></span><span style=display:flex><span>	pruneOpts <span style=color:#666>=</span> [
</span></span><span style=display:flex><span>	  <span style=color:#4070a0>&#34;--keep-daily 7 --keep-weekly 5 --keep-monthly 12 --keep-yearly 75&#34;</span>
</span></span><span style=display:flex><span>	];
</span></span><span style=display:flex><span>      } <span style=color:#666>//</span> conf;
</span></span><span style=display:flex><span>    };
</span></span><span style=display:flex><span>    mkBackup <span style=color:#666>=</span> name: conf:
</span></span><span style=display:flex><span>      go name conf <span style=color:#4070a0>&#34;backup-primary&#34;</span> <span style=color:#666>//</span> go name conf <span style=color:#4070a0>&#34;backup-secondary&#34;</span>;
</span></span><span style=display:flex><span>  <span style=color:#007020;font-weight:700>in</span> mkBackup <span style=color:#4070a0>&#34;vardata&#34;</span> {
</span></span><span style=display:flex><span>    extraBackupArgs <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;--exclude=postgresql&#34;</span> ];
</span></span><span style=display:flex><span>    paths <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;/var/data&#34;</span> ];
</span></span><span style=display:flex><span>  };
</span></span><span style=display:flex><span>};
</span></span></code></pre></div><p>I back up my data every day to two backend storage.
For some files, I need to manually tune the backup process. For example, to back up postgresql database,
I need to run pg_dump first. This may lock the whole table.</p><h2 id=observablility>Observablility</h2><p>I use grafana, loki, prometheus for observablility. I can&rsquo;t praise enough this squad for its simplicity to set up.
I basically just set up the components separately. They just work. Also, it is a share-nothing architecture, so
in order to achieve high availability, all I need to do is add a new remote write target. For that, I use <a href=https://grafana.com/products/cloud/>grafana cloud</a>.</p><h3 id=metrics>Metrics</h3><p>Prometheus is pull based. It is quite easy to obtain nodes data from node exporter. Besides, almost all services now expose prometheus metrics.
I enabled <a href=https://github.com/contrun/infra/blob/e0141e8d5c8caa54ea9e4dfd5de143928e9f4836/nix/common.nix#L1298-L1516>quite a few prometheus exporters (e.g. systemd, node, postgresql)</a>, whose data are sent both to my local machine and grafana cloud.</p><h3 id=logs>Logs</h3><p>Loki lives up to its promise &ndash; like prometheus, for logs. The data are collected by promtail and sent to my local machine and grafana cloud.
Most of my logs are stored with systemd-journal. It is quite easy to <a href=https://github.com/contrun/infra/blob/e0141e8d5c8caa54ea9e4dfd5de143928e9f4836/nix/common.nix#L1518-L1626>collect them with promtail</a>.</p><h3 id=visualization>Visualization</h3><p>Grafana.</p><h3 id=alerts><span class="org-todo todo TODO">TODO</span> Alerts</h3><p>Alert manager.</p><h2 id=continuous-integration-continuous-delivery>Continuous Integration/Continuous Delivery</h2><h3 id=worker>Worker</h3><p>Github is quite generous for the offer of github actions. The free machines&rsquo; performance is quite good.
It is no wonder that there are many <a href=https://github.blog/2021-04-22-github-actions-update-helping-maintainers-combat-bad-actors/>miners trying to abuse them</a>. As good as github actions, there are two nuisances for my usage.</p><ul><li>disk size. The closure size of my top level system profile easily exceeds the size limit. I need to <a href=https://github.com/contrun/infra/blob/01478dd058f9b273644c8800dcd1bc2bf36408a7/.github/workflows/nix.yml#L104-L156>clean up some packages</a> to get more free disk space.</li></ul><p>Some of my machines&rsquo; profile can be as large as 70G. There is no way for github actions to build a profile that large.</p><ul><li>running time limit. Nix channel updates can invaildate many binary caches. I need to build so many packages that github actions workflow frequently times out.</li></ul><p>I need to manually rerun it, and cache my build artifacts with cachix.</p><h3 id=artifacts-store>Artifacts store</h3><p><a href=https://r13y.com/>Most of nix&rsquo;s builds are reproducible</a>. The nix derivation output path depends on the hashes of the build inputs. Given the same inputs, we can easily
check if there are valid binary caches for the output. I use <a href=https://www.cachix.org/>cachix</a> to cache my builds. Think cachix as a docker container registry.
It is quite straightforward to use <a href=https://github.com/cachix/cachix-action>cachix action</a>. I also set up cachix in my local machines, so that I can use the building results of github actions worker.
It greatly reduces the building time on my local machines.</p><h3 id=deployment>Deployment</h3><p>I use <a href=https://github.com/serokell/deploy-rs>deploy-rs</a> to deploy my nixos configuration to the target machine. deploy-rs reads my <code>flake.nix</code>, builds the profile on the machine running deploy-rs command.
It then copies the profile to target machine via ssh. Depending on my configuration, it may choose to download binary caches from substitutes firstly
(thus reduces time by avoid possible slow ssh connection). It should be noted that deploy-rs build the profile on local machine. This is important for me as
many of my machines are not powerful enough to build a profile quickly. deploy-rs also has elementary sanity check, e.g. automatically rollback to previous
generation of profile if ssh connection didn&rsquo;t come back after switch to the new profile.
The only remaining complication is ssh connectivity.</p><h3 id=node-connectivity>Node Connectivity</h3><p>To establish connectivity from github actions runner to my server, I use <a href=https://github.com/erebe/wstunnel>wstunnel</a>. Well, this time I use port-mapping solution.
Note that wstunnel dig tunnels over websocket. And I have described a lot about how I can access my services over http above.
So it is quite a no-brainer for me to set up a tunnel. All I need to do is running wstunnel in server mode, set up a routing rule for it, and then
<code>ssh -o ProxyCommand="wstunnel --upgradePathPrefix=some-superb-secret-path -L stdio:%h:%p wss://wstunnel.example.com" remote-machine</code>
I keep the routing path <code>some-superb-secret-path</code> secret so that it would be impossible for other people to establish a tunnel to my machine.</p><h3 id=secrets-management>Secrets management</h3><p>One more thing, how to make github actions runner&rsquo;s ssh connection to my machines more secure. I fully agree the sentimental of <a href=https://smallstep.com/blog/use-ssh-certificates/>this article</a>.
We should use ssh certificates as more as possible. The question is now how to securely use ssh certificates. I need a system to automatically issue short-lived
certificates. This system must be fully programmable. <a href=https://smallstep.com/certificates/>Smallstep certificates</a> is not good in terms of programmability. I use <a href=https://www.vaultproject.io/docs/secrets/ssh>Hashicorp Vault ssh secret engine</a> for this.
<a href=https://github.com/contrun/infra/commit/63f4456757cb082279e83ae0e4ec1a0ded6ec227>Here</a> is how I use vault to issue short-lived ssh certificates.</p><h2 id=proxy>Proxy</h2><p>It is a mandate to use a proxy on my machines, as too many websites are blocked in China. I can&rsquo;t tolerate my wallabag instance is unable to access articles on Wikipedia.
I use <a href=https://github.com/Dreamacro/clash>clash</a> and iptables for transparent proxy. <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/dot%5Fbin/executable%5Fclash-redir>Here</a> is the script, and <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L2013-L2107>here</a> is the systemd unit to run the script and update clash configuration.
The source of truth for my clash configuration lies in <a href=https://www.cloudflare.com/products/workers-kv/>cloudflare workers kv</a>. All my machines use the same proxy configuration by periodically downloading a subscription from cloudflare worker.
Although it is straightforward to set up transparent proxy on Linux, There are two complications when I want to proxy docker container traffic transparently.</p><h3 id=transparent-proxy-does-not-work-with-docker-container-in-bridge-network-mode>Transparent proxy does not work with docker container in bridge network mode</h3><p>This is a first world problem. Docker/Kubernetes <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>wants</a> <code>sysctl net.bridge.bridge-nf-call-iptables=1</code>, while libvirt <a href=https://wiki.libvirt.org/page/Net.bridge.bridge-nf-call%5Fand%5Fsysctl.conf>wants</a> <code>sysctl net.bridge.bridge-nf-call-iptables=0</code>.
More explanations can be found <a href=http://ebtables.netfilter.org/misc/brnf-faq.html>here</a>, <a href=https://serverfault.com/questions/963759/docker-breaks-libvirt-bridge-network>here</a> and <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way/issues/561#issue-585446276>here</a>. The following scenery illustrates why docker/Kubernetes insists on enabling <code>bridge-netfilter</code>.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>docker run -it --rm -p 8081:8081 nicolaka/netshoot socat -v -v -d -d tcp-listen:8081,fork exec:cat
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#bb60d5>HOST_IP</span><span style=color:#666>=</span><span style=color:#4070a0>&#34;</span><span style=color:#007020;font-weight:700>$(</span>ip -4 -json addr | jq -r <span style=color:#4070a0>&#39;.[] | .addr_info[] | select(.scope == &#34;global&#34;) | .local&#39;</span> | head -n 1<span style=color:#007020;font-weight:700>)</span><span style=color:#4070a0>&#34;</span>
</span></span><span style=display:flex><span>docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081&#34;</span>
</span></span><span style=display:flex><span>docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081,bind=\$(ip -4 -json addr show dev eth0 | jq -r &#39;.[].addr_info[].local&#39;):8082&#34;</span>
</span></span><span style=display:flex><span>docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081,bind=127.1.0.1:8082&#34;</span>
</span></span></code></pre></div><p>When <code>bridge-netfilter</code> is disabled, the last command would time out, while the other two commands will not.
This kind of hairpinning support is seldom needed on my machine.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>sysctl net.bridge.bridge-nf-call-iptables<span style=color:#666>=</span><span style=color:#40a070>0</span> net.bridge.bridge-nf-call-ip6tables<span style=color:#666>=</span><span style=color:#40a070>0</span> net.bridge.bridge-nf-call-arptables<span style=color:#666>=</span><span style=color:#40a070>0</span>
</span></span></code></pre></div><p>So I disable <code>bridge-netfilter</code>. A further complication is that k3s and docker is so smart as to enable <code>bridge-netfilter</code> on startup.
I <a href=https://github.com/contrun/dotfiles/commit/122bef19579e18fcd9e8ca778a64ec0688b9555f>added</a> a <code>ExecStartPost</code> to disable it.</p><h3 id=transparent-proxy-does-not-work-with-docker-container-when-on-ip-is-missing>Transparent proxy does not work with docker container when on-ip is missing</h3><p>To be more precise, sometimes it does not work. I don&rsquo;t know why. I just banged my head for a few hundreds times and find out <code>--on-ip</code> is a must.</p><div class=highlight><pre tabindex=0 style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell><span style=display:flex><span>iptables -t mangle -A CLASH_EXTERNAL -p tcp -j TPROXY --on-port <span style=color:#40a070>7893</span> --on-ip 127.0.0.1 --tproxy-mark 0x4242/0xffffffff
</span></span></code></pre></div><h3 id=alternatives>Alternatives</h3><p>Oh, dear god, iptables is hard. I wish there is an easier way to transparent proxy.</p><ul><li>TUN</li><li>macvlan virtual machine</li></ul><h2 id=server-management>Server Management</h2><h3 id=wstunnel>wstunnel</h3><p>Adding the following to my ssh config,</p><pre tabindex=0><code class=language-nil data-lang=nil>Host wstunnel.*
    CheckHostIP no
    ProxyCommand wstunnel --upgradePathPrefix=some-superb-secret-path -L stdio:$(echo %h | cut -d. -f2):%p wss://%h.example.com
</code></pre><p>I am now road warrior who can access my servers anytime anywhere.</p><h3 id=ttyd>ttyd</h3><p><a href=https://tsl0922.github.io/ttyd/>ttyd</a> is a web based terminal. I added a route for ttyd, then I can manage my servers through a web browser.</p><h3 id=aioproxy>aioproxy</h3><p>As stated above, aioproxy can multiplex ssh and https on the same port. I only need to open one port in my vps.</p><h2 id=next-step>Next Step</h2><h3 id=kubernetes-after-all>Kubernetes after All?</h3><p>I abandoned my plan of using Kubernetes for all. Currently, I refrain my usage of Kubernetes because first I didn&rsquo;t find a satisfactory workflow
for nix and Kubernetes, second I begin to feel Kubernetes is the new c++.
I sincerely hope I can declaratively manage Kubernetes with nix the way I manage docker and traefik with nix.
I find <a href=https://github.com/xtruder/kubenix/issues/26>integrating kustomize and kubenix</a> interesting, but it is not there yet.
Both nix and Kubernetes are too overwhelming. They require you to go all-in. Nix is my daily driver. It is definitely here to stay.
I need some Kubernetes features like node affinity (jupyter hub requires a faster node) and proxying traffic received from any node.
As I said, Kubernetes is like c++. It is extremely powerful, but it is also extremely complex and can be easily misused.
I partially agree <a href=https://pythonspeed.com/articles/dont-need-kubernetes/>“Let’s use Kubernetes!” Now you have 8 problems</a>. I find also find <a href=https://github.com/oam-dev/kubevela>kubevela</a> to be interesting. I haven&rsquo;t tried it yet.
I hope it lives up to its promise. Also, <a href=https://mrkaran.dev/posts/home-server-nomad/>Nomad</a> looks interesting, it may well suit che and jupyter hub, but they do not support nomad.</p><h3 id=configuration-database>Configuration Database</h3><p>Nix is great. But it is hard for outside world to learn my nix configuration.</p><h3 id=security-hardening>Security Hardening</h3><h3 id=federated-storage>Federated Storage</h3><h3 id=personal-data-warehouse>Personal Data Warehouse</h3><h3 id=accounts--ldap>Accounts (ldap)</h3></div><div class=bl-section><h4>Links to this note</h4><div class=backlinks><ul><li><a href=../self-hosted-services/>self-hosted services</a></li></ul></div></div></div></div></div><script src=https://wiki.cont.run/js/URI.js type=text/javascript></script>
<script src=https://wiki.cont.run/js/page.js type=text/javascript></script>
<script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script>
<script>window.store={"https:\/\/wiki.cont.run\/":{title:"",tags:[],content:"Welcome. If you have any comments, please open an issue in the source repo.\nList of Posts ",url:"https://wiki.cont.run/"},"https:\/\/wiki.cont.run\/search\/":{title:"Search",tags:[],content:"",url:"https://wiki.cont.run/search/"},"https:\/\/wiki.cont.run\/tags\/ansible\/":{title:"ansible",tags:[],content:"",url:"https://wiki.cont.run/tags/ansible/"},"https:\/\/wiki.cont.run\/tags\/async-programming\/":{title:"async-programming",tags:[],content:"",url:"https://wiki.cont.run/tags/async-programming/"},"https:\/\/wiki.cont.run\/categories\/":{title:"Categories",tags:[],content:"",url:"https://wiki.cont.run/categories/"},"https:\/\/wiki.cont.run\/tags\/chrome\/":{title:"chrome",tags:[],content:"",url:"https://wiki.cont.run/tags/chrome/"},"https:\/\/wiki.cont.run\/chrome-download-shelf\/":{title:"chrome download shelf",tags:["chrome","internals"],content:"Chrome download shelf is the widget shown in the buttom when you initiate a new download. This document summarizes how the download shelf is implemented. It is sketchy on the details. For more information, click on the source code or documentation links.\nHow to inspect chrome browser ui components In short, run chrome with chromium --enable-ui-devtools=1234 and open url chrome://inspect/#native-ui. See also UI DevTools Overview and Chromium Desktop UI Debugging Tools and Tips.\nLayout in browser window How to layout a new component The elements of chromium UI are views. A View is a UI element, similar to an HTML DOM element. In order to create new UI componenets, we need to create corresponding views. Take download shelf as an example, the entrypoint to register new browser views is browser_view_layout.cc. Here is how chromium layout download shelf in the main window, where the download_shelf_ is a view created here.\nDownload shelf Implementations There are currently two implementations for the download shelf. One is implemented in c++ with MVC architecture, while the other is implemented with webui using web technologies.\nMVC Architecture Chromium uses the well-known MVC design pattern to layout UI components, although sometimes model, view and controller are not strictly separated.\nHere is a diagram which illustrates how mvc architecture works.\n The download shelf view is the UI that is visiable to the user. The user may interact with the download shelf, which then leverages the controller functions to update the download items.\nFor example, when the user click on the context menu (shown by DownloadShelfWebView::ShowDownloadContextMenu) of a download item on the download shelf. The download shelf view executes this download command by calling DownloadCommands::ExecuteCommand, which in turn calls DownloadUIModel::ExecuteCommand of the DownloadUIModel.\nWhen the download items are modified, the views may be updated according by the controllers. The download item models notify controller for updates using the observer design pattern.\nFor example, DownloadUIController observes download item modifications, the notifications are sent by the download manager core services (DownloadNotificationManager). When a new download item is ready, download core service calls DownloadNotificationManager::OnNewDownloadReady, which then calls DownloadShelfUIControllerDelegate::OnNewDownloadReady, which in turn calls browser-\u0026gt;window()-\u0026gt;GetDownloadShelf()-\u0026gt;AddDownload(std::move(model)) to make the download item show up on the download shelf.\nDownload items There are three data models for the download items.\nDownload UI model DownloadUIModel is an interface to define methods to operate on download items (e.g. pasue, resume downloads), show download item status (e.g. saved file name, downloading status).\nDownload item model DownloadItemModel is just a wrapper around download::DownloadItem, which implements the download UI model interface.\nDownload item download::DownloadItem is the underlying download item implementation. It defines what represents a download item, how to serialize download items, how to store their states, etc.\nMenu and commands To show a simple context menu, we need to inherit ui::SimpleMenuModel::Delegate. A SimpleMenuModel::Delegate subclass needs to define some commands and their actions. Commands correspond to menu items. When a menu item is clicked, the corresponding action is executed.\nWebui download shelf “WebUI” is a term used to loosely describe parts of Chrome\u0026rsquo;s UI implemented with web technologies (i.e. HTML, CSS, JavaScript).\nCommunication Between the Browser and WebUI web page Bi-direction communication by mojo (a high level IPC abstraction).\nData definition and RPC The data structures and rpc interfaces are defined in download_shelf.mojom. It contains what structures represent a download when passing from browser to webpage, and vice versa, and what remote procedure calling does the browser provide. Note that the download items defined here is different from download::DownloadItem. We may need to convert from one to another.\nBrowser side handler Browser side handlers are normally called PageHandler. The download shelf PageHandler defines methods to accept rpc request from the website, execute the relevant methods, and return the results. Examples are opening the downloaded item, removing a download. The interface is here. It is implemented here in the browser. The method calls are delegated to the underlying download models. The webui client calls these methods by the generated mojo apis.\nWeb page side handler Web page side handlers are normally called Handler. They are used by the web page to handled native RPC requests from the browser. When the browser did something, it may want to notify the browser for its effects, e.g. there is a new download, the webui may want to show it on its interface. Here are the download shelf web page handlers. There are implmented in javascript here.\nUI embedding The webui have low barriers to entry, but it may lack some desirable effects. For example, we may need to show a native menu. We need to define a UI embedder to show native context menu. The download shelf define a DownloadShelfUIEmbedder here. When the user right clicks on a download item, he actually sees the context menu of the DownloadShelfContextMenuView which is able to display native context menu.\nWebUI registration To register a chrome:// url for a webui, we need to register it in the GetWebUIFactoryFunction. Here is how download shelf is registered.\n",url:"https://wiki.cont.run/chrome-download-shelf/"},"https:\/\/wiki.cont.run\/tags\/cloud-native\/":{title:"cloud-native",tags:[],content:"",url:"https://wiki.cont.run/tags/cloud-native/"},"https:\/\/wiki.cont.run\/tags\/code-generation\/":{title:"code-generation",tags:[],content:"",url:"https://wiki.cont.run/tags/code-generation/"},"https:\/\/wiki.cont.run\/communicating-sequential-processes\/":{title:"communicating sequential processes",tags:[],content:"This is a review of the book Communicating Sequential Processes by C. A. R. Hoare.\nI\u0026rsquo;d like to quote Dijkstra on how enlightening this book is.\n The most profound reason, however, was keenly felt by those who had seen earlier drafts of his manuscript, which shed with surprising clarity new light on what computing science could—or even should—be. To say or feel that the computing scientist’s main challenge is not to get confused by the complexities of his own making is one thing; it is quite a different matter to discover and show how a strict adherence to the tangible and quite explicit elegance of a few mathematical laws can achieve this lofty goal. It is here that we, the grateful readers, reap to my taste the greatest benefits from the scientific wisdom, the notational intrepidity, and the manipulative agility of Charles Antony Richard Hoare.\n The book I agree with Yufei Zhao on the importance of good-story telling in papers/monographs/textbooks.\nWriting a good math paper should be like telling an engaging story. Who are the characters? What are their drives? Who are the villains? Where are the obstacles? Conflict? Suspense? Climax? Ending? I think Communicating Sequential Processes is an excellent example of how an engaging story can be told in a monograph.\nMotivation The motivation of Communicating Sequential Processes is noble. Hoare stated it in the Chapter Discussion.\n The main objective of my research into communicating processes has been to find the simplest possible mathematical theory with the following desirable properties\n It should describe a wide range of interesting computer applications, from vending machines, through process control and discrete event simulation, to shared-resource operating systems. It should be capable of efficient implementation on a variety of conventional and novel computer architectures, from time-sharing computers through microprocessors to networks of communicating microprocessors. It should provide clear assistance to the programmer in his tasks of specification, design, implementation, verification and validation of complex computer systems.   Let me paraphrase it. CSP aims to present a mathematical theory which is able to model various interesting computer applications. This model is efficiently implementable. Guided with the principles of this theory, computer programmer can easily implement concurrent computer programs, whose correctness can be statically validated and is formally verifiable.\nThis is quite intriguing.\nPresentation The presentation is terse and to the point. Most concepts are start from first principle, which can be overwhelming.\nThe first chapter starts with what a process is and how to define it formally. It was like rethink recursions and integers purely in terms of functions. You wonder how far we can go and will be continuously amazed.\nTODO: finish this.\nComplains  I\u0026rsquo;ve found at least 30 typos in this pdf file.  Real world Message-passing over shared memory What\u0026rsquo;s it? Don\u0026rsquo;t communicate by sharing memory; share memory by communicating.\nWhy is it better?  Shared memory: implicit dependencies, state spill. Message-passing: explicit state transition, better encapsulation. Shared memory is impossible in distributed and heterogeneous environment, see multikernel.  relationship with actors CSP and Actors actually do not differ so much. Occam, a language which closely follows the principles of CSP, can be readily recognized as an actor model implementation. According to Rob Pike,\n Erlang\u0026rsquo;s model (aka actors) is very close to the original, pre-channels CSP ca. 1978. As said, Erlang \u0026ldquo;stems from CSP\u0026rdquo;.\n See also, Communicating sequential processes, Actor model and process calculi history, and Bell Labs and CSP Threads.\nLet us briefly describe the similarity and difference on their common implementations (say akka and go).\nsimilarity  processes: they are all based on lightweight processes (respectively actors and processes). communication: the processes communicate with each other by concurrent queues.  difference   Communication channel (mailboxes) are associated with the actor in actor model, while go channel lives by itself. Early version of CSP is like go channel, while latter version of CSP is like akka actor mailbox. Bundling mailboxes with actors provides better encapsulation.\n  Mailboxes are by default asynchronous, while channels can be used to synchronize.\nSending actor messages is non-blocking. In the CSP model, the sender and receiver can synchronize with each other by a channel, e.g. it is a common practice to use a done channel to notify some task in done in go.\n  An actor requires an identity, while goroutines are anonymous. We can send an actor messages by resolving ActorRefs. This is especially useful in communicating processes across nodes.\n  go\u0026rsquo;s implementation of CSP  Some useful CSP concepts are not implemented.  For instance, pipes (like Unix pipes, but for CSPs), subordinate (like subroutines, but for CSPs)\n Channels are not associated with processes. Formal verification is not used, e.g. formally verifying dead lock free.  ",url:"https://wiki.cont.run/communicating-sequential-processes/"},"https:\/\/wiki.cont.run\/tags\/coroutine\/":{title:"coroutine",tags:[],content:"",url:"https://wiki.cont.run/tags/coroutine/"},"https:\/\/wiki.cont.run\/tags\/debugging\/":{title:"debugging",tags:[],content:"",url:"https://wiki.cont.run/tags/debugging/"},"https:\/\/wiki.cont.run\/debugging-systemd-units\/":{title:"debugging systemd units",tags:[],content:"A systemd unit failed, and it worked great when you run the command manually. What the fuck happend? The running environment of your systemd services is different from your command line. It can be hard to reproduce.\noverride units with tmux as ExecStart Say, I need to debug a restic systemd unit. I have set a few environment variables. I need to access some state directory and cache directory. I need to run a few prestart commands. All these are done with high privilege. Below is the systemd unit.\nsudo systemctl cat restic-backups-sync-backup-primary.service --no-pager # /etc/systemd/system/restic-backups-sync-backup-primary.service [Unit]  [Service] Environment=\u0026#34;LOCALE_ARCHIVE=/nix/store/zyc47fh4nkj9anf3cqb5hd87j2dvj5xv-glibc-locales-2.33-55/lib/locale/locale-archive\u0026#34; Environment=\u0026#34;PATH=/nix/store/inypg9myh5j40ym2qzwcdr1zphwnlns9-openssh-8.8p1/bin:/nix/store/qmn7m3wk8b1v1ljhb2dzyjh41d6ingp6-coreutils-9.0/bin:/nix/store/0xzqirrdxw4h9kr0sq4rp1chad5v8fg9-findutils-4.8.0/bin:/nix/store/vcffj451l0bymy3gzkhb9hs4yk0g9yjm-gnugrep-3.7/bin:/nix/store/d9drqi4daha3f0b6wm5y0fnabbggy1r2-gnused-4.8/bin:/nix/store/zsj8b2bkri3yf2hjwh2v1w9w7v5b58ds-systemd-249.4/bin:/nix/store/inypg9myh5j40ym2qzwcdr1zphwnlns9-openssh-8.8p1/sbin:/nix/store/qmn7m3wk8b1v1ljhb2dzyjh41d6ingp6-coreutils-9.0/sbin:/nix/store/0xzqirrdxw4h9kr0sq4rp1chad5v8fg9-findutils-4.8.0/sbin:/nix/store/vcffj451l0bymy3gzkhb9hs4yk0g9yjm-gnugrep-3.7/sbin:/nix/store/d9drqi4daha3f0b6wm5y0fnabbggy1r2-gnused-4.8/sbin:/nix/store/zsj8b2bkri3yf2hjwh2v1w9w7v5b58ds-systemd-249.4/sbin\u0026#34; Environment=\u0026#34;RCLONE_CONFIG=/run/secrets/rclone-config\u0026#34; Environment=\u0026#34;RESTIC_PASSWORD_FILE=/run/secrets/restic-password\u0026#34; Environment=\u0026#34;RESTIC_REPOSITORY=rclone:backup-primary:restic\u0026#34; Environment=\u0026#34;TZDIR=/nix/store/g0gjppf876jkk2p54v5mg20xgizns662-tzdata-2021c/share/zoneinfo\u0026#34;  X-RestartIfChanged=false   CacheDirectory=restic-backups-sync-backup-primary CacheDirectoryMode=0700 ExecStart=/nix/store/l00kzj65rxnj4q7fj1q85p12z6g83h9q-restic-0.12.1/bin/restic backup --cache-dir=%C/restic-backups-sync-backup-primary -v=3 --exclude-larger-than=500M --exclude=.git --exclude-file=/nix/store/5a9zpgqzqd3gbc13r5m5871pqxwzrw4b-restic-excluded-files /home/e/Sync ExecStart=/nix/store/l00kzj65rxnj4q7fj1q85p12z6g83h9q-restic-0.12.1/bin/restic forget --prune --keep-daily 7 --keep-weekly 5 --keep-monthly 12 --keep-yearly 75 ExecStart=/nix/store/l00kzj65rxnj4q7fj1q85p12z6g83h9q-restic-0.12.1/bin/restic check ExecStartPre=/nix/store/160xjy82nllr70pf7nwj0ppwfksnl6wk-unit-script-restic-backups-sync-backup-primary-pre-start/bin/restic-backups-sync-backup-primary-pre-start RuntimeDirectory=restic-backups-sync-backup-primary Type=oneshot User=root   # /run/systemd/system/restic-backups-sync-backup-primary.service.d/override.conf [Service] ExecStart= ExecStartPre= ExecStart=/nix/store/f5kaj2hrnlc4v99gjlic1jygvlq41x55-system-path/bin/tmux new-session -s debug-systemd-unit -d Type=forking As you can see from the override.conf, we first clear off ExecStart, ExecStartPre, and then start a debug tmux session. We can then attach the created tmux session with tmux attach-session -t debug-systemd-unit. Note the -d argument in ExecStart, it is required for tmux to run in a systemd unit. See this answer.\nsystemd-run Another way is to create a temporary unit with system-run. Depending on the complexity of your task, this can be more tendious or less tendious. Here is an example to run curl with SupplementaryGroups.\nsudo systemd-run -p SupplementaryGroups=\u0026#34;noproxy\u0026#34; --uid $USER --pty --same-dir --wait --collect --service-type=exec curl https://cloudflare-quic.com/b/ip ",url:"https://wiki.cont.run/debugging-systemd-units/"},"https:\/\/wiki.cont.run\/tags\/docker\/":{title:"docker",tags:[],content:"",url:"https://wiki.cont.run/tags/docker/"},"https:\/\/wiki.cont.run\/tags\/emulator\/":{title:"emulator",tags:[],content:"",url:"https://wiki.cont.run/tags/emulator/"},"https:\/\/wiki.cont.run\/tags\/fuchsia\/":{title:"fuchsia",tags:[],content:"",url:"https://wiki.cont.run/tags/fuchsia/"},"https:\/\/wiki.cont.run\/fuchsia-starnix\/":{title:"fuchsia starnix",tags:["emulator","ptrace","debugging","kernel","zircon","starnix","wsl","fuchsia"],content:"Starnix is the code name of a Fuchsia project which proposes to run unmodified Linux programs. This is my take to understand what is needed to do in order for Fuchsia to run Linux programs, and how Linux runs programs itself. The main reference is RFC 0082 from Fuchsia, from which you definitely will benefit more.\nA Tale of Two Alternatives So, you want to run unmodified Linux programs in Fuchsia. You have two choices.\n Mimicking Linux only when crossing the system boundary and running other instructions unmodified on the host Creating a virtual machine which is able to emulate instructions of the binary you want to run on Linux  Coincidentally, the first approach is what WSL 1 takes to run Linux programs, and the second approach is what WSL 2 takes. The first one is an easy choice if you don\u0026rsquo;t need tight system integration. You don\u0026rsquo;t need to differentiate the guest kernel and the applications running on the guest kernel. You know, virtualization is a mature field. All you need to do is port the virtual machine monitor (hypervisor). After that, The case is settled for good.\nAlthough the second way is much lightweight (you don\u0026rsquo;t need a scheduler within another scheduler), it has more stringent requirements. First, you will need the same ISA for the Linux program and the host machine. Second, you need to implement a ton of system interfaces (system calls, or API through system libraries like win32 API). If the upstream syscall interface changed, you need to keep up to date. Third, not only there are many syscalls to port, but also there are many unnamed conventions the Linux binaries expect the running host to satisfy. To name a few, ELF loader, dynamic interpreter, System V interface for process initialization, POSIX API, stdin/stdout conventions.\nTo summarize, it is a great price to pay for tight integration. So why fuchsia choose to implement this? And how does fuchsia implement the POSIX interface. I am not able to answer the first question (fuchsia actually implemented a hypervisor called Machina). As for the second one, follow me patiently.\nA Detour through How Debugger Works Ever wonder how a debugger can stop the execution of a debuggee and inspect the running status of the debuggee, and even change its control flow?\nHere is the pseudocode of a Windows debugger. It is copied from How Windows Debuggers Work.\n// Main User-Mode Debugger Loop // CreateProcess(\u0026#34;target.exe\u0026#34;,..., DEBUG_PROCESS, ...); while (1) {  WaitForDebugEvent(\u0026amp;event, ...);  switch (event)  { 	case ModuleLoad: 	Handle/Ignore; 	break; 	case TerminateProcess: 	Handle/Ignore; 	break; 	case Exception (code breakpoint, single step, etc...): 	Handle/Ignore; 	break;  }  ContinueDebugEvent(...); } The debugger first creates a new process with the flag DEBUG_PROCESS. With this on, the operating system will track the running status of the spawned process, and the debugger process is given special privilege over its child process. Whenever there is some notable event, the OS returns necessary data to the debugger process from WaitForDebugEvent.\nThe debugger can then do whatever it needs to facilitate debugging. For instance, it can not only read the memory pages of the debuggee, but also change the control flow, e.g. jump to another address and execute the instruction there.\nSyscalls and How to Emulate Them in Userspace The moral of the above story is that the operating system normally provides a way for one process to trace and modify the running status of another process. If we can \u0026ldquo;arbitrarily\u0026rdquo; modify the control flow of sub processes, we may be able to run foreign binaries.\nSystem Calls Ultimately, the programs running on an OS interferes with the OS by syscalls (some through an indirection a system library, e.g. Fuchsia uses vDSO, openbsd uses system libc).\nTake read a file as an example, this ultimately attributes to three syscalls,\n Userspace program proposes to open a file in the specified path, the kernel returns a file handle in the form of file descriptor. Userspace program continues on by reading the file descriptor. The kernel writes the data it reads from block devices, and then  writes the bytes to the location the userspace program specified.\n When the userspace program is done, it proposes to close the file descriptor. The kernel releases the related resources.  All the hardware resources is managed and utilized this way (almost, the userspace program can bypass the kernel in some situation). The kernel provides a unified abstraction, the userspace programs utilize this abstraction through the convention of syscalls.\nHow to Make a Syscall Manually See Anatomy of a system call, part 1, Anatomy of a system call, part 2 and The Definitive Guide to Linux System Calls for details.\nThe gist is that programs put the required arguments in the specified register. It then runs instruction int 0x80 to raise a soft interruption. The CPU automatically dispatches this interruption to a registered interruption handler, which is a kernel-space procedure. The kernel space procedure then checks the syscall number and dispatches the call to a specialized handler.\nHow to Intercept Syscalls in Linux In Linux, we can easily trace the syscalls made by a program with strace. strace is able to print out all the syscalls a program has called and all the return codes of those syscalls.\nYou might have wondered how strace can have the ability to inspect syscalls. We need the blessing of Linux kernel to do such thing. In order to obtain such blessing, strace needs to, you might have guessed, make a syscall, ptrace(2). ptrace(2) instructs the kernel to stop the execution of the program upon initializing a syscall. The tracer is then notified to take some actions. In the strace case, strace prints out the syscall arguments, and tells the kernel to continue executing syscalls. Just after the kernel finishes the syscall logic and before returns the control to the tracee, the kernel tells the tracer the return code, thus you can see the syscall returning code with strace.\nHow to Hijack Syscalls in Linux As we have mentioned, the kernel is able to let userspace programs hook into syscalls. In order to fully emulate syscalls, the userspace program needs a few more privileges. For example, some syscalls need to write the result to the memory of the caller, an operation strictly forbidden in normal situation. The kernel needs to grant memory read and write permission to the tracing program. Fortunately, this is also doable with ptrace(2). Well, theoretically this is fantastic. Do we have any real world usage of user space syscalls dispatch? Yes.\nUser-mode Linux  User-mode Linux is an ancient poor man\u0026rsquo;s virtualization on Linux. It use ptrace(2) to implement a Linux on Linux. See User-mode Linux paper and kernel documentation for details.\ngVisor A modern application is gVisor. According to its official website documentation,\n gVisor is an application kernel, written in Go, that implements a substantial portion of the Linux system call interface. It provides an additional layer of isolation between running applications and the host operating system.\n Quite mouthful, isn\u0026rsquo;t it? In gVisor-managed environments, safe syscalls from the applications are passed to the underlying kernel, while dangerous ones are censored by a mediator component called Sentry. Sentry passes the syscalls to the Platform, which emulates real syscalls. gVisor currently supports two platforms, ptrace and kvm. When the emulation is done, the results are delivered to user applications. In this way, gVisor provides greater isolation between applications, which is quite useful in container environment. Google cloud functions use gVisor to harden the system.\nA New Mechanism to Dispatch Syscalls Syscall user dispatch.\nThe Starnix Runner Fuchsia already has the ability to run unmodified Linux binaries. See initial implementation here. The basic idea is already presented. We need a hook mechanism in the kernel to run specific handler when some exceptional events happened. Those kinds of exceptional events are called exceptions in Fuchsia.\n When a thread encounters a fault condition, for example a segfault, execution is paused and the thread enters exception handling. Handlers that have registered to receive these exceptions are notified and given a chance to inspect or correct the condition.\n We now dive into the details.\nHooks in the Kernel As a matter of fact, fuchsia (more precisely, zircon, fuchsia\u0026rsquo;s kernel) provides system APIs through vDSO (which is great for binary compatibility and updatability, see P20 of these slides). When you invoke normal Linux syscalls in Fuchsia, exceptions are raised. Here is how zircon handles syscalls.\n// Stamped out syscall veneer routine for every syscall. Try to maximize shared code by forcing // most of the setup and teardown code into non-inlined preamble and postamble code. template \u0026lt;typename T\u0026gt; inline syscall_result do_syscall(uint64_t syscall_num, uint64_t pc, bool (*valid_pc)(uintptr_t), 	T make_call) {  // Call the shared preamble code  auto pre_ret = do_syscall_pre(syscall_num, pc);  const uintptr_t vdso_code_address = pre_ret.vdso_code_address;  ProcessDispatcher* current_process = pre_ret.current_process;   // Validate the user space program counter originated from the vdso at the proper location,  // otherwise call through to the invalid syscall handler  uint64_t ret;  if (unlikely(!valid_pc(pc - vdso_code_address))) {  ret = sys_invalid_syscall(syscall_num, pc, vdso_code_address);  } else {  // Per syscall inlined routine to marshall args appropriately  ret = make_call(current_process);  }   // Call through to the shared postamble code  return do_syscall_post(ret, syscall_num); } The line ret = sys_invalid_syscall(syscall_num, pc, vdso_code_address) saves the original syscall number, raises an exception. Then the kernel would suspend current thread and notify the registered exception handler.\nHandlers in the Userspace Here is the code snippet copied from fuchsia\u0026rsquo;s starnix runner.\n/// Runs the given task. /// /// The task is expected to already have been started. This function listens to /// the exception channel for the process (`exceptions`) and handles each /// exception by: /// /// - verifying that the exception represents a `ZX_EXCP_POLICY_CODE_BAD_SYSCALL` /// - reading the thread\u0026#39;s registers /// - executing the appropriate syscall /// - setting the thread\u0026#39;s registers to their post-syscall values /// - setting the exception state to `ZX_EXCEPTION_STATE_HANDLED` /// /// Once this function has completed, the process\u0026#39; exit code (if one is available) can be read from /// `process_context.exit_code`. fn run_task(mutcurrent_task: CurrentTask,exceptions: zx::Channel)-\u0026gt; Result\u0026lt;i32,Error\u0026gt;{letmutbuffer=zx::MessageBuf::new();loop{read_channel_sync(\u0026amp;exceptions,\u0026amp;mutbuffer)?;letinfo=as_exception_info(\u0026amp;buffer);assert!(buffer.n_handles()==1);letexception=zx::Exception::from(buffer.take_handle(0).unwrap());ifinfo.type_!=ZX_EXCP_POLICY_ERROR{info!(\u0026#34;exception type: 0x{:x}\u0026#34;,info.type_);exception.set_exception_state(\u0026amp;ZX_EXCEPTION_STATE_TRY_NEXT)?;continue;}letthread=exception.get_thread()?;assert!(thread.get_koid()==current_task.thread.get_koid(),\u0026#34;Exception thread did not match task thread.\u0026#34;);letreport=thread.get_exception_report()?;ifreport.context.synth_code!=ZX_EXCP_POLICY_CODE_BAD_SYSCALL{info!(\u0026#34;exception synth_code: {}\u0026#34;,report.context.synth_code);exception.set_exception_state(\u0026amp;ZX_EXCEPTION_STATE_TRY_NEXT)?;continue;}letsyscall_number=report.context.synth_dataasu64;current_task.registers=thread.read_state_general_regs()?;letregs=\u0026amp;current_task.registers;letargs=(regs.rdi,regs.rsi,regs.rdx,regs.r10,regs.r8,regs.r9);strace!(current_task,\u0026#34;{}({:#x}, {:#x}, {:#x}, {:#x}, {:#x}, {:#x})\u0026#34;,SyscallDecl::from_number(syscall_number).name,args.0,args.1,args.2,args.3,args.4,args.5);matchdispatch_syscall(\u0026amp;mutcurrent_task,syscall_number,args){Ok(SyscallResult::Exit(error_code))=\u0026gt;{strace!(current_task,\u0026#34;-\u0026gt; exit {:#x}\u0026#34;,error_code);exception.set_exception_state(\u0026amp;ZX_EXCEPTION_STATE_THREAD_EXIT)?;returnOk(error_code);}Ok(SyscallResult::Success(return_value))=\u0026gt;{strace!(current_task,\u0026#34;-\u0026gt; {:#x}\u0026#34;,return_value);current_task.registers.rax=return_value;}Ok(SyscallResult::SigReturn)=\u0026gt;{// Do not modify the register state of the thread. The sigreturn syscall has // restored the proper register state for the thread to continue with. strace!(current_task,\u0026#34;-\u0026gt; sigreturn\u0026#34;);}Err(errno)=\u0026gt;{strace!(current_task,\u0026#34;!-\u0026gt; {}\u0026#34;,errno);current_task.registers.rax=(-errno.value())asu64;}}dequeue_signal(\u0026amp;mutcurrent_task);thread.write_state_general_regs(current_task.registers)?;exception.set_exception_state(\u0026amp;ZX_EXCEPTION_STATE_HANDLED)?;}}Sans a few setup work (see ELF loader, dynamic interpreter and process initialization below) and the actual dispatch logic, this is how starnix runs unmodified Linux binaries. The starnix runner first sets up an exception channel. and then runs a loop in which it waits for any message from the exception channel. When the data arrive at this channel, the runner first checks if this message is actually bad syscall exception. If so, the runner acquires the current registers state, then dispatches the original syscall number and its arguments to the user-defined functions. The actually implementations are scattered among different files named syscalls.rs. As an example, here is the link to sendto.\nFor a Few Dollars More Although I have mentioned how starnix intercepts and hijacks normal Linux syscalls. There are still quite a few things omitted for Linux programs running normally.\nMore Syscalls There are quite a few syscalls to reimplement. Linux offers many syscalls, most of which require a reimplementation. Some syscalls like gettimeofday need only stateless shims, while some require starnix to save state internally. For example, you may not want other process to access your file descriptor. When starnix opens a file on the Linux binaries\u0026rsquo; behave, it needs to keep track of the ownership of handles. Some syscalls are performance critical. Any implementation needs careful measurement. Memory access is an example.\nELF Loader and Dynamic Interpreter Programs do not automagically run on a platform. The platform needs to do a few setup work. The first thing it needs to do is load the program from disk to memory. This is what the ELF loader does. The ELF loader for fuchsia is implemented here. To complicate things further, not all programs are self-contained. Some of them require a symbol resolution at runtime. After the program is loaded into memory. Depending on whether the program has a PT_INTERP segment, the runner may run the dynamic interpreter first. The interpreter resolves symbols in the dynamically linked binaries and then jumps to the entry point address (which is available from the auxiliary vector AT_ENTRY, see below) of this program.\nProcess Initialization On Linux, the kernel does a few setup works for the programs which is quite different from the process initialization logic of Fuchsia. For example, the Linux kernel set up the stack for the binaries, and then push some auxiliary vector, environment variables, argv and argc onto the stack (See System V x86 psABIs, How programs get run and How programs get run: ELF binaries for details), while on Fuchsia leverages processargs protocol to pass initial environmental information to the binary. The environmental information may be in a quite different format. Here is the shim to this.\nletstack=stack_base+(stack_size-8);letcreds=current_task.creds.read();letauxv=vec![(AT_UID,creds.uidasu64),(AT_EUID,creds.euidasu64),(AT_GID,creds.gidasu64),(AT_EGID,creds.egidasu64),(AT_BASE,interp_elf.map_or(0,|interp|interp.file_baseasu64)),(AT_PAGESZ,*PAGE_SIZE),(AT_PHDR,main_elf.file_base.wrapping_add(main_elf.headers.file_header().phoff)asu64),(AT_PHNUM,main_elf.headers.file_header().phnumasu64),(AT_ENTRY,main_elf.vaddr_bias.wrapping_add(main_elf.headers.file_header().entry)asu64),(AT_SECURE,0),];letstack=populate_initial_stack(\u0026amp;stack_vmo,argv,environ,auxv,stack_base,stack)?;It is immediately clear that what is populated to the initial stack from the parameter names.\nOther Conventions There are many other implicit conventions Linux programs rely on. For example, if you can\u0026rsquo;t open stdout/stderr on your system, I expect more than 50% of the programs will crash immediately.\n   Posix Compatibility\n Many libraries system(3) Posix threads       Linux Standard Base\n Many libraries Filesystem Hierarchy Standard    Alien Interfaces Some Interfaces are alien to Fuchsia (there are not any counterparts in fuchsia). These are the things which requires more consideration.\n kill Async Signal Linux Futex  Performance Run Rabbit Run Android Given all those hints on Android apex and Android\u0026rsquo;s effort to modularize its system components, I wonder how long it will be till we have Android system components on Fuchsia and when the ART runner will be ready.\n",url:"https://wiki.cont.run/fuchsia-starnix/"},"https:\/\/wiki.cont.run\/identity-aware-proxy\/":{title:"identity-aware proxy",tags:["proxy","zero-trust"],content:"An identity-aware proxy is a reverse proxy which forwards normal payload and attaches the identity information to the backend servers.\nIdentity-aware proxy is an essential part of Google\u0026rsquo;s beyondcorp.\nBeyondcorp To me, there\u0026rsquo;re three key ingredients for beyondcorp.\n Collecting all collectible information about the device and user. Proxying all traffic through an access proxy, instead of setting up a trusted perimeter. Specifying security policy through a unified configuration center and a powerful dsl (cel).  How to obtain identity information? This is implementation-specific. For http, there is a more or less generic method to obtain identity information, i.e. nginx http_auth_request_module and its equivalents. For other protocols, there is no such thing?\nHow to pass identity information?   HTTP is simple. We can easily attach any metadata in the HTTP world just by adding additional header. How do we do that securely? Http header is easy to fabricate. Luckily, there is a standard way to do this, json web token (jwt). JWT consists of three parts, metadata, payload and signature. The payload can be any valid json data. The integrity is ensured by the signature, which can be verified with json web keys (jwks).\n  Life is harder for other tcp/udp services. We have mutual ssl authentication. All we need is a common, trusted certificate authority, and certificates signed by this CA is trusted by all services. We attach the necessary metadata to the certificates.\n  What about L3 solutions? In the brave new Kubernetes world, we have different ip addresses for different pods (using services are more appropriate), i.e. we can identify pods with the ip addresses assigned to them. Some container network interface (cni) (like cilium) implementer did implement identity-awareness in the ip level. I expect they are implemented with something like tailscale to do network traffic, with a full-featured control plane. One thing I am curious is that how do they pass the identity information. Applications normally do interact with stuff that low level.\n  Some implementations Teleport Let\u0026rsquo;s have a look at the architecture of teleport, an open source unified access plane which supports a wide range of applications, including kubernetes, postgresql and http applications.\n  Initiate Client Connection Authenticate Client Connect to Node Authorize Client Access to Node  A notable difference is that instead of connecting directly to the backend service. Teleport has a pool of nodes which acts on the user\u0026rsquo;s behaviour to connect to backend servers. This has a few benefits.\n This evades the problem of backend services not being directly routable from teleport proxy. Loosing coupling between proxy and backend services. Different kind of backend services have different kind of nodes, which can have different kind of auditing logic.  Also note teleport proxy does not push information directly to node service. Node service actively pull information from teleport auth.\nTODO Pomerium TODO Cilium ",url:"https://wiki.cont.run/identity-aware-proxy/"},"https:\/\/wiki.cont.run\/tags\/incremental-backup\/":{title:"incremental-backup",tags:[],content:"",url:"https://wiki.cont.run/tags/incremental-backup/"},"https:\/\/wiki.cont.run\/tags\/infrastructure-as-code\/":{title:"infrastructure-as-code",tags:[],content:"",url:"https://wiki.cont.run/tags/infrastructure-as-code/"},"https:\/\/wiki.cont.run\/tags\/internals\/":{title:"internals",tags:[],content:"",url:"https://wiki.cont.run/tags/internals/"},"https:\/\/wiki.cont.run\/tags\/kernel\/":{title:"kernel",tags:[],content:"",url:"https://wiki.cont.run/tags/kernel/"},"https:\/\/wiki.cont.run\/kernel-development-with-nix\/":{title:"kernel development with nix",tags:["qemu","linux-kernel","nix"],content:"See also Kernel Debugging with QEMU.\nThis nix file should be placed in the root directory of kernel source code.\n{ system ? builtins.currentSystem, configuration ? null , nixpkgs ? import \u0026lt;nixpkgs\u0026gt; { }, extraConfigFile ? \u0026#34;config\u0026#34;, ... }@args: with nixpkgs.pkgs; let  buildLinuxArgs = builtins.removeAttrs args [  \u0026#34;system\u0026#34;  \u0026#34;configuration\u0026#34;  \u0026#34;nixpkgs\u0026#34;  \u0026#34;extraConfigFile\u0026#34;  ];   makeKernelVersion = src:  stdenvNoCC.mkDerivation {  name = \u0026#34;my-kernel-version\u0026#34;;  inherit src;  phases = \u0026#34;installPhase\u0026#34;;  # make kernelversion also works.  installPhase = \u0026#39;\u0026#39; set -x s=\u0026#34;$(\u0026lt; \u0026#34;$src/Makefile\u0026#34;)\u0026#34; get() { awk \u0026#34;/^$1 = / \u0026#34;\u0026#39;{print $3}\u0026#39; \u0026lt;\u0026lt;\u0026lt; \u0026#34;$s\u0026#34; } printf \u0026#39;%s.%s.%s%s\u0026#39; \u0026#34;$(get VERSION)\u0026#34; \u0026#34;$(get PATCHLEVEL)\u0026#34; \u0026#34;$(get SUBLEVEL)\u0026#34; \u0026#34;$(get EXTRAVERSION)\u0026#34; | tee $out \u0026#39;\u0026#39;;  };   getKernelVersion = src: builtins.readFile \u0026#34;${makeKernelVersion src}\u0026#34;;   kernelSrc = let  filter = name: type:  let baseName = builtins.baseNameOf (builtins.toString name);  in lib.cleanSourceFilter name type \u0026amp;\u0026amp; !(baseName == \u0026#34;.ccls-cache\u0026#34; 	|| baseName == extraConfigFile || lib.hasSuffix \u0026#34;.nix\u0026#34; baseName);  in lib.cleanSourceWith {  inherit filter;  src = ./.;  };   kernelVersion = getKernelVersion kernelSrc;   latestConfigFile = linuxPackages_latest.kernel.configfile;   defaultConfigFile = (linuxConfig {  src = kernelSrc;  version = kernelVersion;  }).overrideAttrs ({ prePatch ? \u0026#34;\u0026#34;, ... }: {  prePatch = linuxPackages_latest.kernel.prePatch + prePatch;  });   # We need to merge some `CONFIG_` to make qemu happy.  allConfigFiles = let  p = \u0026#34;${builtins.toPath ./.}/${extraConfigFile}\u0026#34;;  extraConfig = lib.optionals (builtins.pathExists p) [  \u0026#34;${builtins.path { 	name = \u0026#34;extra-kernel-config\u0026#34;; 	path = p;  }}\u0026#34;  ];  in [ defaultConfigFile latestConfigFile ] ++ extraConfig;   mergedConfigFile = (stdenv.mkDerivation {  name = \u0026#34;merged-kernel-config\u0026#34;;  src = kernelSrc;  phases = \u0026#34;unpackPhase prePatchPhase installPhase\u0026#34;;  prePatchPhase = linuxPackages_latest.kernel.prePatch;  # make qemu happy with `CONFIG_EXPERIMENTAL=y`.  installPhase = \u0026#39;\u0026#39; set -x KCONFIG_CONFIG=$out RUNMAKE=false \u0026#34;$src/scripts/kconfig/merge_config.sh\u0026#34; ${ 	builtins.concatStringsSep \u0026#34; \u0026#34; allConfigFiles  }grep -q \u0026#39;^CONFIG_EXPERIMENTAL=\u0026#39; $out \u0026amp;\u0026amp; sed -i \u0026#39;s/^CONFIG_EXPERIMENTAL=.*/CONFIG_EXPERIMENTAL=y/\u0026#39; $out || echo \u0026#39;CONFIG_EXPERIMENTAL=y\u0026#39; \u0026gt;\u0026gt; $out \u0026#39;\u0026#39;;  }).overrideAttrs ({ prePatch ? \u0026#34;\u0026#34;, ... }: {  prePatch = linuxPackages_latest.kernel.prePatch + prePatch;  });   nixosConfiguration = { config, pkgs, ... }: {  imports = [ ] ++ lib.optionals (configuration != null) [ configuration ];   boot.kernelPackages =  # TODO: Does not work yet.  # buildLinux {  # src = kernelSrc;  # version = kernelVersion;  # };  linuxPackages_custom { 	src = kernelSrc; 	version = kernelVersion; 	configfile = \u0026#34;${mergedConfigFile}\u0026#34;;  };   environment = {  enableDebugInfo = true;  etc = let 	getHome = x: builtins.elemAt (builtins.split \u0026#34;:\u0026#34; x) 10; 	entries = builtins.filter (x: x != \u0026#34;\u0026#34; \u0026amp;\u0026amp; x != [ ]) 	(builtins.split \u0026#34;\\n\u0026#34; (builtins.readFile /etc/passwd)); 	homes = builtins.map getHome entries; 	currentFile = \u0026#34;${builtins.toPath ./.}\u0026#34;; 	possibleUserHomes = 	builtins.filter (x: lib.hasPrefix x currentFile) homes; 	keyFiles = builtins.filter (x: builtins.pathExists x) 	(builtins.map (x: \u0026#34;${x}/.ssh/authorized_keys\u0026#34;) possibleUserHomes); 	keys = builtins.concatStringsSep \u0026#34;\\n\u0026#34; 	(builtins.map (x: builtins.readFile x) keyFiles);  in lib.optionalAttrs (keys != \u0026#34;\u0026#34;) { 	\u0026#34;ssh/authorized_keys.d/root\u0026#34; = { 	text = builtins.trace \u0026#34;Added the following keys for ssh access.\\n${keys}\\n\u0026#34; keys; 	mode = \u0026#34;0444\u0026#34;; 	};  };  };   # Generate with mkpasswd -m sha-512 pwFuerRoot  users.users.root.initialHashedPassword = \u0026#34;$6$3zAawH1uhs$dlOiT.ckvpbBQ21tax1J4RI1EGm/1j1HDBoe5u1jy.gHw0QXCKA1dVEwKF.LD0bvzqBu4co.eaZCIK7b2E17k1\u0026#34;;   services.sshd.enable = true;  };   nixos = import (nixpkgs.path + \u0026#34;/nixos/\u0026#34;) {  inherit system;  configuration = nixosConfiguration;  }; in nixos // {  inherit allConfigFiles defaultConfigFile latestConfigFile mergedConfigFile  kernelSrc; } To build a vm image, run it and connect to it with ssh\nnix-build . -A vmWithBootLoader QEMU_OPTS=-nographic QEMU_NET_OPTS=\u0026#34;hostfwd=tcp::2222-:22\u0026#34; ./result/bin/run-*-vm # password is pwFuerRoot ssh -p 2222 root@localhost TODO:\n [] Incremental build  ",url:"https://wiki.cont.run/kernel-development-with-nix/"},"https:\/\/wiki.cont.run\/tags\/kubernetes\/":{title:"kubernetes",tags:[],content:"",url:"https://wiki.cont.run/tags/kubernetes/"},"https:\/\/wiki.cont.run\/tags\/linux-kernel\/":{title:"linux-kernel",tags:[],content:"",url:"https://wiki.cont.run/tags/linux-kernel/"},"https:\/\/wiki.cont.run\/tags\/llvm\/":{title:"llvm",tags:[],content:"",url:"https://wiki.cont.run/tags/llvm/"},"https:\/\/wiki.cont.run\/tags\/load-balancers\/":{title:"load-balancers",tags:[],content:"",url:"https://wiki.cont.run/tags/load-balancers/"},"https:\/\/wiki.cont.run\/lowering-async-await-in-rust\/":{title:"lowering async await in rust",tags:["coroutine","llvm","code-generation","async-programming","rust"],content:"We have a simple program (playground link) built with rust\u0026rsquo;s async/await feature.\nusetokio::time::{sleep,Duration};#[tokio::main]asyncfn main(){sleep(Duration::from_secs(1)).await;}We want to understand all the magic rust compiler did to make this come to life. Here are a few references:\n generators - The Rust Unstable Book 2033-experimental-coroutines - The Rust RFC Book 2394-async_await - The Rust RFC Book Stackless coroutines Generator support Asynchronous Programming in Rust  Below (adapted from here) is roughly how rust compiler compiles the rust source code into machine code.\n We will dive into the code generation process of async/await in a moment.\nHigh level intermediate representations Let\u0026rsquo;s first try to expand all the macros with cargo-expand.\n\u0026gt; cargo expand Checking libc v0.2.112 Checking log v0.4.14 Checking memchr v2.4.1 Checking parking_lot_core v0.8.5 Checking signal-hook-registry v1.4.0 Checking num_cpus v1.13.1 Checking mio v0.7.14 Checking parking_lot v0.11.2 Checking tokio v1.15.0 Checking generator v0.1.0 (/home/by/Workspace/playground/rust/generator) Finished dev [unoptimized + debuginfo] target(s) in 4.48s #![feature(prelude_import)] #[prelude_import] use std::prelude::rust_2021::*; #[macro_use] extern crate std; use tokio::time::{sleep, Duration}; fn main() { let body = async { sleep(Duration::from_secs(1)).await; }; #[allow(clippy::expect_used)] tokio::runtime::Builder::new_multi_thread() .enable_all() .build() .expect(\u0026#34;Failed building the Runtime\u0026#34;) .block_on(body); } We can see that the async main function is replaced with a variable called body. We now have a synchronous main function which stops at a block_on function, whose signature shows that it accepts a future.\nHow does this async {sleep(Duration::from_secs(1)).await;} turn out to be a future?\nLet\u0026rsquo;s go lower, and expand this program to the High-Level Intermediate Representation (HIR). HIR does not actually have canonical text representation. We copy the text representation from the playground. It is slightly edited with some style changes.\n#[prelude_import]usestd::prelude::rust_2021::*;#[macro_use]externcratestd;usetokio::time::{};usetokio::time::sleep;usetokio::time::Duration;fn main(){letbody=#[lang = \u0026#34;from_generator\u0026#34;](|mut_task_context|{match#[lang = \u0026#34;into_future\u0026#34;](sleep(Duration::from_secs(1))){mutpinned=\u0026gt;loop{matchunsafe{#[lang = \u0026#34;poll\u0026#34;](#[lang = \u0026#34;new_unchecked\u0026#34;](\u0026amp;mutpinned),#[lang = \u0026#34;get_context\u0026#34;](_task_context))}{#[lang = \u0026#34;Ready\u0026#34;]{0: result}=\u0026gt;breakresult,#[lang = \u0026#34;Pending\u0026#34;]{}=\u0026gt;{}}_task_context=yield();},};});#[allow(clippy :: expect_used)]tokio::runtime::Builder::new_multi_thread().enable_all().build().expect(\u0026#34;Failed building the Runtime\u0026#34;).block_on(body);}There are quite a few lang_items in this snippet.\nWe view those lang_items as compiler plugins to generate some specific codes (maybe from some specific inputs). For example, the lang_item from_generator is used to generate a future from a generator. We used a few lang_items in our example. Here is a list of all the lang items.\nIn our case, lowering to HIR is basically a combination of expanding async in async fn main { body } and expanding await in future.await, where the body is our async main function, and future is our sleeping task.\nThese two expansion are accomplished by make_async_expr and lower_expr_await.\nmake_async_expr takes an async function or an async block, and converts it to a future. Below is its comment.\nLower an `async` construct to a generator that is then wrapped so it implements `Future`. This results in: ```text std::future::from_generator(static move? |_task_context| -\u0026gt; \u0026lt;ret_ty\u0026gt; { \u0026lt;body\u0026gt; }) ``` lower_expr_await desugar the expression into part of a generator. Below is its comment.\nDesugar`\u0026lt;expr\u0026gt;.await`into: ```rustmatch::std::future::IntoFuture::into_future(\u0026lt;expr\u0026gt;){mutpinned=\u0026gt;loop{matchunsafe{::std::future::Future::poll(\u0026lt;::std::pin::Pin\u0026gt;::new_unchecked(\u0026amp;mutpinned),::std::future::get_context(task_context),)}{::std::task::Poll::Ready(result)=\u0026gt;breakresult,::std::task::Poll::Pending=\u0026gt;{}}task_context=yield();}}```Substitute all the variable values, body is then set to\nstd::future::from_generator(staticmove?|task_context|-\u0026gt; (){match::std::future::IntoFuture::into_future(sleep(Duration::from_secs(1))){mutpinned=\u0026gt;loop{matchunsafe{::std::future::Future::poll(\u0026lt;::std::pin::Pin\u0026gt;::new_unchecked(\u0026amp;mutpinned),::std::future::get_context(task_context),)}{::std::task::Poll::Ready(result)=\u0026gt;breakresult,::std::task::Poll::Pending=\u0026gt;{}}task_context=yield();}}})We will come to the task_context thing in a later point. For now, we are satisfied with the fact that, task_context is passed from the async runtime and it is used by the reactor to notify the executor a future is ready to continue.\nThe argument of from_generator seems to be a closure, but it is a generator. The secret lies in the yield statement.\nGenerator code generation What is this yield thing? We have encountered yield in other languages. Legend has it that in programming languages with cooperative multitasking feature, when one procedure runs to the yielding point it automagically gives up its control of the CPU so that other tasks can continue, and when other procedures yield, it have a chance to continue. But how? Frequently it is implemented with setjmp/longjmp. What about rust? Is it using mechanism like that?\nLet\u0026rsquo;s go lower to Rust\u0026rsquo;s Mid-level Intermediate Representation (MIR) with RUSTFLAGS=\u0026quot;--emit mir\u0026quot; cargo -v run. Below is MIR of the generated coroutine of the async main function (found in the path target/debug/deps/*.mir).\nfn main::{closure#0}(_1: Pin\u0026lt;\u0026amp;mut [static generator@src/main.rs:4:17: 6:2]\u0026gt;, _2: ResumeTy) -\u0026gt; GeneratorState\u0026lt;(), ()\u0026gt; { debug _task_context =\u0026gt; _18; // in scope 0 at src/main.rs:4:17: 6:2 let mut _0: std::ops::GeneratorState\u0026lt;(), ()\u0026gt;; // return place in scope 0 at src/main.rs:4:17: 6:2 let mut _3: tokio::time::Sleep; // in scope 0 at src/main.rs:5:34: 5:40 let mut _4: tokio::time::Sleep; // in scope 0 at src/main.rs:5:5: 5:34 let mut _5: std::time::Duration; // in scope 0 at src/main.rs:5:11: 5:33 let mut _6: std::task::Poll\u0026lt;()\u0026gt;; // in scope 0 at src/main.rs:5:34: 5:40 let mut _7: std::pin::Pin\u0026lt;\u0026amp;mut tokio::time::Sleep\u0026gt;; // in scope 0 at src/main.rs:5:34: 5:40 let mut _8: \u0026amp;mut tokio::time::Sleep; // in scope 0 at src/main.rs:5:34: 5:40 let mut _9: \u0026amp;mut tokio::time::Sleep; // in scope 0 at src/main.rs:5:34: 5:40 let mut _10: \u0026amp;mut std::task::Context; // in scope 0 at src/main.rs:5:34: 5:40 let mut _11: \u0026amp;mut std::task::Context; // in scope 0 at src/main.rs:5:34: 5:40 let mut _12: std::future::ResumeTy; // in scope 0 at src/main.rs:5:34: 5:40 let mut _13: isize; // in scope 0 at src/main.rs:5:34: 5:40 let mut _15: std::future::ResumeTy; // in scope 0 at src/main.rs:5:34: 5:40 let mut _16: (); // in scope 0 at src/main.rs:5:34: 5:40 let mut _17: (); // in scope 0 at src/main.rs:4:17: 6:2 let mut _18: std::future::ResumeTy; // in scope 0 at src/main.rs:4:17: 6:2 let mut _19: u32; // in scope 0 at src/main.rs:4:17: 6:2 scope 1 { debug pinned =\u0026gt; (((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep); // in scope 1 at src/main.rs:5:34: 5:40 let _14: (); // in scope 1 at src/main.rs:5:34: 5:40 scope 2 { } scope 3 { debug result =\u0026gt; _14; // in scope 3 at src/main.rs:5:34: 5:40 } } bb0: { _19 = discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))); // scope 0 at src/main.rs:4:17: 6:2 switchInt(move _19) -\u0026gt; [0_u32: bb1, 1_u32: bb17, 2_u32: bb16, 3_u32: bb15, otherwise: bb18]; // scope 0 at src/main.rs:4:17: 6:2 } bb1: { _18 = move _2; // scope 0 at src/main.rs:4:17: 6:2 _5 = Duration::from_secs(const 1_u64) -\u0026gt; [return: bb2, unwind: bb14]; // scope 0 at src/main.rs:5:11: 5:33 // mir::Constant // + span: src/main.rs:5:11: 5:30 // + literal: Const { ty: fn(u64) -\u0026gt; std::time::Duration {std::time::Duration::from_secs}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb2: { _4 = tokio::time::sleep(move _5) -\u0026gt; [return: bb3, unwind: bb14]; // scope 0 at src/main.rs:5:5: 5:34 // mir::Constant // + span: src/main.rs:5:5: 5:10 // + literal: Const { ty: fn(std::time::Duration) -\u0026gt; tokio::time::Sleep {tokio::time::sleep}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb3: { _3 = \u0026lt;Sleep as IntoFuture\u0026gt;::into_future(move _4) -\u0026gt; [return: bb4, unwind: bb14]; // scope 0 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: fn(tokio::time::Sleep) -\u0026gt; \u0026lt;tokio::time::Sleep as std::future::IntoFuture\u0026gt;::Future {\u0026lt;tokio::time::Sleep as std::future::IntoFuture\u0026gt;::into_future}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb4: { (((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep) = move _3; // scope 0 at src/main.rs:5:34: 5:40 goto -\u0026gt; bb5; // scope 1 at src/main.rs:5:34: 5:40 } bb5: { _9 = \u0026amp;mut (((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep); // scope 2 at src/main.rs:5:34: 5:40 _8 = \u0026amp;mut (*_9); // scope 2 at src/main.rs:5:34: 5:40 _7 = Pin::\u0026lt;\u0026amp;mut Sleep\u0026gt;::new_unchecked(move _8) -\u0026gt; [return: bb6, unwind: bb13]; // scope 2 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: unsafe fn(\u0026amp;mut tokio::time::Sleep) -\u0026gt; std::pin::Pin\u0026lt;\u0026amp;mut tokio::time::Sleep\u0026gt; {std::pin::Pin::\u0026lt;\u0026amp;mut tokio::time::Sleep\u0026gt;::new_unchecked}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb6: { _12 = _18; // scope 2 at src/main.rs:5:34: 5:40 _11 = get_context(move _12) -\u0026gt; [return: bb7, unwind: bb13]; // scope 2 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: unsafe fn(std::future::ResumeTy) -\u0026gt; \u0026amp;mut std::task::Context {std::future::get_context}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb7: { _10 = \u0026amp;mut (*_11); // scope 2 at src/main.rs:5:34: 5:40 _6 = \u0026lt;Sleep as Future\u0026gt;::poll(move _7, move _10) -\u0026gt; [return: bb8, unwind: bb13]; // scope 2 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: for\u0026lt;\u0026#39;r, \u0026#39;s, \u0026#39;t0\u0026gt; fn(std::pin::Pin\u0026lt;\u0026amp;\u0026#39;r mut tokio::time::Sleep\u0026gt;, \u0026amp;\u0026#39;s mut std::task::Context\u0026lt;\u0026#39;t0\u0026gt;) -\u0026gt; std::task::Poll\u0026lt;\u0026lt;tokio::time::Sleep as std::future::Future\u0026gt;::Output\u0026gt; {\u0026lt;tokio::time::Sleep as std::future::Future\u0026gt;::poll}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb8: { _13 = discriminant(_6); // scope 1 at src/main.rs:5:34: 5:40 switchInt(move _13) -\u0026gt; [0_isize: bb11, 1_isize: bb9, otherwise: bb10]; // scope 1 at src/main.rs:5:34: 5:40 } bb9: { ((_0 as Yielded).0: ()) = move _16; // scope 1 at src/main.rs:5:34: 5:40 discriminant(_0) = 0; // scope 1 at src/main.rs:5:34: 5:40 discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))) = 3; // scope 1 at src/main.rs:5:34: 5:40 return; // scope 1 at src/main.rs:5:34: 5:40 } bb10: { unreachable; // scope 1 at src/main.rs:5:34: 5:40 } bb11: { _14 = ((_6 as Ready).0: ()); // scope 1 at src/main.rs:5:34: 5:40 drop((((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep)) -\u0026gt; [return: bb12, unwind: bb14]; // scope 0 at src/main.rs:5:39: 5:40 } bb12: { _17 = const (); // scope 0 at src/main.rs:4:17: 6:2 ((_0 as Complete).0: ()) = move _17; // scope 0 at src/main.rs:6:2: 6:2 discriminant(_0) = 1; // scope 0 at src/main.rs:6:2: 6:2 discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))) = 1; // scope 0 at src/main.rs:6:2: 6:2 return; // scope 0 at src/main.rs:6:2: 6:2 } bb13 (cleanup): { drop((((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep)) -\u0026gt; bb14; // scope 0 at src/main.rs:5:39: 5:40 } bb14 (cleanup): { discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))) = 2; // scope 0 at src/main.rs:4:17: 6:2 resume; // scope 0 at src/main.rs:4:17: 6:2 } bb15: { _15 = move _2; // scope 0 at src/main.rs:4:17: 6:2 _18 = move _15; // scope 1 at src/main.rs:5:34: 5:40 goto -\u0026gt; bb5; // scope 1 at src/main.rs:5:34: 5:40 } bb16: { assert(const false, \u0026#34;`async fn` resumed after panicking\u0026#34;) -\u0026gt; bb16; // scope 0 at src/main.rs:4:17: 6:2 } bb17: { assert(const false, \u0026#34;`async fn` resumed after completion\u0026#34;) -\u0026gt; bb17; // scope 0 at src/main.rs:4:17: 6:2 } bb18: { unreachable; // scope 0 at src/main.rs:4:17: 6:2 } } We can generate a control flow graph of the generated coroutine with RUSTFLAGS=\u0026quot;-Z dump-mir=main -Z dump-mir-graphviz -Z dump-mir-dataflow -Z dump-mir-spanview --emit=mir\u0026quot; cargo -v run.\n The entry point of this generated coroutine is basic block bb0 (the block 0 in the above diagram).\nbb0: { _19 = discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))); // scope 0 at src/main.rs:4:17: 6:2 switchInt(move _19) -\u0026gt; [0_u32: bb1, 1_u32: bb17, 2_u32: bb16, 3_u32: bb15, otherwise: bb18]; // scope 0 at src/main.rs:4:17: 6:2 } bb0 first finds out the current state of the generated generator (variable _1 in the second line). The current state is a enum, whose branches are identified by the discriminants, tags prepended to the actual payload. Below is a llvm itermediate representation to obtain a discriminant.\n%7 = bitcast i64* %_1 to %\u0026#34;[static generator@src/main.rs:4:17: 6:2]\u0026#34;*, !dbg !2665 %8 = getelementptr inbounds %\u0026#34;[static generator@src/main.rs:4:17: 6:2]\u0026#34;, %\u0026#34;[static generator@src/main.rs:4:17: 6:2]\u0026#34;* %7, i32 0, i32 1, !dbg !2665 %9 = load i8, i8* %8, align 128, !dbg !2665, !range !2623 %_19 = zext i8 %9 to i32, !dbg !2665 switch i32 %_19, label %bb18 [ i32 0, label %bb1 i32 1, label %bb17 i32 2, label %bb16 i32 3, label %bb15 ], !dbg !2665 Our program decides jumping to which basic block based on the state\u0026rsquo;s current discriminant. For example, when the discriminant is 0, the program jumps to bb1. Some branch is unreachable because those discriminants are just not possible to have those values (the otherwise branch above). Some states (the 1_u32 and 2_u32 branches above) are malformed. The state 0_u32 means that we just get started. The state 3_u32 means that polling is already started, but the task is not finished yet. When the sleeping task is finished, the state is transitioned to 1_u32.\nLet\u0026rsquo;s look at an exemplary state transition.\nbb6: { _12 = _18; // scope 2 at src/main.rs:5:34: 5:40 _11 = get_context(move _12) -\u0026gt; [return: bb7, unwind: bb13]; // scope 2 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: unsafe fn(std::future::ResumeTy) -\u0026gt; \u0026amp;mut std::task::Context {std::future::get_context}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb7: { _10 = \u0026amp;mut (*_11); // scope 2 at src/main.rs:5:34: 5:40 _6 = \u0026lt;Sleep as Future\u0026gt;::poll(move _7, move _10) -\u0026gt; [return: bb8, unwind: bb13]; // scope 2 at src/main.rs:5:34: 5:40 // mir::Constant // + span: src/main.rs:5:34: 5:40 // + literal: Const { ty: for\u0026lt;\u0026#39;r, \u0026#39;s, \u0026#39;t0\u0026gt; fn(std::pin::Pin\u0026lt;\u0026amp;\u0026#39;r mut tokio::time::Sleep\u0026gt;, \u0026amp;\u0026#39;s mut std::task::Context\u0026lt;\u0026#39;t0\u0026gt;) -\u0026gt; std::task::Poll\u0026lt;\u0026lt;tokio::time::Sleep as std::future::Future\u0026gt;::Output\u0026gt; {\u0026lt;tokio::time::Sleep as std::future::Future\u0026gt;::poll}, val: Value(Scalar(\u0026lt;ZST\u0026gt;)) } } bb8: { _13 = discriminant(_6); // scope 1 at src/main.rs:5:34: 5:40 switchInt(move _13) -\u0026gt; [0_isize: bb11, 1_isize: bb9, otherwise: bb10]; // scope 1 at src/main.rs:5:34: 5:40 } bb9: { ((_0 as Yielded).0: ()) = move _16; // scope 1 at src/main.rs:5:34: 5:40 discriminant(_0) = 0; // scope 1 at src/main.rs:5:34: 5:40 discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))) = 3; // scope 1 at src/main.rs:5:34: 5:40 return; // scope 1 at src/main.rs:5:34: 5:40 } bb11: { _14 = ((_6 as Ready).0: ()); // scope 1 at src/main.rs:5:34: 5:40 drop((((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2])) as variant#3).0: tokio::time::Sleep)) -\u0026gt; [return: bb12, unwind: bb14]; // scope 0 at src/main.rs:5:39: 5:40 } bb12: { _17 = const (); // scope 0 at src/main.rs:4:17: 6:2 ((_0 as Complete).0: ()) = move _17; // scope 0 at src/main.rs:6:2: 6:2 discriminant(_0) = 1; // scope 0 at src/main.rs:6:2: 6:2 discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 6:2]))) = 1; // scope 0 at src/main.rs:6:2: 6:2 return; // scope 0 at src/main.rs:6:2: 6:2 } bb6 and bb7 obtains the result of the poll function of the sleeping future. Depending on whether the sleeping task is finished, the control flow may go from bb8 to bb9 (which sets the state to be 3) or bb11 and bb12 (which sets the state to be 1).\nIn summary, the rust compiler generates a closure which captures the state of the async block. The state transition is driven by repeated execution of this closure. The pausing of a coroutine is just an early return on no final results, while the resumption is just a rerun of the closure.\nTo make this more clear, let\u0026rsquo;s add one more suspension point (playground link).\nusetokio::time::{sleep,Duration};#[tokio::main]asyncfn main(){sleep(Duration::from_secs(1)).await;sleep(Duration::from_secs(1)).await;}This time the entry point has one more branches to go. A new state 4_u32, which represents the time gap between the first future finished and the second future still running, is created.\nbb0: { _33 = discriminant((*(_1.0: \u0026amp;mut [static generator@src/main.rs:4:17: 7:2]))); // scope 0 at src/main.rs:4:17: 7:2 switchInt(move _33) -\u0026gt; [0_u32: bb1, 1_u32: bb30, 2_u32: bb29, 3_u32: bb27, 4_u32: bb28, otherwise: bb31]; // scope 0 at src/main.rs:4:17: 7:2 } Bridging generators to futures One final thing for the rust compiler, the async runtime accepts only futures. Fortunately, it\u0026rsquo;s quite simple to convert a generator to a future.\nThe from_generator function does exactly this.\n/// Wrap a generator in a future. /// /// This function returns a `GenFuture` underneath, but hides it in `impl Trait` to give /// better error messages (`impl Future` rather than `GenFuture\u0026lt;[closure.....]\u0026gt;`). // This is `const` to avoid extra errors after we recover from `const async fn` #[lang = \u0026#34;from_generator\u0026#34;]#[doc(hidden)]#[unstable(feature = \u0026#34;gen_future\u0026#34;, issue = \u0026#34;50547\u0026#34;)]#[rustc_const_unstable(feature = \u0026#34;gen_future\u0026#34;, issue = \u0026#34;50547\u0026#34;)]#[inline]pubconstfn from_generator\u0026lt;T\u0026gt;(gen: T)-\u0026gt; implFuture\u0026lt;Output=T::Return\u0026gt;whereT: Generator\u0026lt;ResumeTy,Yield=()\u0026gt;,{#[rustc_diagnostic_item = \u0026#34;gen_future\u0026#34;]struct GenFuture\u0026lt;T: Generator\u0026lt;ResumeTy,Yield=()\u0026gt;\u0026gt;(T);// We rely on the fact that async/await futures are immovable in order to create // self-referential borrows in the underlying generator. impl\u0026lt;T: Generator\u0026lt;ResumeTy,Yield=()\u0026gt;\u0026gt;!UnpinforGenFuture\u0026lt;T\u0026gt;{}impl\u0026lt;T: Generator\u0026lt;ResumeTy,Yield=()\u0026gt;\u0026gt;FutureforGenFuture\u0026lt;T\u0026gt;{type Output=T::Return;fn poll(self: Pin\u0026lt;\u0026amp;mutSelf\u0026gt;,cx: \u0026amp;mutContext\u0026lt;\u0026#39;_\u0026gt;)-\u0026gt; Poll\u0026lt;Self::Output\u0026gt;{// SAFETY: Safe because we\u0026#39;re !Unpin + !Drop, and this is just a field projection. letgen=unsafe{Pin::map_unchecked_mut(self,|s|\u0026amp;muts.0)};// Resume the generator, turning the `\u0026amp;mut Context` into a `NonNull` raw pointer. The // `.await` lowering will safely cast that back to a `\u0026amp;mut Context`. matchgen.resume(ResumeTy(NonNull::from(cx).cast::\u0026lt;Context\u0026lt;\u0026#39;static\u0026gt;\u0026gt;())){GeneratorState::Yielded(())=\u0026gt;Poll::Pending,GeneratorState::Complete(x)=\u0026gt;Poll::Ready(x),}}}GenFuture(gen)}As we can see, what the poll function for a generator future does is essentially data conversion. One delicate yet crucial matter is the before-mentioned parameter task_context.\nRecall the generated coroutine is something like\n|task_context|-\u0026gt; (){match::std::future::IntoFuture::into_future(sleep(Duration::from_secs(1))){mutpinned=\u0026gt;loop{matchunsafe{::std::future::Future::poll(\u0026lt;::std::pin::Pin\u0026gt;::new_unchecked(\u0026amp;mutpinned),::std::future::get_context(task_context),)}{::std::task::Poll::Ready(result)=\u0026gt;breakresult,::std::task::Poll::Pending=\u0026gt;{}}task_context=yield();}}}The generator takes an argument task_context and in the suspension point we have a peculiar statement task_context = yield (). Who is this task_context and where did it come from?\nIt turns out, task_context is just the argument passed from resume function. To illustrate this, let\u0026rsquo;s generate a generator gen, which is something like\n|closure_args|-\u0026gt; (){do_something();yield_args=yield();}When we run gen.resume(args_1), the generator\u0026rsquo;s closure_args is set to args_1. Then, when we run gen.resume(args_2), the yield_args is set to args_2. So in our case, when the async runtime calls gen.resume(ResumeTy(NonNull::from(cx).cast::\u0026lt;Context\u0026lt;'static\u0026gt;\u0026gt;())) task_context is repeatedly set to ResumeTy(NonNull::from(cx).cast::\u0026lt;Context\u0026lt;'static\u0026gt;\u0026gt;()), which is nothing but a wrapper of cx, a futures::task::Context. In this way, futures inside the generator can inform the executor when they are ready to make progress (see The Future Trait - Asynchronous Programming in Rust for more information).\n",url:"https://wiki.cont.run/lowering-async-await-in-rust/"},"https:\/\/wiki.cont.run\/tags\/mesh-networking\/":{title:"mesh-networking",tags:[],content:"",url:"https://wiki.cont.run/tags/mesh-networking/"},"https:\/\/wiki.cont.run\/tags\/nix\/":{title:"nix",tags:[],content:"",url:"https://wiki.cont.run/tags/nix/"},"https:\/\/wiki.cont.run\/tags\/overlay-networks\/":{title:"overlay-networks",tags:[],content:"",url:"https://wiki.cont.run/tags/overlay-networks/"},"https:\/\/wiki.cont.run\/posts\/":{title:"Posts",tags:[],content:"",url:"https://wiki.cont.run/posts/"},"https:\/\/wiki.cont.run\/tags\/proxy\/":{title:"proxy",tags:[],content:"",url:"https://wiki.cont.run/tags/proxy/"},"https:\/\/wiki.cont.run\/tags\/ptrace\/":{title:"ptrace",tags:[],content:"",url:"https://wiki.cont.run/tags/ptrace/"},"https:\/\/wiki.cont.run\/tags\/qemu\/":{title:"qemu",tags:[],content:"",url:"https://wiki.cont.run/tags/qemu/"},"https:\/\/wiki.cont.run\/quine-in-haskell\/":{title:"quine in haskell",tags:[],content:"See How to make compressed file quines, step by step\n#!/usr/bin/env stack -- stack --resolver lts-15.01 script  module Main where  import Data.List (intercalate)  main :: IO () main = putStr $ (unlines prefix) ++ (getPrefixDef prefix) where  getPrefixDef list = (\u0026#34; prefix =\\n[\\n\u0026#34;) ++ (intercalate \u0026#34;,\\n\u0026#34; (map show list)) ++ \u0026#34;\\n]\u0026#34;  prefix =  [  \u0026#34;#!/usr/bin/env stack\u0026#34;,  \u0026#34;-- stack --resolver lts-15.01 script\u0026#34;,  \u0026#34;\u0026#34;,  \u0026#34;module Main where\u0026#34;,  \u0026#34;\u0026#34;,  \u0026#34;import Data.List (intercalate)\u0026#34;,  \u0026#34;\u0026#34;,  \u0026#34;main :: IO ()\u0026#34;,  \u0026#34;main = putStr $ (unlines prefix) ++ (getPrefixDef prefix) where\u0026#34;,  \u0026#34; getPrefixDef list = (\\\u0026#34;prefix =\\\\n [\\\\n \\\u0026#34;) ++ (intercalate \\\u0026#34;,\\\\n \\\u0026#34;(map show list)) ++ \\\u0026#34;\\\\n ]\\\u0026#34;\u0026#34;  ] ",url:"https://wiki.cont.run/quine-in-haskell/"},"https:\/\/wiki.cont.run\/tags\/rust\/":{title:"rust",tags:[],content:"",url:"https://wiki.cont.run/tags/rust/"},"https:\/\/wiki.cont.run\/self-hosted-infrastructure\/":{title:"self-hosted infrastructure",tags:["incremental-backup","docker","nix","ansible","kubernetes","split-horizon-dns","transparent-proxy","software-defined-networking","infrastructure-as-code","cloud-native","load-balancers","overlay-networks","mesh-networking"],content:"TLDR: I use tailscale/zerotier to establish a smallish mesh network. I use envoy (not anymore, I now use nginx) as an edge router to forward L4 traffic. I mainly provision and manage services with nix, docker and sops. When it is absolutely required, I use k3s to deploy Kubernetes services. Traefik is used for routing, and authelia is used for blocking unauthorized access. To multiplexing protocols with a single port, I use aioproxy. I use syncoid and restic to back up my inevitably accumulated state. For CI/CD, I use github actions, hashicorp vault, depoly-rs and cachix. I use the grafana stack (prometheus, AlertManager, grafana and loki) for observablility.\nLife of a Request  Principles My principles can be best described as cloud nativeness. Cloud-native is an all-encompassing and vague term. I have a few concrete points on my mind.\n Software-defined everything Declarative Infrastructure as code Minimal state maintenance Self-organization Single source of truth  Networking The first obstacle to self-host everything is that you don\u0026rsquo;t have a stable public accessible IP. There are a few solutions.\nThe cloud  I am paranoid enough to not trust the cloud, aka other people\u0026rsquo;s computer. This approach is not cost-efficient. Even my Raspberry PI can beat many VPSes in terms of computing power. Not to mention I can easily insert a 256G SD card. Locality. There is no place like LAN. I see no benefit in downloading youtube video to another VPS.  DDNS This is simplest. I don\u0026rsquo;t use this mainly because it is not reliable in my setup. To name a few problems of DDNS,\n 80, 443, 8080 blocked Not portable router configurations. You need to set up port mapping or DMZ host in your router, which is hard to codify, if not impossible CGNAT ipv6 is still yet to come  Port Forwarding There are many port-forwarding software. To name a few, autossh (my favorite), ngrok, frp, nps. You may also combine DDNS with port forwarding of your router. Here was my attempt to do this. The biggest problem of port forwarding is that it is not scalable and there is no generic inter-node connectivity. Port forwarding has the following weaknesses.\n star topology, single point of failure no inter-node connectivity number of ports are limited, you only have one 443 hard to set up (authorization) no hairpinning support most port forwarding only supports TCP  Overlay Networks Overlay networks are magic. What I meant is not container network interface kind of overlay network, but solutions like zerotier, tailscale, innernet, nebula, n2n. There all have interesting aspects. But none of they are self-organizing. They all require a centralized coordination server. What I have on my mind is something like matrix pinecone. I have been thinking on implementing a pinecone-like overlay network for a while, self-organizing, and tunneling traffic with libp2p. I currently rely on tailscale and zerotier to establish peer-to-peer connectivity. This works great in the following perspectives.\n inter-node connectivity all ports are belong to you easy to set up (implementation-dependent) transparent hole punching transparent multi-path  Routing L3 L3 routing is provided by the overlay network solutions.\nL4 Considerations For L4 routing, I care about transparency, protocol multiplexing and configuration-complexity.\n   Transparency\nThis means that the backend service does not need to know there is a middle man do the heavy lifting. In particular, it means that the origin requester\u0026rsquo;s address is preserved. Typical HTTP reverse proxies are not transparent. They pass the original requester\u0026rsquo;s information by injecting an X-Forwarded-For header.\n     Protocol Multiplexing\nL4 protocol multiplexing means that we can use the same TCP port for HTTP, TLS and SSH. An example is sslh. It normally works by peeking into a few first bytes and determine which protocol this packet is, and then handing off the connection to another application which is listening on some other port.\n     Configuration Complexity\nDo we have to configure both the proxy and backend services? What if we change a user-fronting proxy address? Do the backend server need to adjust for this change? Any special configuration for different user-fronting proxies? What if an upstream server is down? Must I manually edit the configuration to reflect this change?\n  Solutions    iptables\nThis is just like NAT. It is transparent. I believe you can multiplex port with some iptables extensions. It is not super pretty. A lethal problem is that the user-fronting proxy must be in the return path of the connection (usually the proxy is the default gateway). To circumvent this problem, we need some modifications to the routing table and routing policies. When there are two proxies which are connected to the same interface, there are multiple return paths, to select the correct one, we need policy based routing.\n     ipvs\nCompared with iptables, ipvs is much more manageable and scalable. Yet it still is too complicated.\n     usespace L4 proxy\nenvoy/haproxy/nginx etc. can be used as L4 proxy. They accept incoming downstream connection and establish a new upstream connection, just like a pipe. This is much more manageable, the downside is that the original client\u0026rsquo;s information is lost in translation. To ease this problem, haproxy designed a protocol called PROXY (I can haz a more searchable name?). In short, it appends original request\u0026rsquo;s source and destination addresses to the TCP connection or UDP stream. As stated in the above document, this will solve the multiple return paths because we are initiating another TCP connection/UDP stream. Unfortunately, this solution is invasive as it requires the backend service to support PROXY protocol explicitly. Fortunately we have mmproxy. It accepts PROXY protocol packets, unwraps them and then forwards them to upstream. Moreover, it does so transparently. The original mmproxy does not support UDP, while this go implementation go-mmproxy supports.\n     aioproxy\nmmproxy is great when working with envoy. But it does not multiplex port like sslh, is not transparent, and does not work with non-PROXY protocol traffic. Non-transparent proxy is useful when we are trying to proxy a connection whose original requester, proxy and the backend server are all the same host (see below).\n   How transparent proxy works\nLet cip be the client ip, pip be the proxy ip and sip be the backend server IP.\n Client connection: cip:45678 -\u0026gt; sip:22, client tries to connect to sip:22, but it actually connects to transparent proxy Transparent proxy downstream connection: cip:45678 -\u0026gt; pip:44443, transparent proxy accepts traffic from cip:45678, the traffic originally targeted sip:22 is redirected to pip:44443 by netfilter. Transparent proxy upstream connection: pip:45678 -\u0026gt; sip:22, transparent proxy establish a new connection to sip:22, it changes the socket source address to cip:45678 with the help of IP_TRANSPARENT. Backend server connection: cip:45678 -\u0026gt; sip:22, backend server is fooled by the connection socket address, this connection is actually started from the transparent proxy. If the transparent proxy stands right in the middle of the return path from the backend server to the client, then the proxy can get the return packet from its upstream connection and send it to the client on behalf of backend server by its downstream connection.       What could go wrong when client and transparent proxy are on the same host\nIf client and transparent proxy are on the same host 127.0.0.1, both of them will try to bind 127.0.0.1:45678, which would fail with Address Already in Use.\n     What could go wrong when we chain more than one transparent proxy\nOn the other hand, if we use the scheme client \u0026lt;-\u0026gt; envoy \u0026lt;-\u0026gt; mmproxy \u0026lt;-\u0026gt; sslh \u0026lt;-\u0026gt; ssh, and when both mmproxy and sslh are configured to proxy transparently, the same bind error would occur (I have not tried it, I expect it to fail).\nSo it is sometimes useful to proxy non-transparently, and it would be great if we can have an all-in-one proxy which can intelligently unwrap PROXY protocol traffic (when it fails to do so, just treats it as normal traffic and forwards it), supports transparent proxy to upstream and multiplexes port for different protocols.\nHere is my take on this problem. Aioproxy has rudimentary solutions for all above problems. There are a few things I intended to add. First, more protocol support for multiplexing. Most outstandingly, peeking into SNI, and forwarding connection accordingly. Second, as discussed above, it could go wrong when client and transparent proxy is on the same host. We need intelligent transparent forwarding, i.e. when client and transparent proxy is on the same host, do not use the same client address tuple. At this point, the aioproxy is abandoned in favor of caddy-l4. Caddy-l4 is not mature enough currently, but it has much greater potential, as we can use anything caddy already provided.\n       envoy+traefik+aioproxy\nThis is my current setup. Envoy, traefik and aioproxy are a great match. Client connection to my edge proxy is wrapped with PROXY protocol by envoy and forwarded to traefik. Depending on the packet format, traefik would forward it to HTTP traffic to docker or Kubernetes, other TCP traffic to aioproxy (this works by setting SNI to rules to match Host(\u0026quot;*\u0026quot;), see here), the PROXY protocol header is automatically peeled off when possible. It is not transparent to aioproxy. I don\u0026rsquo;t intend to optimize it for now. In fact, it would be better if I insert aioproxy in front of traefik, as this way every service is now ignorant of the proxy. But I didn\u0026rsquo;t implement intelligent transparent proxy mentioned above yet (this is fairly easy, and I am fairly lazy currently). It\u0026rsquo;s now done. There will be some problem when client and transparent proxy are on the same host, which is a frequent use case for me.\n  Intermission: Split Horizon DNS I have a few ways to access my services. When I use my own devices, I can just access my services by overlay networks. My devices are part of the overlay network. I can access services via a stable address within 10.144.0.0/16. Overlay networks are magic. They automatically select paths for me, e.g. when my two devices are in the same network, they connect each other using LAN address, otherwise, they connect each other over WAN. Overlay networks can transparently do NAT-PMP/UPNP, punch holes. When one device is behind an impenetrable NAT, they automatically select a relay. I may want to make part of my services available outside the overlay network. In that case, access to the services is proxied by two public accessible VPSes. They forward traffic as described above. The problem is that my VPSes live in Far Far Away. I don\u0026rsquo;t want to travel around the world when I am in the overlay network. Can my device be intelligent enough to just try the overlay network first, when it fails to do so, use the backup VPSes? This is a well-known problem of split horizon dns. I have a stable domain name service-a.example.com, I want it to be resolved as 10.2.3.4 when I am in the corporate network (or I was using a VPN), otherwise please resolve it to 1.2.3.4. Here is a few solutions. By the way, this is a great read on this problem.\nHosts The easiest and the most abominable solution. The downsides are\n no wildcard support for Windows, Linux no flexibility. You can not graceful fallback to another host or easily add another entry  Nsswitch If you ever use mdns, you may wonder how is abc.local resolved to the host abc. The secret sauce lies in the following stanza of /etc/nsswitch.conf.\nhosts: files mdns_minimal [NOTFOUND=return] mymachines resolve [!UNAVAIL=return] dns mdns myhostname Here, mdns_minimal and mymachine are dynamic libraries used by NSS to resolve hosts. They provide the functionality of resolving mdns hosts and machinectl hosts. Theoretically, I can just write another plugin for nsswitch like mdns_minimal, but nsswitch is also an abomination. It is glibc only, thus musl-linked and statically linked binaries would fail. As a matter of fact, supporting mdns on musl is a future idea, while golang fallbacks to glibc to resolve hostname when the hosts entry in nsswitch is too complicated. So it does not worth the effort to fiddle with nsswitch.\nCoredns I found salvation in coredns. Here is how I resolve a domain name with coredns enriched by coredns-mdns and coredns-alternate. The source code to this coredns instance is here.\n.:5355 { template IN A mydomain.tld { match ^(|[.])(?P\u0026lt;p\u0026gt;.*)\\.(?P\u0026lt;s\u0026gt;(?P\u0026lt;h\u0026gt;.*?)\\.(?P\u0026lt;d\u0026gt;mydomain.tld)[.])$ answer \u0026#34;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.\u0026#34; fallthrough } template IN AAAA mydomain.tld { match ^(|[.])(?P\u0026lt;p\u0026gt;.*)\\.(?P\u0026lt;s\u0026gt;(?P\u0026lt;h\u0026gt;.*?)\\.(?P\u0026lt;d\u0026gt;mydomain.tld)[.])$ answer \u0026#34;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.\u0026#34; fallthrough } mdns mydomain.tld alternate original NXDOMAIN,SERVFAIL,REFUSED . 1.0.0.1 8.8.4.4 9.9.9.9 180.76.76.76 223.5.5.5 } The Corefile above does the following things.\n cname *.hostname.mydomain.tld to hostname.mydomain.tld Let hostname.mydomain.tld be resolved to hostname.local by coredns-mdns Anything not matched or not resolved here is forwarded to real world DNS servers  To resolve hostname.local, I use avahi to announce the workstation hostname. This solution is particular elegant, in the sense that all hosts need only to configure themselves. To use this DNS server for all applications, I configured systemd-resolved here. It is also possible to make other devices in the overlay network to use this DNS server. I haven\u0026rsquo;t done it yet.\nMulticasting LLMNR and mDNS can be leveraged to resolve hosts, if your VPN support multicasting (which zerotier supports, while tailscale doesn\u0026rsquo;t support yet). The downside is that, most resolvers only support single label for LLMNR, and `.local` postfix is required for mDNS. So you can not easily resolve usual domain like `test.example.com` to host `test`. The solution is to use coredns as described above.\nL7 Now that we can resolve domains to desirable hosts, we can access services directly in the browser.\nTLS Certificates and Termination I use acme with dns-chanlledge. My DNS service provider is cloudflare. From letsencrypt, I got free wildcard certificates for *.hostname.mydomain.tld, *.local.mydomain.tld, optionally also some alias domains like *.hub.mydomain.tld. The certificates are obtained by setting NixOS options security.acme, and are shared between multiple applications. Currently, TLS is terminated by traefik using above certificates.\nEdge Routing Given TLS termination is not handled by the edge routers, we can only do intelligent routing based SNI. As far as I can tell, SNI dynamic forward proxy of envoy relies heavily on the DNS server to find out which backend server to forward traffic. This is less than ideal in my use case, because with the help of systemd-resolved\u0026rsquo;s LLMNR support (mDNS must be manually enabled for each interface, LLMNR seems to be easier to use), I can use easily resolve hostnames. All I need is obtaining a new hostname from the original hostname by a simple regex. I choose nginx over envoy to do that. Here is my nginx configuration.\nuser nginx; worker_processes auto; error_log /var/log/nginx/error.log notice; pid /var/run/nginx.pid; events { worker_connections 1024; } stream { log_format format \u0026#39;$remote_addr [$time_iso8601] \u0026#39; \u0026#39;$protocol $status $bytes_sent $bytes_received \u0026#39; \u0026#39;$session_time \u0026#34;$upstream_addr\u0026#34; \u0026#39; \u0026#39;\u0026#34;$upstream_bytes_sent\u0026#34; \u0026#34;$upstream_bytes_received\u0026#34; \u0026#34;$upstream_connect_time\u0026#34;\u0026#39;; access_log /var/log/nginx/access.log format; map $ssl_preread_server_name $ssl_backend { ~^([^.]+\\.)*alias\\.[^.]+\\.[^.]+$ real-server:$server_port; ~^([^.]+\\.)*(?P\u0026lt;my_hostname\u0026gt;[^.]+)\\.[^.]+\\.[^.]+$ $my_hostname:$server_port; default default-server:$server_port; } map $hostname $backend { ~^([^.]+\\.)*alias\\.[^.]+\\.[^.]+$ real-server:$server_port; ~^([^.]+\\.)*(?P\u0026lt;my_hostname\u0026gt;[^.]+)\\.[^.]+\\.[^.]+$ $my_hostname:$server_port; default $ssl_backend; } resolver 127.0.0.53 ipv6=off; server { listen 0.0.0.0:80 reuseport; listen 0.0.0.0:2022 reuseport; listen 0.0.0.0:2122 reuseport; listen 0.0.0.0:2222 reuseport; listen 0.0.0.0:443 reuseport; listen 0.0.0.0:4443 reuseport; listen 0.0.0.0:4000 reuseport; listen 0.0.0.0:5678 reuseport; listen 0.0.0.0:8080 reuseport; listen 0.0.0.0:80 udp reuseport; listen 0.0.0.0:2022 udp reuseport; listen 0.0.0.0:2122 udp reuseport; listen 0.0.0.0:2222 udp reuseport; listen 0.0.0.0:443 udp reuseport; listen 0.0.0.0:4443 udp reuseport; listen 0.0.0.0:4000 udp reuseport; listen 0.0.0.0:5678 udp reuseport; listen 0.0.0.0:8080 udp reuseport; proxy_pass $backend; proxy_protocol on; ssl_preread on; } } With this configuration, nginx can both forward http and https traffic based on TLS SNI and http hostname. $my_hostname here is resolved by systemd-resolved. I also added some aliases to simplify management of domain name prefixes which do not have a backing hostname. This is needed as I find no easy way to add an alias to an existing domain (c.f. this issue). Besides, I didn\u0026rsquo;t find any good LLMNR responder with customizable aliases. Note this is particularly easy to manage as nginx need not know its serving domain. I can add more edge proxies as needed. They work the same way. I can also add more backend servers as needed. All they need to respond to LLMNR requests.\nService and Routing Registration Service and router registration is done in a self-organizing way. I don\u0026rsquo;t use subpath routing rules, as it may require extra work of rewriting paths. Routing is only matched by Host. All my services have dedicated domains. Cloudflare provides wildcard DNS resolution. My coredns configuration above also resolves domain names in a wildcard-matching fashion.\n   Fixed Services and Routings\nGenerated from nix expressions. It is a obligation for me to praise how easily nix (a real programming language, albeit a weak one) eliminates boilerplate. Why is everyone trying to use some half-baked configuration format? Can we have a good language for general configurations? Spoiler alert: dhall-lang.\n     Docker\nThis is managed by traefik with docker provider. All I need to do is add a label to the container. Traefik will automatically pick up the label and set up a routing rule according to the defaultRule. My rule is to use domainprefix label when applicable, otherwise fall back to container name.\nproviders = {  docker = {  defaultRule = getRule  \u0026#39;\u0026#39;{{ (or (index .Labels \u0026#34;domainprefix\u0026#34;) .Name) | normalize }}\u0026#39;\u0026#39;;  }; }      Kubernetes\nJust the usual Kubernetes ingress. I passed k3s kubeconfig to traefik by systemd environment variable here. Traefik will automatically apply Kubernetes ingress rules.\n  Deployment I currently use nix to manage all my personal devices, ansible to manage all the cloud resources. Most services are managed by nix. When nix becomes too unwieldy, I resort to Kubernetes. An ideal setup would be using terraform to provision cloud resources, using nix to manage all services including Kubernetes ones. This is currently not possible for me because firstly, many resources I used does not have terraform provider. Secondly, nix currently does not support ad hoc variable assignment like terraform and ansible. It is possible to pass variables from the command line, but it is not pleasant to use. Thirdly, Kubernetes requires a lot of dedication. Currently nix, can\u0026rsquo;t manage Kubernetes efficiently.\nNix Nix is a much more declarative, reliable and reproducible way to build infrastructure. Here is a short introduction. In short, building NixOS profiles is like building docker image. You build a new container image and run a container with that image as base. The container image itself is immutable. When you change your code, you need to build a new image. When you need some new operating system configuration, you build a new NixOS profile and switch to it. The best thing about NixOS is that nearly every aspect of the OS is tunable by NixOS options. The knobs are formed by the purely functional, lazy language nix.\nDocker I manage docker containers declaritively with nix. A typical docker container configuration is\nmkContainer \u0026#34;wallabag\u0026#34; prefs.ociContainers.enableWallabag {  dependsOn = [ \u0026#34;postgresql\u0026#34; ];  environment = {  \u0026#34;SYMFONY__ENV__DOMAIN_NAME\u0026#34; =  \u0026#34;https://${prefs.getFullDomainName \u0026#34;wallabag\u0026#34;}\u0026#34;;  };  traefikForwardingPort = 8978;  middlewares = [ \u0026#34;authelia\u0026#34; ];  volumes = [  \u0026#34;/var/data/wallabag/data:/var/www/wallabag/data\u0026#34;  \u0026#34;/var/data/wallabag/images:/var/www/wallabag/web/assets/images\u0026#34;  ];  environmentFiles = [ \u0026#34;/run/secrets/wallabag-env\u0026#34; ]; } mkContainer is a function to make a new container. If prefs.ociContainers.enableWallabag is true, nix would make a container named wallabag which depends on the postgresql container and has such such volumes and such such environment variables. The environmentFiles is also read to set up environment variables. The file /run/secrets/wallabag-env is managed by sops-nix and is version-controlled. I also specified the middleware authelia for traefik, which means that not everyone is allowed to access this service.\nService Discovery This is easy. Docker container within the same bridge network can access each other by the container name.\nConfigmaps and Secrets I use docker command line flag --env and --env-file to pass my configurations as container environment variable. To mount secrets like Kubernetes, I use docker volume. The secrets are managed by sops-nix, which generate secret files according to my sops.yaml file.\nInit Containers and Jobs Kubernetes init containers are sometimes used to manage pods/services dependencies. For this specific use case, init containers are ugly hacks. Using systemd to manage container dependency is much more elegant. I only need to specify dependsOn in my nix file, e.g. dependsOn = [\u0026quot;postgresql\u0026quot;]; above. I override the ExecStartPost option for systemd units to do initialization job. Kubernetes jobs are just more containers, while cronjobs are just containers with systemd timer.\nIngress See routing.\nAnsible As much as I love NixOS, I don\u0026rsquo;t use nix for everything. Nix does not work along with some technologies. I use ansible for two purposes, first setting up cloud resources (like setting up tailscale and envoy), second managing Kubernetes. Kubernetes is declarative, but using command line to manage Kubernetes is imperative. I use community.kubernetes. One pleasant side effect of using ansible to manage Kubernetes is what I did and what I need to do are well-documented.\nKubernetes My Kubernetes distribution is k3s (provisioned by nix). Each Kubernetes cluster includes exactly one node for the time being. There are a few edge cases where I can\u0026rsquo;t simply use nix and docker. Jupyterhub and eclipse che are major ones, as they need to provision cluster resources dynamically, e.g. they need to spawn new containers on user request. This is doable with vanilla docker spawner for jupyter hub. I don\u0026rsquo;t think Che support this natively. Using Kubernetes is much preferable.\nSecurity Authentication and Authorization Setup I use authelia for authentication and authorization. I created an ForwardAuth middleware for traefik, which works like nginx auth_request. Upon receiving a client request, depending on the routing, traefik may initiate a subrequest to authelia possibly with necessary client credentials, if authelia is able to authenticate the user and authorize the request, the client request will be forwarded to the backend service with some extra headers containing client user information. There is not such thing as authorization yet. It\u0026rsquo;s only me using my services.\nWeakness Authelia is not satisfactory in many aspects. First, its policy engine is not flexible enough. Second, it requires a lot of boilerplate in the configuration, e.g. I need to specify many hard-coded base domain hostname-a.mydomain.tld instead of hostname-a. This is not desirable as I have many postfixes, and the configuration is shared.\nStrength What I really like about authelia is its simplicity and easy integration with traefik.\nFuture I want to use a beyondcorp style identity-aware proxy with open policy agent support some other day. The last time I checked pomerium, I found envoy was hard to pack and pomerium was too oidc-centric, most of all it did not support ldap or other local user database.\nSSO Authelia just landed openid connect support. I haven\u0026rsquo;t tried it yet. One more thing about authelia is that I currently use a single text file as account backend. I have set up openldap on my machines, but I haven\u0026rsquo;t tried it on authelia yet. I intend to use freeipa instead (tried container, systemd within the container didn\u0026rsquo;t work), which is much more versatile.\nIntrusion Prevention Because of my distrust to other people\u0026rsquo;s computer, I intentionally made my edge proxy to be as dumb as possible. There ain\u0026rsquo;t such thing as intrusion detection system yet. Setting up fail2ban is easy, but I need to integrate it with traefik and aioproxy.\nBackup syncoid I use syncoid for on-site backup. Syncoid is basically a zfs send | zfs receive wrapper. With naive rsync -avzh --process, I can easily encounter database corruption. Thanks to Zfs\u0026rsquo;s hard work, I don\u0026rsquo;t have to worry about this consistency. Syncoid also works incrementally. Another advantage of this method is that I can easily restore an entire zpool. But it requires a lot of free space, and it may take a while to finish. I attached an external disk to my main computer.\nrestic I use restic for off-site backup. Of all the incremental backup tools, there are two distinctive features about restic. First, it supports all rclone backends, second, I can back up different directories from different hosts to the same endpoint. Here is my nix configuration.\nrestic = {  backups = let  go = name: conf: backend: {  \u0026#34;${name}-${backend}\u0026#34; = { 	initialize = true; 	passwordFile = \u0026#34;/run/secrets/restic-password\u0026#34;; 	repository = \u0026#34;rclone:${backend}:restic\u0026#34;; 	rcloneConfigFile = \u0026#34;/run/secrets/rclone-config\u0026#34;; 	timerConfig = { 	OnCalendar = \u0026#34;00:05\u0026#34;; 	RandomizedDelaySec = \u0026#34;5h\u0026#34;; 	}; 	pruneOpts = [ 	\u0026#34;--keep-daily 7 --keep-weekly 5 --keep-monthly 12 --keep-yearly 75\u0026#34; 	];  } // conf;  };  mkBackup = name: conf:  go name conf \u0026#34;backup-primary\u0026#34; // go name conf \u0026#34;backup-secondary\u0026#34;;  in mkBackup \u0026#34;vardata\u0026#34; {  extraBackupArgs = [ \u0026#34;--exclude=postgresql\u0026#34; ];  paths = [ \u0026#34;/var/data\u0026#34; ];  }; }; I back up my data every day to two backend storage. For some files, I need to manually tune the backup process. For example, to back up postgresql database, I need to run pg_dump first. This may lock the whole table.\nObservablility I use grafana, loki, prometheus for observablility. I can\u0026rsquo;t praise enough this squad for its simplicity to set up. I basically just set up the components separately. They just work. Also, it is a share-nothing architecture, so in order to achieve high availability, all I need to do is add a new remote write target. For that, I use grafana cloud.\nMetrics Prometheus is pull based. It is quite easy to obtain nodes data from node exporter. Besides, almost all services now expose prometheus metrics. I enabled quite a few prometheus exporters (e.g. systemd, node, postgresql), whose data are sent both to my local machine and grafana cloud.\nLogs Loki lives up to its promise \u0026ndash; like prometheus, for logs. The data are collected by promtail and sent to my local machine and grafana cloud. Most of my logs are stored with systemd-journal. It is quite easy to collect them with promtail.\nVisualization Grafana.\nTODO Alerts Alert manager.\nContinuous Integration/Continuous Delivery Worker Github is quite generous for the offer of github actions. The free machines\u0026rsquo; performance is quite good. It is no wonder that there are many miners trying to abuse them. As good as github actions, there are two nuisances for my usage.\n disk size. The closure size of my top level system profile easily exceeds the size limit. I need to clean up some packages to get more free disk space.  Some of my machines\u0026rsquo; profile can be as large as 70G. There is no way for github actions to build a profile that large.\n running time limit. Nix channel updates can invaildate many binary caches. I need to build so many packages that github actions workflow frequently times out.  I need to manually rerun it, and cache my build artifacts with cachix.\nArtifacts store Most of nix\u0026rsquo;s builds are reproducible. The nix derivation output path depends on the hashes of the build inputs. Given the same inputs, we can easily check if there are valid binary caches for the output. I use cachix to cache my builds. Think cachix as a docker container registry. It is quite straightforward to use cachix action. I also set up cachix in my local machines, so that I can use the building results of github actions worker. It greatly reduces the building time on my local machines.\nDeployment I use deploy-rs to deploy my nixos configuration to the target machine. deploy-rs reads my flake.nix, builds the profile on the machine running deploy-rs command. It then copies the profile to target machine via ssh. Depending on my configuration, it may choose to download binary caches from substitutes firstly (thus reduces time by avoid possible slow ssh connection). It should be noted that deploy-rs build the profile on local machine. This is important for me as many of my machines are not powerful enough to build a profile quickly. deploy-rs also has elementary sanity check, e.g. automatically rollback to previous generation of profile if ssh connection didn\u0026rsquo;t come back after switch to the new profile. The only remaining complication is ssh connectivity.\nNode Connectivity To establish connectivity from github actions runner to my server, I use wstunnel. Well, this time I use port-mapping solution. Note that wstunnel dig tunnels over websocket. And I have described a lot about how I can access my services over http above. So it is quite a no-brainer for me to set up a tunnel. All I need to do is running wstunnel in server mode, set up a routing rule for it, and then ssh -o ProxyCommand=\u0026quot;wstunnel --upgradePathPrefix=some-superb-secret-path -L stdio:%h:%p wss://wstunnel.example.com\u0026quot; remote-machine I keep the routing path some-superb-secret-path secret so that it would be impossible for other people to establish a tunnel to my machine.\nSecrets management One more thing, how to make github actions runner\u0026rsquo;s ssh connection to my machines more secure. I fully agree the sentimental of this article. We should use ssh certificates as more as possible. The question is now how to securely use ssh certificates. I need a system to automatically issue short-lived certificates. This system must be fully programmable. Smallstep certificates is not good in terms of programmability. I use Hashicorp Vault ssh secret engine for this. Here is how I use vault to issue short-lived ssh certificates.\nProxy It is a mandate to use a proxy on my machines, as too many websites are blocked in China. I can\u0026rsquo;t tolerate my wallabag instance is unable to access articles on Wikipedia. I use clash and iptables for transparent proxy. Here is the script, and here is the systemd unit to run the script and update clash configuration. The source of truth for my clash configuration lies in cloudflare workers kv. All my machines use the same proxy configuration by periodically downloading a subscription from cloudflare worker. Although it is straightforward to set up transparent proxy on Linux, There are two complications when I want to proxy docker container traffic transparently.\nTransparent proxy does not work with docker container in bridge network mode This is a first world problem. Docker/Kubernetes wants sysctl net.bridge.bridge-nf-call-iptables=1, while libvirt wants sysctl net.bridge.bridge-nf-call-iptables=0. More explanations can be found here, here and here. The following scenery illustrates why docker/Kubernetes insists on enabling bridge-netfilter.\ndocker run -it --rm -p 8081:8081 nicolaka/netshoot socat -v -v -d -d tcp-listen:8081,fork exec:cat  HOST_IP=\u0026#34;$(ip -4 -json addr | jq -r \u0026#39;.[] | .addr_info[] | select(.scope == \u0026#34;global\u0026#34;) | .local\u0026#39; | head -n 1)\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081,bind=\\$(ip -4 -json addr show dev eth0 | jq -r \u0026#39;.[].addr_info[].local\u0026#39;):8082\u0026#34; docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c \u0026#34;echo test | socat - tcp:$HOST_IP:8081,bind=127.1.0.1:8082\u0026#34; When bridge-netfilter is disabled, the last command would time out, while the other two commands will not. This kind of hairpinning support is seldom needed on my machine.\nsysctl net.bridge.bridge-nf-call-iptables=0 net.bridge.bridge-nf-call-ip6tables=0 net.bridge.bridge-nf-call-arptables=0 So I disable bridge-netfilter. A further complication is that k3s and docker is so smart as to enable bridge-netfilter on startup. I added a ExecStartPost to disable it.\nTransparent proxy does not work with docker container when on-ip is missing To be more precise, sometimes it does not work. I don\u0026rsquo;t know why. I just banged my head for a few hundreds times and find out --on-ip is a must.\niptables -t mangle -A CLASH_EXTERNAL -p tcp -j TPROXY --on-port 7893 --on-ip 127.0.0.1 --tproxy-mark 0x4242/0xffffffff Alternatives Oh, dear god, iptables is hard. I wish there is an easier way to transparent proxy.\n TUN macvlan virtual machine  Server Management wstunnel Adding the following to my ssh config,\nHost wstunnel.* CheckHostIP no ProxyCommand wstunnel --upgradePathPrefix=some-superb-secret-path -L stdio:$(echo %h | cut -d. -f2):%p wss://%h.example.com I am now road warrior who can access my servers anytime anywhere.\nttyd ttyd is a web based terminal. I added a route for ttyd, then I can manage my servers through a web browser.\naioproxy As stated above, aioproxy can multiplex ssh and https on the same port. I only need to open one port in my vps.\nNext Step Kubernetes after All? I abandoned my plan of using Kubernetes for all. Currently, I refrain my usage of Kubernetes because first I didn\u0026rsquo;t find a satisfactory workflow for nix and Kubernetes, second I begin to feel Kubernetes is the new c++. I sincerely hope I can declaratively manage Kubernetes with nix the way I manage docker and traefik with nix. I find integrating kustomize and kubenix interesting, but it is not there yet. Both nix and Kubernetes are too overwhelming. They require you to go all-in. Nix is my daily driver. It is definitely here to stay. I need some Kubernetes features like node affinity (jupyter hub requires a faster node) and proxying traffic received from any node. As I said, Kubernetes is like c++. It is extremely powerful, but it is also extremely complex and can be easily misused. I partially agree “Let’s use Kubernetes!” Now you have 8 problems. I find also find kubevela to be interesting. I haven\u0026rsquo;t tried it yet. I hope it lives up to its promise. Also, Nomad looks interesting, it may well suit che and jupyter hub, but they do not support nomad.\nConfiguration Database Nix is great. But it is hard for outside world to learn my nix configuration.\nSecurity Hardening Federated Storage Personal Data Warehouse Accounts (ldap) ",url:"https://wiki.cont.run/self-hosted-infrastructure/"},"https:\/\/wiki.cont.run\/self-hosted-services\/":{title:"self-hosted services",tags:[],content:"TODO. For now, See self-hosted infrastructure.\n",url:"https://wiki.cont.run/self-hosted-services/"},"https:\/\/wiki.cont.run\/sigma-bullets\/":{title:"sigma bullets",tags:["zero-knowledge-proofs"],content:"Introduction This document summarize how a zero-knowledge confidential transaction scheme works and how to implement it. A confidential transaction scheme is a transaction scheme with which the transaction value and the balance of the sender and receiver are encrypted. The zero-knowledge part means that outsider can effectively learn nothing about the values, although he can verify the transaction is not fabricated. The main references are Bulletproofs and Zether. We use Zether to homomorphically encrypt the transaction value so that the we can directly add/subtract ciphertext of the encrypted balance which can then be decrypted into the correct balance after the transaction. We use bulletproofs to check the transaction value is valid, i.e. it is a non-negative number within the range \\([0, 2^n)\\), and after the transaction the sender must still have a non-negative balance. The vanilla bulletproofs do not apply to the scenario of zether as Elgamal commitments are not fully homomorphic. We need to tweak bulletproofs to support \\(\\Sigma\\)-protocols, i.e. interactive proof of the values commited in Bulletproofs are truly the values involved in zether, whence we obtain a complete and sound proof of a confidential transaction.\nEncryption and Decryption of Balance From now no, Let \\(G\\) be a group where the discrete logarithm problem is assumed to be hard to solve. Let \\(g, \\overrightarrow{g}=(g_1, g_2, \\cdots), h, \\overrightarrow{h}=(h_1, h_2, \\cdots)\\) be base points of \\(G\\) whose logarithm relationship is unclear.\nLet \\(y_1 = sk_1 * g\\) (resp. \\(y_2 = sk_2 * g\\)) be public key of secret key \\(sk_1\\) (resp. \\(sk_2\\)). Assume the balance of every account is in the interval \\([0, 2^n)\\), where \\(n\\) is a small integer like 32. The ciphertexts obtained from encrypting balance \\(b_1\\) (resp. \\(b_2\\)) with public key \\(y_1\\) (resp. \\(y_2\\)) is \\(c_1 = (b_1 * g + r_1 *y_1, r_1 * g)\\) (resp. \\(c_2 = (b_2 * g + r_2 * y_2, r_2 * g)\\)), where \\(r_1\\) and \\(r_2\\) are random scalars. This is also called ElGamal Commitment.\nUsing the usual ElGamal decryption, we can obtain \\(b_1 * g\\) (resp. \\(b_2 * g\\)) from ciphertext \\(c_1\\) (resp. \\(c_2\\)), i.e. we calculate \\((b_1 * g + r_1 * y_1) - (sk_1 * r_1 * g)\\) which equals \\(b_1 * g\\) by definition of the public key \\(y_1 = sk_1 * g\\). We then obtain \\(b_1\\) (resp. \\(b_2\\)) with brute force. This is feasible as \\(b_1, b_2\\) are small enough.\nConfidential Transfer For the same public key \\(y\\), we define the addition/subtraction of two ciphertexts \\(c_1 = (b_1 * g + r_1 * y, r_1 * g)\\) and \\(c_2 = (b_2 * g + r_2 * y, r_2 * g)\\) as the multiplication/division in the group \\(G^2\\), for example define \\(c_1 + c_2 = ((b_1 + b_2) * g + (r_1+r_2) * y, (r_1+r_2) * g)\\). It is easy to verify the decryption of the resulting ciphertext is indeed the addition/subtraction of corresponding balance.\nThat is to say, the mapping from balance interval to ciphertext homomorphic, we can do the math on ciphertexts which corresponds exactly to the math on balances.\nWe want to make a transaction from account \\(Y\\) to account \\(\\bar{Y}\\), we assume \\(Y\\) initially has balance \\(b\\), he/she wants to transfer \\(b^\\star\\) to \\(\\bar{Y}\\). In the good old bitcoin world. We need only check, it is indeed \\(Y\\) made the transaction, and \\(Y\\) didn\u0026rsquo;t transfer more than what he/she has, i.e. \\(b\\).\nIn the brave new world of cryptopia, we have no way to know what \\(b\\) and \\(b^\\star\\) are, as they are both encrypted.\nProof of knowledge of discrete logarithm Let\u0026rsquo;s summarize what we need to do.\nSuppose \\(Y\\), whose public key is \\(y\\), secret key is \\(sk\\), wants to transfer \\(b^\\star\\) to \\(\\bar{Y}\\), whose public key is \\(\\bar{y}\\), in the end, X has only \\(b^\\prime\\) left in his/her wallet. Our goal is then to prove the following statements.\n The ciphertext \\((C, D)\\) of \\(b^\\star\\) under public key \\(y\\) and random number \\(r\\) is \\((b^\\star * g + r * y, r * g)\\). The ciphertext \\((\\bar{C}, D)\\) of \\(b^\\star\\) under public key \\(\\bar{y}\\) and random number \\(r\\) is \\((b^\\star * g + r * \\bar{y}, r * g)\\). Note we also enforce \\(b^\\star\\) is encrypted under the same random number \\(r\\). \\(C_{n}, D_{n}\\), the amount of money of \\(Y\\) left after the transaction is the ElGamal encryption of \\(b^\\prime\\) under public key \\(y\\), i.e. \\(C_n = b^\\star * g + sk * D_n\\) and \\(sk * g = y\\). Both \\(b^\\star\\) and \\(b^\\prime\\) are within the range \\([0, 2^n)\\).  We can make use of Schnorr\u0026rsquo;s protocol to prove the first three statements.\nIn Schnorr\u0026rsquo;s protocol, the prover wants to prove that he knows \\(x\\) such that \\((x, h)\\) satisfies relationship \\(h = x * g\\) where \\(g\\) is a known element of group \\(G\\), and \\(x\\) is hidden. Schnorr\u0026rsquo;s protocol. First the prover randomly choose a scalar \\(r\\) and send \\(u = r * g\\) to the verifier. The verifier send the randomly chosen challenge \\(c\\) to the prover. The honest prover send \\(v = (c * x + r) * g\\) to the verifier. The verifier outputs \\( v == c*h + u \\).\nWith little changes, we can extend Schnorr\u0026rsquo;s protocol to prove statements like \\(C = b^\\star * g + r * y\\). Now We need only a proof which proves both \\(b^\\star\\) and \\(b^\\prime\\) are within the range \\([0, 2^n)\\), without ever revealing the actual values. In order to do so, we will first commit the value, and then prove properties concerning the commitment.\nPederson Commitment We now provide another way to hide balance which also allow us prove to statements about the hidden balance. Given a value \\(v\\) in the message space, we can commit this value and obtain a output \\(c\\) in the commitment space. \\(c\\) is called the commitment of \\(v\\).\nThere are two properties concerning the security of a commitment scheme, binding and hiding. Informally, a commitment is said to be binding if we can not find two values whose commitment are equal, a commitment is said to be hiding is we can not discern two values from each other. If in addition to this two properties, the commitment scheme is homomorphic, then we translate statements from message space to commitment space. This is quite useful for us to prove properties of hidden values.\nOne of the hiding, binding and homomorphic commitment schemes is Pederson commitment. The Pederson commitment of \\((b, r) \\in (\\mathbb{Z}, \\mathbb{Z})\\) is defined to be the function \\(PC: (b, r) \\mapsto b*g + r*h\\) where \\(g\\) and \\(h\\) are fixed base points, \\(r\\) is called the blinding factor of \\(b\\). Note that in Pederson Commitment \\(h\\) is fixed. We can easily verify Pedenson commitment is a homomorphic commitment scheme, i.e. \\(\\forall b_1, r_1, b_2, r_2\\), we have \\(PC(b_1+b_2, r_1+r_2) = PC(b_1, r_1) + PC(b_2, r_2)\\).\nWe generalize Pederson Commitment to vectors. Let \\((\\overrightarrow{a_L}, \\overrightarrow{a_R}) \\in (\\mathbb{Z}^n, \\mathbb{Z}^n)\\), we define the Pederson commitment to be the function \\(PC: (\\overrightarrow{a_L}, \\overrightarrow{a_R}, r) \\mapsto \\sum a_{L_i} * g_i + \\sum a_{R_i} * h_i + rh\\) where \\(h\\), \\(g_i\\) and \\(h_i\\) are fixed base points, \\(r\\) is called the blinding factor of \\((\\overrightarrow{a_L}, \\overrightarrow{a_R})\\).\nZen of Range Checking Instead of proving \\(a\\) is within the range \\([0, 2^n)\\) directly. We prove the following equivalent equation.\n\n\\[a - \\sum_{i=1}^{n} a_{L_i} \\times 2^{i-1} = 0 \\text{ (eqn:1)}\\]\n\n\\[a_{L_i} - 1 - a_{R_i} = 0, \\forall i = 1, \\cdots, n \\text{ (eqn:2)}\\]\n\n\\[a_{R_i} * a_{L_i} = 0, \\forall i = 1, \\cdots, n \\text{ (eqn:3)}\\]\nCombining 1 and 1, we have \\((a_{L_i} - 1) * a_{L_i} = 0\\), i.e. \\(a_{L_i} = 1\\) or \\(a_{L_i} = 0\\). Together with the first equation, we can see that \\(a_{L_i}\\) is the binary representation of \\(a\\). As we have only \\(n\\) \\(a_{L_i}\\), \\(a\\) is indeed within the range \\([0, 2^n)\\).\nDenote \\(\\overrightarrow{y_n}\\), or simply \\(\\overrightarrow{y}\\) when \\(n\\) is clear, \\((1, y, \\cdots, y^{n-1})\\), \\(overrightarrow{a_L} = (a_{L_1}, a_{L_2}, \\cdots, a_{L_n})\\), \\(\\overrightarrow{a_R} = (a_{R_1}, a_{R_2}, \\cdots, a_{R_n})\\). Let \\(X\\), \\(Y\\) be two vector in \\(\\mathbb{Z}^n\\), we denote the Euclid inner product \\(X\\cdot Y\\), the Hermitian product \\(X \\circ Y\\).\nTo verify the second equation, the verifier makes a challenge, a random scalar \\(y\\), to the prove. The prover proves that,\n\n\\[ \\sum_{i=1}^{i=n} (a_{L_i} - 1 - a_{R_i}) * y^{i-1} = 0 \\text{ (eqn:4)}\\]\nThe left side of the above equation is a polynomial of degree at most \\(n-1\\), so it has at most \\(n-1\\) roots. \\(y\\) is highly unlikely to be a root of the polynomial unless all coefficients are zero.\nUsing the same argument, verifying the following equation is enough for the third equation.\n\n\\[ \\sum_{i=1}^{i=n} (a_{L_i} * a_{R_i}) * y^{i-1} = 0 \\text{ (eqn:5)}\\]\nRewrite equation 1 as \\((\\overrightarrow{a_L} - \\overrightarrow{1_n} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_n} = 0\\), rewrite equation 1 as \\((\\overrightarrow{a_L}) \\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_n}) = 0\\), rewrite 1 as \\(a - \\overrightarrow{a_L} \\cdot \\overrightarrow{2_n} = 0\\). Using the trick as above, we combine this equations to a single equation\n\n\\[(\\overrightarrow{a_L} - \\overrightarrow{1_n} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_n} + \\overrightarrow{a_L}\\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_n}) * z + (a - \\overrightarrow{a_L} \\cdot \\overrightarrow{2_n})* z^2 = 0 \\text{ (eqn:6)}\\]\nThis equation is equivalent to\n\n\\[(\\overrightarrow{a_L} - z\\overrightarrow{1_n}) \\cdot (\\overrightarrow{a_R}\\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n}) = z^2 v + \\delta(y, z) \\text{ (eqn:7)}\\]\nwhere \\(\\delta(y, z) = (z - z^2)(\\overrightarrow{1_n} \\cdot \\overrightarrow{y_n}) - z^3 (\\overrightarrow{1_n} \\cdot \\overrightarrow{2_n})\\) is a term involves only \\(y\\) and \\(z\\).\nIn order to make the range proof zero knowledge, we will add additional term \\(\\overrightarrow{s_L}x\\) (resp. \\(\\overrightarrow{s_R}x\\)) to \\(\\overrightarrow{a_L}\\) (resp. \\(\\overrightarrow{a_R}\\)), where \\(\\overrightarrow{s_L}, \\overrightarrow{s_R} \\in \\mathbb{Z}^n\\) are random vectors, \\(x\\) is unknown variable in \\(\\mathbb{Z}\\). Thus the left-hand side of equation 1 is now a polynomial in \\(x\\) of degree 2. Adjust the right-hand side to a polynomial in \\(x\\) of degree 2, Then we have a equation of the following form\n\n\\[\\overrightarrow{l(x)} \\cdot \\overrightarrow{r(x)} = t(x) \\text{ (eqn:8)}\\]\nwhere\n\n\\[\\overrightarrow{l(x)} = \\overrightarrow{a_L} + \\overrightarrow{s_L}x - z\\overrightarrow{1_n} \\text{ (eqn:9)}\\]\n\n\\[\\overrightarrow{r(x)} = (\\overrightarrow{a_R} + \\overrightarrow{s_R}x) \\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n} \\text{ (eqn:10)}\\]\n\n\\[t(x) = t_0 + t_1 x + t_2 x^2 = z^2 v + \\delta(y, z) + t_1 x + t_2 x^2 \\text{ (eqn:11)}\\]\nRange Proof We now view \\(x\\) as a chosen random scalar. Let \\(V\\) be the Pederson Commitment of \\(v\\), \\(T_1\\) be the Pederson Commitment of \\(t_1\\), \\(T_2\\) be the Pederson Commitment of \\(t_2\\), \\(A = PC(\\overrightarrow{a_L}, \\overrightarrow{a_R}, \\tilde{a})\\), \\(S = PC(\\overrightarrow{s_L}, \\overrightarrow{s_R}, \\tilde{s})\\), \\(P = PC(\\overrightarrow{l(x)}, \\overrightarrow{r(x)}, \\tilde{p})\\).\nThe range proof consists of \\((V, A, S, T_1, T_2, \\tilde{t}(x), t(x), \\tilde{p})\\) and a proof which proves that \\(t(x)\\) is indeed the inner product of \\(\\overrightarrow{l(x)}\\) and \\(\\overrightarrow{r(x)}\\), i.e. 1 holds.\nTo verify 1 and 1, we note that knowing the blinding factor, the Pederson commitment of \\((\\overrightarrow{a_L} + \\overrightarrow{s_L}x - z\\overrightarrow{1_n}, (\\overrightarrow{a_R} + \\overrightarrow{s_R}x) \\circ \\overrightarrow{y_n} + z\\overrightarrow{1_n}\\circ \\overrightarrow{y_n} + z^2 \\overrightarrow{2_n})\\) can be calculated from \\(A, S\\). Given the blinding factor of the Pedenson commitment of \\(\\overrightarrow{l(x)}, \\overrightarrow{r(x)}\\), we can calculate the commitment of \\((\\overrightarrow{l(x)}, \\overrightarrow{r(x)})\\) in the inner product proof. Thus we can only verify the given inner product proof with the commitment calculated from above.\nTo verify 1, we compare the commitment of \\(t(x)\\) with the commitment of \\(z^2 v + \\delta(y, z) + t_1 x + t_2 x^2\\). The first term can be calculated directly with \\(\\tilde{t}(x), t(x)\\), and the second term can be calculated with \\(V, T_1, T_2\\).\nAggregated Range Proof In our use case, we want to aggregate two range proofs. To aggregate range proofs of terms \\(a^{(k)}, k = 1, \\cdots, m\\) are within the range \\([0, 2^n)\\), we have the following equations\n\n\\[ a^{(k)} - \\sum_{i=1}^{n} a^{(k)}_{L_i} \\times 2^{i-1} = 0, \\forall k = 1, \\cdots, m \\text{ (eqn:12)}\\]\n\n\\[a^{(k)}_{L_i} - 1 - a^{(k)}_{R_i} = 0, \\forall i = 1, \\cdots, n, \\forall k = 1, \\cdots, m \\text{ (eqn:13)}\\]\n\n\\[a^{(k)}_{R_i} * a^{(k)}_{L_i} = 0, \\forall i = 1, \\cdots, n, \\forall k = 1, \\cdots, m \\text{ (eqn:14)}\\]\nNote when we concatenate all the binary representation of \\(\\overrightarrow{a^{(k)}_{L}}\\) (resp. \\(\\overrightarrow{a^{(k)}_{R}}\\)) into \\(\\overrightarrow{a_{L}}\\) (resp. \\(\\overrightarrow{a_{R}}\\)), we can condense 1 (resp. 1) into the 1 (resp. 1). We use the same trick as before to compress equations in 1, then we have \\[(\\overrightarrow{a_L} - \\overrightarrow{1_{mn}} - \\overrightarrow{a_R}) \\cdot \\overrightarrow{y_{mn}} + \\overrightarrow{a_L}\\cdot (\\overrightarrow{a_R} \\circ \\overrightarrow{y_{mn}}) * z + \\sum_k(a^{(k)} - \\overrightarrow{a^{(k)}_{L}} \\cdot \\overrightarrow{2_n}) * z^{2+k} = 0\\]\nAccordingly, we adjust terms in 1, 1 and 1. After that, we can verify the proof in the same way. Now we have got rid of all the roadblocks. A zero-knowledge confidential transaction scheme is here to stay.\n",url:"https://wiki.cont.run/sigma-bullets/"},"https:\/\/wiki.cont.run\/tags\/software-defined-networking\/":{title:"software-defined-networking",tags:[],content:"",url:"https://wiki.cont.run/tags/software-defined-networking/"},"https:\/\/wiki.cont.run\/tags\/split-horizon-dns\/":{title:"split-horizon-dns",tags:[],content:"",url:"https://wiki.cont.run/tags/split-horizon-dns/"},"https:\/\/wiki.cont.run\/tags\/starnix\/":{title:"starnix",tags:[],content:"",url:"https://wiki.cont.run/tags/starnix/"},"https:\/\/wiki.cont.run\/tags\/":{title:"Tags",tags:[],content:"",url:"https://wiki.cont.run/tags/"},"https:\/\/wiki.cont.run\/tags\/transparent-proxy\/":{title:"transparent-proxy",tags:[],content:"",url:"https://wiki.cont.run/tags/transparent-proxy/"},"https:\/\/wiki.cont.run\/tags\/wsl\/":{title:"wsl",tags:[],content:"",url:"https://wiki.cont.run/tags/wsl/"},"https:\/\/wiki.cont.run\/zero-config-proxies\/":{title:"zero config proxies",tags:["ptrace","proxy"],content:"In most cases, we can export http_proxy=http://localhost:8081. But at times, programs do not recognize the environment variable http_proxy. We may need to refer to their manual to learn how to configure its proxy settings. It is tedious. For example, the way to specify proxy of git over HTTPS different from that of git over SSH. We need some non-intrusive (transparent) way to specify proxies. Applications are required of nothing. No configuration entry for proxies, no environment variable.\nVPN May be you\u0026rsquo;re wondering what\u0026rsquo;s wrong with the good old VPN. This is the way to go. It ultimately boils down to the flexibility.\nproxying rules We need TCP connections to foreign countries to be proxied, while those to domestic servers should not be proxied. VPN solutions normally can only decide to proxy traffic based on simple routing tables.\nDNS Simple forwarding will not work in complicated environment. Somethings we want our proxying decisions based on the domain name of the request. Traditional VPNs are futile in those cases.\nHooks injection A few packages can force some programs to use proxy. They work great in their specific use case and if you know how to invoke the program by command line.\nproxychains Dynamically linked programs normally initiate network requests by calling the system libc. Proxychains uses the LD_PRELOAD trick (see ld.so(8) for details) to wrap these connection requests, i.e. they are not really connecting to the target server, instead, they are being sent to the proxy services.\ngraftcp The above method does not work with statically linked binaries. For that, we need to hook into the syscalls. graftcp does this. It uses ptrace(2) under the hood.\nproxifier One windows alternative for the above tools is proxifier. In my experience, proxifier does not work in some situation. I think it is because, proxifier hijack win32 API calling like proxychains, which does not always work as programs theoretically can just use system calls.\nLinux netfilter Netfitler is a beast. If only I know how to domesticate it. There are a few ways to proxying all traffic with netfilter. Before we start we should note that it is really easy to cause infinite loop. The traffic from the proxy service itself should not be proxied or there will be an infinite loop.\ntraffic redirection methods This section gives an overview on how to redirect the traffic to a backing proxy service. Note that the backing proxy service must support tproxy/redirect proxy mode actively. For a working script to manipulate iptables, see clash-redir.\ntproxy TPROXY stands for transparent proxy. This documentation is more clear than the kernel documentation. The traffic is transparently redirected to the proxy server. The tproxy server captures the traffic, and pretends to be the target server. It is quite easy for the tproxy server to get the original destination, as tproxy server\u0026rsquo;s receiving socket is set to be of original destination.\niptables -t mangle -N CLASH_EXTERNAL iptables -t mangle -A CLASH_EXTERNAL -d 0.0.0.0/8 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 127.0.0.0/8 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 224.0.0.0/4 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 172.16.0.0/12 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 169.254.0.0/16 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 240.0.0.0/4 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 192.168.0.0/16 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 10.0.0.0/8 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 100.64.0.0/10 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -d 255.255.255.255 -j RETURN iptables -t mangle -A CLASH_EXTERNAL -p tcp -j TPROXY --on-port \u0026#34;$CLASH_TPROXY_PORT\u0026#34; --on-ip 127.0.0.1 --tproxy-mark \u0026#34;$CLASH_MARK\u0026#34; iptables -t mangle -I PREROUTING -p tcp -j CLASH_EXTERNAL if [[ -z \u0026#34;$(ip rule list fwmark \u0026#34;$CLASH_MARK\u0026#34; table \u0026#34;$CLASH_TABLE\u0026#34;)\u0026#34; ]]; then  ip rule add fwmark \u0026#34;$CLASH_MARK\u0026#34; table \u0026#34;$CLASH_TABLE\u0026#34; fi ip route replace local 0.0.0.0/0 dev lo table \u0026#34;$CLASH_TABLE\u0026#34; A few notes:\n The above script makes all TCP traffic to be tproxied to 127.0.0.1:$CLASH_TPROXY_PORT. I intentionally create a new chain called CLASH_EXTERNAL to make it easier to restore order.  Running iptables-save -c | grep -v CLASH_ | iptables-restore -c is enough.\n The ip rule and ip route commands make sure all traffic is forwarded, including local traffic and forwarding traffic. if check is used for idem-potency.  redirect Iptables redirect just redirects the traffic to the target server. The original server could be a normal server (say a normal HTTP server), or a proxy (say a socks5 proxy which would then forward the traffic to socks5 proxy server).\niptables -t nat -N CLASH_LOCAL iptables -t nat -A CLASH_LOCAL -m owner --uid-owner \u0026#34;$CLASH_USER\u0026#34; -j RETURN iptables -t nat -A CLASH_LOCAL -m owner --gid-owner \u0026#34;$NOPROXY_GROUP\u0026#34; --suppl-groups -j RETURN || true iptables -t nat -A CLASH_LOCAL -d 0.0.0.0/8 -j RETURN iptables -t nat -A CLASH_LOCAL -d 127.0.0.0/8 -j RETURN iptables -t nat -A CLASH_LOCAL -d 224.0.0.0/4 -j RETURN iptables -t nat -A CLASH_LOCAL -d 172.16.0.0/12 -j RETURN iptables -t nat -A CLASH_LOCAL -d 169.254.0.0/16 -j RETURN iptables -t nat -A CLASH_LOCAL -d 240.0.0.0/4 -j RETURN iptables -t nat -A CLASH_LOCAL -d 192.168.0.0/16 -j RETURN iptables -t nat -A CLASH_LOCAL -d 10.0.0.0/8 -j RETURN iptables -t nat -A CLASH_LOCAL -d 100.64.0.0/10 -j RETURN iptables -t nat -A CLASH_LOCAL -d 255.255.255.255 -j RETURN iptables -t nat -A CLASH_LOCAL -p tcp -j REDIRECT --to-ports \u0026#34;$CLASH_REDIRECT_PORT\u0026#34; iptables -t nat -I OUTPUT -p tcp -j CLASH_LOCAL  iptables -t nat -N CLASH_EXTERNAL iptables -t nat -A CLASH_EXTERNAL -d 0.0.0.0/8 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 127.0.0.0/8 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 224.0.0.0/4 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 172.16.0.0/12 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 169.254.0.0/16 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 240.0.0.0/4 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 192.168.0.0/16 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 10.0.0.0/8 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 100.64.0.0/10 -j RETURN iptables -t nat -A CLASH_EXTERNAL -d 255.255.255.255 -j RETURN iptables -t nat -A CLASH_EXTERNAL -p tcp -j REDIRECT --to-ports \u0026#34;$CLASH_REDIRECT_PORT\u0026#34; iptables -t nat -I PREROUTING -p tcp -j CLASH_EXTERNAL A few notes:\n Need to add rule for both PREROUTING and OUTPUT for local traffic and forwarding traffic. NAT under the hood. It is a little tricky for the redirect proxy to obtain the original destination address and port. Works perfectly if you don\u0026rsquo;t need the original destination (like DNS request).  DNAT Just like redirect.\ntraffic matching methods See iptables-extensions(8) for more methods.\nipset netfilter itself is able to match a few IPs effectively. When an entire country\u0026rsquo;s IP addresses need matching, it would be better to use ipset.\ncgroup See cgproxy.\nowner, supplementary groups I use iptables owner module to avoid infinite loop. Use supplementary groups to skip proxy for some programs.\niptables -t mangle -I CLASH_LOCAL -m owner --uid-owner \u0026#34;$CLASH_USER\u0026#34; -j RETURN iptables -t mangle -I CLASH_LOCAL -m owner --gid-owner \u0026#34;$NOPROXY_GROUP\u0026#34; --suppl-groups -j RETURN || true I also add the following to a systemd unit to skip proxy for a systemd unit.\n[Service] SupplementaryGroups=noproxy DNS resolution It is of no use for the proxy server to send its requests to a fake server. There are mainly to methods to avoid DNS poisoning.\nBogon IP  Client initiate a DNS request to resolve google.com The proxy service immediately returns the IP address 192.18.0.22, insert the mapping from 192.18.0.22 to google.com into its internal state The client initiate a TCP connection to 192.18.0.22 Upon receive the IP packet to 192.18.0.22, the proxy service finds out the request is to google.com. It decides to send the traffic through the proxy server  Redirect DNS requests  Client initiate a DNS request to resolve google.com The proxy service hijack the request and redirect the traffic to its internal DNS server. The un-posioned address 142.250.66.46 is returned The client initiate a TCP connection to 142.250.66.46 The proxy service checks the IP database, and decides to redirect the traffic to the proxy server  L4/L7 proxies to L3/L2 VPNs If you ever used macOS, iOS and android, you will find how easy it is on these platform to set up an VPN service. These VPN services, unlike traditional ones, are much more flexible. They are like PAC proxies, but for all programs. Below is a typical proxy traffic flow in those platforms.\napps \u0026lt;-\u0026gt; OS \u0026lt;-(L2/L3)-\u0026gt; virtual tunnel \u0026lt;-(L2/L3)-\u0026gt; proxy frontend \u0026lt;-(L4)-\u0026gt; socks5 client \u0026lt;-\u0026gt; proxy client \u0026lt;-\u0026gt; internet \u0026lt;-\u0026gt; proxy server \u0026lt;-\u0026gt; internet  The proxy first creates a virtual tunnel using OS-specific APIs (e.g. TUN/TAP on Linux, VPNService on Android). Upon receiving app request, the OS constructs L2/L3 packets (ethernet/ip packets), and send those packets to the proxy over the virtual tunnel. The proxy unwraps those L2/L3 packets and then sends TCP/UDP packets to the socks5 client. The proxy backend client sends the proxy requests to the proxy backend server over the Internet. Upon receiving the response, the proxy server sends it back to the proxy client.  There are a few things requiring special attention.\n How does the proxy frontend get L4 packets from L2/L3 packets in the chain OS \u0026lt;-(L2/L3)-\u0026gt; virtual tunnel \u0026lt;-(L2/L3)-\u0026gt; proxy frontend \u0026lt;-(L4)-\u0026gt; socks5 client ? How does the proxy client avoid infinite loop?  The first question is solved by redsocks and tun2socks. There are quite a few solutions on the market. Moreover, apple, by the OS itself, provides such L2/L3 to L4 convertor. It is called NEAppProxyProvider. This is why there are some many proxies on macOS with enhanced mode. networkextension also provides DNS Proxy, Filtering API Windows users also have a few generic solutions like leaf and maple. Besides, clash premium supports tun, and it does those tunnel to sockss conversions automatically.\nThe second question is platform-dependent. For android, this article is oldie and goodie.\nrouter in the middle You can also set up a router in the middle to transparently proxy your traffic.\niptables/tun on openwrt All you need to enable IP forwarding and following the above instructions.\nannounce another host as gateway or customize routing table manually If your router is powerful enough, just set up proxy in the router. Otherwise, announce the gateway to be a proxy server in DHCP. You may also change the default routing table manually.\nARP spoofing The downsides for above method is that, you either need control to the router or you need to change a few things manually on each device. fqrouter had a slick trick. It fools the hosts in the LAN to believe that this host is the gateway by ARP spoofing. See here for details.\nVPN Hotspot The Android APP VPN Hotspot works like a charm.\n WiFi relay new host spot  Windows/macOS l2 proxy ",url:"https://wiki.cont.run/zero-config-proxies/"},"https:\/\/wiki.cont.run\/tags\/zero-knowledge-proofs\/":{title:"zero-knowledge-proofs",tags:[],content:"",url:"https://wiki.cont.run/tags/zero-knowledge-proofs/"},"https:\/\/wiki.cont.run\/tags\/zero-trust\/":{title:"zero-trust",tags:[],content:"",url:"https://wiki.cont.run/tags/zero-trust/"},"https:\/\/wiki.cont.run\/tags\/zircon\/":{title:"zircon",tags:[],content:"",url:"https://wiki.cont.run/tags/zircon/"}}</script><script src=https://wiki.cont.run/js/lunr.min.js type=text/javascript></script>
<script src=https://wiki.cont.run/js/search.js type=text/javascript></script></body></html>