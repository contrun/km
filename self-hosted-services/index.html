<!doctype html><html><title>self-hosted services - wiki</title><meta charset=utf-8><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,maximum-scale=1,user-scalable=no"><meta name=apple-mobile-web-app-capable content="yes"><meta name=description content="TLDR: I use tailscale/zerotier to establish a smallish mesh network. I use envoy as an edge router to forward L4 traffic. I mainly provision and manage services with nix, docker and sops. When it is absolutely required, I use k3s to deploy Kubernetes services. Traefik is used for routing, and authelia is used for blocking unauthorized access. To multiplexing protocols with a single port, I use aioproxy. I use restic to back up my inevitably accumulated state."><link rel=stylesheet href=https://wiki.cont.run/css/main.min.9876f229ece74a87e784cb6ac19f558d58f7c34e91d316c290e35437ca00d7d6.css><body><header><a href=../ id=logo><svg id="Capa_1" enable-background="new 0 0 511.992 511.992" height="512" viewBox="0 0 511.992 511.992" width="512" xmlns="http://www.w3.org/2000/svg"><g><g><g><path d="m256 420.826c0 38.345-11.844 68.545-49.991 68.014-27.744-.385-51.481-15.31-61.853-39.46-1.239-2.887-4.024-4.734-7.154-4.725-.07.0-.135.0-.201.0-47.474.0-75.537-26.171-75.537-73.882.0-5.633.542-11.138 1.568-16.468.62-3.229-.825-6.489-3.671-8.125C23.668 325.748 10 300.053 10 255.997c0-44.057 13.668-69.757 49.161-90.185 2.846-1.636 4.291-4.896 3.671-8.13-1.026-5.33-1.568-10.83-1.568-16.463.0-47.711 28.064-73.882 75.537-73.882h.201c3.13.009 5.915-1.837 7.154-4.729 10.372-24.145 34.109-39.069 61.853-39.455C244.159 22.621 256 52.821 256 91.166" fill="#ff9eb1"/></g><g><g><g><path d="m256 91.166c0-38.344 11.844-68.545 49.991-68.014 27.744.385 51.481 15.31 61.853 39.46 1.239 2.887 4.024 4.734 7.154 4.724h.201c47.474.0 75.537 26.171 75.537 73.882.0 5.633-.542 11.138-1.568 16.468-.62 3.229.825 6.489 3.671 8.125C488.332 186.245 502 211.939 502 255.996s-13.668 69.756-49.161 90.185c-2.846 1.636-4.291 4.896-3.671 8.13 1.026 5.33 1.568 10.83 1.568 16.463.0 47.711-28.064 73.882-75.537 73.882h-.201c-3.13-.009-5.915 1.837-7.154 4.729-10.372 24.145-34.109 39.069-61.853 39.455-38.15.531-49.991-29.669-49.991-68.014" fill="#ff7d97"/></g></g><g><g><path d="m502 265.996c-4.193.0-7.984-2.713-9.407-6.636-1.419-3.912-.16-8.459 3.063-11.092 3.291-2.689 8.009-2.99 11.621-.758 3.568 2.205 5.404 6.578 4.478 10.669-1.02 4.501-5.126 7.817-9.755 7.817z"/></g></g></g><g><path d="m340.83 229.18h-58.013v-58.014h-53.634v58.014H171.17v53.633h58.013v58.013h53.634v-58.013h58.013z" fill="#faf7f5"/></g></g><g><g><path d="m498.468 291.859c-5.141-2.02-10.945.508-12.965 5.648-6.442 16.389-18.055 28.727-37.649 40.005-6.513 3.746-9.932 11.253-8.505 18.689.921 4.783 1.388 9.686 1.388 14.572.0 41.792-22.662 63.882-65.537 63.882h-.225c-.938.0-1.864.074-2.771.217-15.031-4.92-23.796-20.93-19.661-36.479 1.42-5.337-1.757-10.815-7.094-12.234-5.333-1.418-10.814 1.756-12.234 7.094-5.958 22.405 4.241 45.396 23.384 56.443-9.583 17.791-28.602 28.836-50.748 29.145-11.303.146-19.802-2.743-26.011-8.867-9.184-9.057-13.84-25.592-13.84-49.148v-70h16.816c5.522.0 10-4.477 10-10v-48.014h48.014c5.522.0 10-4.477 10-10V229.18c0-5.523-4.478-10-10-10h-48.014v-48.014c0-5.523-4.478-10-10-10H266v-70c0-23.555 4.657-40.09 13.841-49.148 6.21-6.123 14.696-9.022 26.011-8.867 23.862.332 44.096 13.132 52.803 33.405.218.509.458 1.003.719 1.483-3.225 8.243-9.084 15.093-16.833 19.574-8.993 5.2-19.465 6.581-29.485 3.89-5.337-1.434-10.819 1.73-12.251 7.064-1.433 5.334 1.729 10.819 7.063 12.252 5.074 1.363 10.221 2.037 15.338 2.037 10.2.0 20.272-2.681 29.346-7.928 11.083-6.408 19.607-16.017 24.616-27.575 41.6.67 63.569 22.718 63.569 63.866.0 4.89-.467 9.794-1.389 14.582-1.427 7.426 1.991 14.933 8.502 18.678 19.572 11.267 31.176 23.584 37.625 39.936 1.552 3.934 5.318 6.334 9.306 6.333 1.221.0 2.462-.225 3.666-.7 5.138-2.026 7.66-7.834 5.634-12.972-7.943-20.141-22.201-35.773-44.801-49.086.967-5.521 1.457-11.155 1.457-16.772.0-52.114-31.48-83.391-84.288-83.876-12.157-26.863-38.963-43.753-70.318-44.189-16.67-.232-30.255 4.688-40.332 14.625-3.813 3.76-7.08 8.226-9.797 13.382-2.717-5.156-5.984-9.622-9.797-13.382-10.075-9.937-23.668-14.852-40.333-14.625-31.353.437-58.158 17.325-70.32 44.189-52.807.485-84.286 31.762-84.286 83.876.0 5.617.49 11.253 1.458 16.771-37.422 22.031-52.724 50.552-52.724 98.008.0 47.451 15.299 75.969 52.721 98.006-.967 5.521-1.457 11.154-1.457 16.772.0 52.114 31.48 83.391 84.288 83.876 12.157 26.863 38.963 43.753 70.318 44.189.377.005.751.008 1.125.008 16.172.0 29.358-4.92 39.207-14.632 3.813-3.76 7.08-8.226 9.797-13.382 2.717 5.156 5.984 9.622 9.797 13.382 9.849 9.713 23.034 14.633 39.208 14.632.373.0.749-.003 1.125-.008 31.352-.436 58.158-17.325 70.32-44.189 52.807-.485 84.286-31.762 84.286-83.876.0-5.617-.49-11.253-1.458-16.772 22.634-13.329 36.901-28.989 44.838-49.178 2.022-5.14-.507-10.945-5.647-12.966zM282.816 239.18h48.014v33.633h-48.014c-5.522.0-10 4.477-10 10v48.014h-33.633v-48.014c0-5.523-4.477-10-10-10H181.17V239.18h48.014c5.523.0 10-4.477 10-10v-48.014h33.633v48.014c-.001 5.523 4.477 10 9.999 10zm-50.657 230.794c-6.21 6.124-14.717 9.018-26.011 8.867-23.862-.331-44.096-13.132-52.803-33.405-.218-.509-.458-1.003-.719-1.483 3.225-8.243 9.085-15.093 16.833-19.574 8.992-5.2 19.463-6.581 29.485-3.89 5.337 1.434 10.819-1.73 12.251-7.064 1.433-5.333-1.73-10.819-7.064-12.251-15.188-4.08-31.059-1.988-44.684 5.891-11.083 6.408-19.607 16.017-24.616 27.575-41.6-.67-63.569-22.718-63.569-63.866.0-4.89.467-9.794 1.389-14.582 1.427-7.427-1.991-14.934-8.502-18.678-32.183-18.528-44.149-40.621-44.149-81.517.0-40.9 11.966-62.994 44.146-81.517 6.513-3.746 9.932-11.253 8.505-18.689-.921-4.783-1.388-9.686-1.388-14.572.0-41.792 22.662-63.882 65.537-63.882h.225c.938.0 1.864-.074 2.771-.217 15.031 4.92 23.796 20.93 19.661 36.479-1.42 5.337 1.757 10.815 7.094 12.234.861.229 1.726.338 2.577.338 4.422.0 8.467-2.956 9.657-7.432 5.958-22.405-4.241-45.396-23.384-56.443 9.583-17.791 28.602-28.836 50.748-29.145 11.267-.15 19.801 2.743 26.011 8.867 9.184 9.057 13.84 25.592 13.84 49.148v70h-16.816c-5.522.0-10 4.477-10 10v48.014H171.17c-5.522.0-10 4.477-10 10v53.633c0 5.523 4.478 10 10 10h48.014v48.014c0 5.523 4.478 10 10 10H246v70c0 23.554-4.657 40.09-13.841 49.147z"/><path d="m139.699 228.227c-6.766.0-13.186 1.514-18.907 4.31-3.049-8.65-8.286-16.485-15.336-22.673-4.151-3.643-10.469-3.233-14.113.918-3.643 4.15-3.232 10.469.918 14.112 6.711 5.891 10.816 14.143 11.498 22.953-1.213 1.914-2.293 3.946-3.225 6.088-1.145 2.633-2.015 5.316-2.615 8.019-13.414 11.422-33.601 10.834-46.225-1.792-3.906-3.904-10.236-3.904-14.143.0-3.905 3.905-3.905 10.237.0 14.143 10.524 10.524 24.354 15.784 38.193 15.784 8.04.0 16.083-1.775 23.484-5.325 1.904 5.443 4.967 10.557 9.136 15.03.596.639 1.204 1.269 1.826 1.891 1.953 1.953 4.512 2.929 7.071 2.929 2.56.0 5.118-.976 7.071-2.929 3.905-3.905 3.905-10.237.0-14.143-.458-.458-.906-.922-1.342-1.389-6.26-6.716-7.799-15.778-4.118-24.241 2.878-6.616 9.86-13.686 20.826-13.686 5.522.0 10-4.477 10-10 .001-5.522-4.476-9.999-9.999-9.999z"/><path d="m387.667 287.543c-3.905 3.905-3.905 10.237.0 14.143 1.953 1.953 4.512 2.929 7.071 2.929s5.118-.976 7.071-2.929c.622-.622 1.23-1.253 1.83-1.896 4.167-4.471 7.229-9.583 9.133-15.025 7.401 3.549 15.444 5.324 23.484 5.324 13.839.0 27.67-5.261 38.193-15.784 3.905-3.905 3.905-10.237.0-14.143-3.906-3.904-10.236-3.904-14.143.0-12.624 12.625-32.811 13.214-46.225 1.792-.6-2.702-1.47-5.386-2.615-8.019-.932-2.142-2.012-4.175-3.225-6.088.682-8.81 4.787-17.062 11.498-22.953 4.15-3.644 4.561-9.962.918-14.112-3.646-4.151-9.964-4.563-14.113-.918-7.05 6.189-12.287 14.023-15.336 22.673-5.721-2.796-12.141-4.31-18.907-4.31-5.523.0-10 4.477-10 10s4.477 10 10 10c10.966.0 17.948 7.07 20.826 13.686 3.681 8.463 2.142 17.525-4.114 24.237-.44.47-.888.935-1.346 1.393z"/></g></g></g></svg></a><h3 class=site-title>wiki</h3></header><div class=grid-container><div class=grid><div class=page data-level=1><div class=content><h1>self-hosted services</h1><p>TLDR: I use <a href=https://tailscale.com/>tailscale</a>/<a href=https://www.zerotier.com/>zerotier</a> to establish a smallish mesh network. I use <a href=https://www.envoyproxy.io/>envoy</a> as an edge router to forward L4 traffic.
I mainly provision and manage services with <a href=https://nixos.org/>nix</a>, <a href=https://www.docker.com/>docker</a> and <a href=https://github.com/mozilla/sops>sops</a>. When it is absolutely required, I use <a href=https://k3s.io/>k3s</a> to deploy <a href=https://kubernetes.io/>Kubernetes</a> services.
<a href=https://traefik.io/>Traefik</a> is used for routing, and <a href=https://github.com/authelia/authelia>authelia</a> is used for blocking unauthorized access.
To multiplexing protocols with a single port, I use <a href=https://github.com/contrun/aioproxy/>aioproxy</a>. I use <a href=https://restic.net/>restic</a> to back up my inevitably accumulated state.</p><h2 id=principles>Principles</h2><p>My principles can be best described as <a href=https://github.com/cncf/toc/blob/main/DEFINITION.md>cloud nativeness</a>. Cloud-native is a all-encompassing and vague term.
I have a few concrete points on my mind.</p><ul><li>Software-defined everything</li><li>Declarative</li><li>Infrastructure as code</li><li>Minimal state maintenance</li><li>Self-organization</li><li>Single source of truth</li></ul><h2 id=networking>Networking</h2><p>The first obstacle to self-host everything is that you don&rsquo;t have a stable public accessible IP. There are a few solutions.</p><h3 id=the-cloud>The cloud</h3><ul><li>I am paranoid enough to not trust the cloud, aka other people&rsquo;s computer.</li><li>This approach is not cost-efficient. Even my Raspberry PI can beat many VPSes in terms of CPU frequency. Not to mention I can easily insert a 256G SD card.</li><li>Locality. There is no place like LAN. I see no benefit in downloading youtube video to another VPS.</li></ul><h3 id=ddns>DDNS</h3><p>This is simplest. I don&rsquo;t use this mainly because it is not reliable in my setup. To name a few problems of DDNS.</p><ul><li>80, 443, 8080 blocked.</li><li>Not portable router configurations. You need to set up port mapping or dmz host in your router, which is hard to codify, if not impossible</li><li><a href=https://en.wikipedia.org/wiki/Carrier-grade%5FNAT>CGNAT</a></li><li>ipv6 is still yet to come</li></ul><h3 id=port-forwarding>Port Forwarding</h3><p>There are many port forwarding software. To name a few, <a href=https://www.harding.motd.ca/autossh/>autossh</a> (my favorite), <a href=https://ngrok.com/>ngrok</a>, <a href=https://github.com/fatedier/frp>frp</a>, <a href=https://github.com/ehang-io/nps>nps</a>.
The biggest problem of port forwarding is that it is not scalable and there is no generic inter-node connectivity with port forwarding.
Port forwarding has the following weaknesses.</p><ul><li>star topology, single point of failure</li><li>no inter-node connectivity.</li><li>number of ports are limited, you only have one 443</li><li>hard to set up (authorization)</li><li>no <a href=https://en.wikipedia.org/wiki/Hairpinning>hairpinning</a> support</li><li>most port forwarding only supports TCP</li></ul><h3 id=overlay-networks>Overlay Networks</h3><p><a href=https://en.wikipedia.org/wiki/Overlay%5Fnetwork>Overlay networks</a> are magic. What I meant is not <a href=https://github.com/containernetworking/cni>container network interface</a> kind of overlay network, but solutions like <a href=https://www.zerotier.com/>zerotier</a>, <a href=https://tailscale.com/>tailscale</a>, <a href=https://github.com/tonarino/innernet>innernet</a>, <a href=https://github.com/slackhq/nebula>nebula</a>, <a href=https://github.com/ntop/n2n>n2n</a>.
There all have interesting aspects. But none of they are self-organizing. They all require a centralized coordination server.
What I have on my mind is something like <a href=https://matrix.org/blog/2021/05/06/introducing-the-pinecone-overlay-network/>matrix pinecone</a>. I have been thinking on implementing a pinecone like overlay network for a while, self-organizing, and tunneling traffic with <a href=https://libp2p.io/>libp2p</a>.
I currently rely on tailscale and zerotier to establish peer-to-peer connectivity. This works great in the following perspectives.</p><ul><li>inter-node connectivity</li><li>all ports are belong to you</li><li>easy to set up (implementation-dependent)</li><li>transparent <a href=https://en.wikipedia.org/wiki/Hole%5Fpunching%5F(networking)>hole punching</a></li><li>transparent multi-path</li></ul><h2 id=routing>Routing</h2><h3 id=l3>L3</h3><p>L3 routing is provided by the overlay network solutions.</p><h3 id=l4>L4</h3><h4 id=considerations>Considerations</h4><p>For L4 routing, I care about transparency, protocol multiplexing and configuration-complexity.</p><ul><li><p>Transparency</p><p>This means that the backend service does not know there is a middle man do the heavy lifting.
In particular, it means that the origin requester&rsquo;s address is preserved. Typical HTTP reverse proxies are not transparent.
They pass the original requester&rsquo;s information by injecting an <code>X-Forwarded-For</code> header.</p></li></ul><ul><li><p>Protocol Multiplexing</p><p>L4 multiplexing means that we can use the same TCP port for HTTP, TlS and SSH. An example is <a href=https://github.com/yrutschle/sslh>sslh</a>.
It normally works by peeking into a few first bytes and determine which
protocol this packet is, and then handing off the connection to another application which is Listening on some specific port.</p></li></ul><ul><li><p>Configuration Complexity</p><p>Do we have to configure both the proxy and backend services? What if we change a user-fronting proxy address?
Do the backend server need to adjust for this change? Any special configuration for different user-fronting configurations?
What if an upstream server is down? Must I manually remove edit the configuration to reflect this change?</p></li></ul><h4 id=solutions>Solutions</h4><ul><li><p>iptables</p><p>This is just like NAT. It is transparent. I believe you can multiplex port with some <a href=https://ipset.netfilter.org/iptables-extensions.man.html>iptables extensions</a>. It is not super pretty.
A lethal problem is that the user-fronting proxy must be in the return path of the connection (usually the proxy is the default gateway).
To circumvent this problem, we need <a href=https://unix.stackexchange.com/questions/4420/reply-on-same-interface-as-incoming>some modifications to the routing table and routing policies</a>.
When there are two proxies which are connected to the same interface, there are multiple return paths, to select the correct one,
we need policy based routing.</p></li></ul><ul><li><p>ipvs</p><p>Compared with iptables, ipvs is much more manageable and scalable. Yet it still is too complicated.</p></li></ul><ul><li><p>usespace L4 proxy</p><p>envoy/haproxy/nginx etc can be used as L4 proxy. They accept the incoming downstream connection and establish a new upstream connection like a pipe.
This is much more manageable, the downside is that the original client&rsquo;s information is lost in translation.
To ease this problem, haproxy designed a protocol called <a href=https://www.haproxy.org/download/1.8/doc/proxy-protocol.txt>PROXY</a> (I can haz a more searchable name?).
In short, it appends original request&rsquo;s source and destination addresses to the TCP connection or UDP stream.
As stated in the above document, this will solve the multiple return paths because we are initiating another TCP connection/UDP stream.
Unfortunately, this solution is invasive as it requires the backend service to support PROXY protocol explicitly.
Fortunately we have <a href=https://github.com/cloudflare/mmproxy>mmproxy</a>. It accepts PROXY protocol packets, unwraps it and then forwards it to upstream. Moreover. Moreover, it does so transparently.
The original mmproxy does not support UDP, while this go implementation <a href=https://github.com/path-network/go-mmproxy>go-mmproxy</a> supports.</p></li></ul><ul><li><p>aioproxy</p><p>mmproxy is great when work with envoy. But it can not multiplex port like sslh, does not support non-transparent proxy,
and does not support non-PROXY protocol traffic.
Non-transparent proxy is useful when we are trying to proxy a connection whose original requester, proxy and the backend server are all the same host.</p><ul><li><p>How transparent proxy works</p><p>Let cip be the client ip, pip be the proxy ip and sip be the backend server IP.</p><ul><li>Client connection: cip:45678 -> sip:22,
client tries to connect to sip:22, but it actually connects to transparent proxy</li><li>Transparent proxy downstream connection: cip:45678 -> pip:44443,
transparent proxy accepts traffic from cip:45678, the traffic originally targeted sip:22 is redirected to pip:44443 by netfilter.</li><li>Transparent proxy upstream connection: pip:45678 -> sip:22,
transparent proxy establish a new connection to sip:22, it changes the socket source address to cip:45678 with the help of IP_TRANSPARENT.</li><li>Backend server connection: cip:45678 -> sip:22,
backend server is fooled by the connection socket address, this connection is actually started from the transparent proxy.
If the transparent proxy stands right in the middle of the return path from the backend server to the client, then the proxy can get the return packet from its upstream connection
and send it to the client on behalf of backend server by its downstream connection.</li></ul></li></ul><ul><li><p>What could go wrong when client and transparent proxy are on the same host</p><p>If client and transparent proxy are on the same host <code>127.0.0.1</code>,
both of them will try to bind <code>127.0.0.1:45678</code>, which would fail with <code>Address Already in Use</code>.</p></li></ul><ul><li><p>What could go wrong when we chain more than one transparent proxy</p><p>On the other hand, if we use the scheme client &lt;-> envoy &lt;-> mmproxy &lt;-> sslh &lt;-> ssh, and when both mmproxy and sslh are configured to proxy
transparently, the same bind error would occur (I have not tried it, I expect it to fail).</p><p>So it is sometimes useful to proxy non-transparently, and it would be great if we can have an all-in-one proxy which can intelligently unwrap PROXY protocol
traffic (when it fails to do so, just treats it as normal traffic as forwards it), supports transparent proxy to upstream and multiplexes port for different protocols.</p><p><a href=https://github.com/contrun/aioproxy>Here</a> is my take on this problem. Aioproxy has basic rudimentary solutions for all above problems.
There are a few things I intended to add. First, more protocol support for multiplexing. Most outstandingly, peeking into SNI, and forwarding connection accordingly.
Second, as discussed above, it could go wrong when client and transparent proxy is on the same host. We need intelligent transparent forwarding, i.e.
when client and transparent proxy is on the same host, do not use the same client address tuple.
At this point, the proxy is now been abandoned in favor of <a href=https://github.com/mholt/caddy-l4>caddy-l4</a>. Caddy-l4 is not mature enough currently, but it has much greater potential,
as we can use anything caddy already provided.</p></li></ul></li></ul><ul><li><p>envoy+traefik+aioproxy</p><p>This is my current setup. Envoy, traefik and aioproxy are a great match. Client connection to my edge proxy <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/templates/envoy.yaml.j2>is wrapped with PROXY protocol</a> by envoy
and forwarded to traefik. Depending on the packet format, traefik would forward it to HTTP traffic to docker or Kubernetes, other TCP traffic to aioproxy
(this works by setting SNI to rules to match <code>Host("*")</code>, see <a href=https://community.traefik.io/t/routing-ssh-traffic-with-traefik-v2/717>here</a>), the PROXY protocol header is automatically peeled off when possible.
It is not transparent to aioproxy. I don&rsquo;t intend to optimize it for now. In fact, it would be better if I insert aioproxy
in front of traefik, as this way every service is now ignorant of the proxy. But I didn&rsquo;t implement intelligent transparent proxy mentioned
above yet (this is fairly easy, and I am fairly lazy currently).
There will be some problem when client and transparent proxy are on the same host, which is a frequent user case for me.</p></li></ul><h3 id=intermission-split-horizon-dns>Intermission: Split Horizon DNS</h3><p>I have a few ways to access my services. When I use my own devices, I can just access my services by overlay networks.
My devices are part of the overlay network. I can access services via a stable address within <code>10.144.0.0/16</code>.
Overlay networks are magic. They automatically select path for me, e.g. when my two devices are in the same network, they connect each other
using LAN address. Overlay networks can transparently do NAT-PMP/UPNP, punch holes. When one device is behind an impenetrable NAT, they automatically select a relay.
I may want to make part of my services available outside the overlay network. In that case,
access to the services is proxied by two public accessible VPS. They forward traffic as described above.
The problem is that my VPS lives in Far Far Away. I don&rsquo;t want to travel around the world when I am in the overlay network.
Can my device be intelligent enough to just try overlay network first, when it fails to do so, use the backup VPSes?
This is a well-known problem of <a href=https://en.wikipedia.org/wiki/Split-horizon%5FDNS>split horizon dns</a>. I have a stable domain name <code>service-a.example.com</code>, I want it to be resolved as <code>10.2.3.4</code> when I am
in the corporate network (or I was using a VPN), otherwise please resolve it to <code>1.2.3.4</code>. Here is a few solutions.
By the way, <a href=https://tailscale.com/blog/sisyphean-dns-client-linux/>this</a> is a great read on this problem.</p><h4 id=hosts>Hosts</h4><p>The easiest and the most abominable solution. The downside is</p><ul><li>No wildcard support for <a href=https://superuser.com/questions/135595/using-wildcards-in-names-in-windows-hosts-file>Windows</a>, <a href=https://stackoverflow.com/questions/20446930/how-to-put-wildcard-entry-into-etc-hosts>Linux</a></li><li>flexibility. You can not graceful fallback to another host, easily add another entry</li></ul><h4 id=nsswitch>Nsswitch</h4><p>If you ever use mdns, you may wonder how <code>abc.local</code> resolve to the host <code>abc</code>. The secret sauce lies in
the following stanza of <code>/etc/nsswitch.conf</code>.</p><pre><code class=language-nil data-lang=nil>hosts:     files mdns_minimal [NOTFOUND=return] mymachines resolve [!UNAVAIL=return] dns mdns myhostname
</code></pre><p>Here, <code>mdns_minimal</code> and <code>mymachine</code> are dynamic libraries used by <a href=https://wiki.archlinux.org/title/Domain%5Fname%5Fresolution#Name%5FService%5FSwitch>NSS</a> to resolve hosts.
They provide the functionality of resolving mdns hosts and machinectl hosts. Theoretically, I can just
write another plugin for nsswitch like <code>mdns_minimal</code>, but nsswitch is also an abomination.
It is glibc only, thus first musl-linked and statically linked binaries would fail.
As a matter of fact, <a href=https://wiki.musl-libc.org/future-ideas.html>supporting mdns on musl is a future idea</a>, while <a href=https://github.com/golang/go/issues/10485>golang fallbacks to glibc to resolve hostname</a> when
the hosts entry in nsswitch is too conflicted. So it does not worth the effort to fiddle with nsswitch.</p><h4 id=coredns>Coredns</h4><p>I find salvation in coredns. Here is how I resolve a domain name with coredns enriched by <a href=https://github.com/openshift/coredns-mdns>coredns-mdns</a> and <a href=https://github.com/coredns/alternate>coredns-alternate</a>.
The source code to this coredns instance is <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/coredns/main.go>here</a>.</p><pre><code class=language-nil data-lang=nil>.:5355 {
    template IN A mydomain.tld {
      match ^(|[.])(?P&lt;p&gt;.*)\.(?P&lt;s&gt;(?P&lt;h&gt;.*?)\.(?P&lt;d&gt;mydomain.tld)[.])$
      answer &quot;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.&quot;
      fallthrough
    }
    template IN AAAA mydomain.tld {
      match ^(|[.])(?P&lt;p&gt;.*)\.(?P&lt;s&gt;(?P&lt;h&gt;.*?)\.(?P&lt;d&gt;mydomain.tld)[.])$
      answer &quot;{{ .Name }} 60 IN CNAME {{ if eq .Group.h `hub` }}hub_hostname{{ else }}{{ .Group.h }}{{ end }}.{{ .Group.d }}.&quot;
      fallthrough
    }
    mdns mydomain.tld
    alternate original NXDOMAIN,SERVFAIL,REFUSED . 1.0.0.1 8.8.4.4 9.9.9.9 180.76.76.76 223.5.5.5
}
</code></pre><p>The corefile above does the following things.</p><ul><li>cname <code>*.hostname.mydomain.tld</code> to <code>hostname.mydomain.tld</code></li><li>Let <code>hostname.mydomain.tld</code> be resolved to <code>hostname.local</code> by coredns-mdns</li><li>Anything not matched or not resolved here is forwarded to real world DNS servers</li></ul><p>To resolve <code>hostname.local</code>, I use <a href=https://www.avahi.org/>avahi</a> to <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L824-L863>announce the workstation</a> <code>hostname</code>. This solution is particular elegant,
in the sense that all hosts need only to configure themselves. To use this DNS server for all applications,
I added the systemd-resolved configuration <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L773-L780>here</a>. It is also possible to make other devices in the overlay network
to use this DNS server. I haven&rsquo;t done it yet.</p><h3 id=l7>L7</h3><p>Now that we can resolve domains to desirable hosts, we can access services directly in the browser.</p><h4 id=tls-certificates-and-termination>TLS Certificates and Termination</h4><p>I use acme with dns-chanlledge. My DNS service provider is cloudflare. From <a href=https://letsencrypt.org/>letsencrypt</a>, I got free wildcard certificates for
<code>*.hostname.mydomain.tld</code>, <code>*.local.mydomain.tld</code>, optionally also some alias domains like <code>*.hub.mydomain.tld</code>.
The certificates are obtained by setting <a href="https://search.nixos.org/options?channel=20.09&from=0&size=50&sort=relevance&query=security.acme">NixOS options security.acme</a>, and are shared between multiple applications.
Currently TLS is terminated by traefik using above certificates.</p><h4 id=service-and-routing-registration>Service and Routing Registration</h4><p>Service and router registration is done in a self-organizing way.
I don&rsquo;t use subpath routing rules, as it may require extra works of rewriting paths.
Routing is only matched by <code>Host</code>. All my services have dedicated domain.
Cloudflare provides wildcard DNS resolution. My coredns configuration above also resolves domain names in the wildcard matching fashion.</p><ul><li><p>Fixed Services and Routings</p><p><a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L933-L1102>Generated from nix expressions</a>. It is obligate for me to praise how easily nix (a real programming language, albeit a weak one) eliminates boilerplate.
Why is everyone trying to use some half-baked configuration format? Can we have a good language for general configurations? Spoiler alert: <a href=https://dhall-lang.org/>dhall-lang</a>.</p></li></ul><ul><li><p>Docker</p><p>This is managed by traefik with <a href=https://doc.traefik.io/traefik/providers/docker/>docker provider</a>. All I need to do is add a label to the container. Traefik will automatically pick up the label
and set up the router according to the <code>defaultRule</code>. My rule is to use domainprefix label when applicable, else fall back to container name.</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix>providers <span>=</span> {
  docker <span style=color:#666>=</span> {
    defaultRule <span style=color:#666>=</span> getRule
      <span style=color:#4070a0>&#39;&#39;{{ (or (index .Labels &#34;domainprefix&#34;) .Name) | normalize }}&#39;&#39;</span>;
  };
}
</code></pre></div></li></ul><ul><li><p>Kubernetes</p><p>Just <a href=https://kubernetes.io/docs/concepts/services-networking/ingress/>the usual Kubernetes ingress</a>. I passed k3s kubeconfig to traefik by systemd environment variable <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1897-L1903>here</a>.
Traefik will automatically apply Kubernetes ingress rules.</p></li></ul><h2 id=deployment>Deployment</h2><p>I currently use nix to manage all my personal devices, <a href=https://www.ansible.com/>ansible</a> to manage all the cloud resources. Most services are managed by nix.
When nix becomes too unwieldy, I resort to Kubernetes.
An ideal setup would be using <a href=https://www.terraform.io/>terraform</a> to provision cloud resources, using nix to manage all services including Kubernetes ones.
This is currently not possible for me because firstly, many resources I used does not have terraform provider. Secondly,
nix currently does not support ad hoc variable assignment like terraform and ansible. It is possible to pass variables from the command line, but it is not pleasant to use.
Thirdly, Kubernetes requires a lot of dedication. Currently nix can&rsquo;t manage Kubernetes efficiently.</p><h3 id=nix>Nix</h3><p>Nix is a much more declarative, reliable and reproducible way to build infrastructure. <a href=https://talks.cont.run/the-hitchhiker-s-guide-to-nixos/>Here</a> is a short introduction.
In short, building NixOS profiles is like building docker image.
You build a new container image and run a container with that image as a base. The container image itself is immutable. When you change your code,
you need to build a new image. When you need some new operating system configuration, you build a new NixOS profile and switch to it.
The best thing about NixOS is that nearly every aspect of the OS is tunable by NixOS options. The knobs are formed by the purely functional, lazy language nix.</p><h3 id=docker>Docker</h3><p>I <a href=https://www.breakds.org/post/declarative-docker-in-NixOS/>manage docker containers declaritively with nix</a>. A typical <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1509-L1712>docker container configuration</a> is</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix>mkContainer <span style=color:#4070a0>&#34;wallabag&#34;</span> prefs<span style=color:#666>.</span>ociContainers<span style=color:#666>.</span>enableWallabag {
  dependsOn <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;postgresql&#34;</span> ];
  environment <span style=color:#666>=</span> {
    <span style=color:#4070a0>&#34;SYMFONY__ENV__DOMAIN_NAME&#34;</span> <span style=color:#666>=</span>
      <span style=color:#4070a0>&#34;https://</span><span style=color:#70a0d0;font-style:italic>${</span>prefs<span style=color:#666>.</span>getFullDomainName <span style=color:#4070a0>&#34;wallabag&#34;</span><span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>&#34;</span>;
  };
  traefikForwardingPort <span style=color:#666>=</span> <span style=color:#40a070>8978</span>;
  middlewares <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;authelia&#34;</span> ];
  volumes <span style=color:#666>=</span> [
    <span style=color:#4070a0>&#34;/var/data/wallabag/data:/var/www/wallabag/data&#34;</span>
    <span style=color:#4070a0>&#34;/var/data/wallabag/images:/var/www/wallabag/web/assets/images&#34;</span>
  ];
  environmentFiles <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;/run/secrets/wallabag-env&#34;</span> ];
}
</code></pre></div><p><code>mkContainer</code> is a function to make a new container on. If <code>prefs.ociContainers.enableWallabag</code> is true, nix would make a container named
<code>wallabag</code> which depends on the <code>postgresql</code> container, has the such such volumes and such such environment variables. The environmentFiles is also
read to set up environment variables. The file <code>/run/secrets/wallabag-env</code> is managed by <a href=https://github.com/Mic92/sops-nix>sops-nix</a> and source controlled. I also specified
the middleware <code>authelia</code> for traefik, which means that not everyone is allowed to access this service.</p><h4 id=service-discovery>Service Discovery</h4><p>This is easy. Docker container within the same bridge network can access each other by the container name.</p><h4 id=configmaps-and-secrets>Configmaps and Secrets</h4><p>I use docker command line flag <code>--env</code> and <code>--env-file</code> to pass my configurations as container environment variable.
To mount secrets like Kubernetes, I use docker volume. The secrets are managed by sops-nix, which generate secret files
according to my <code>sops.yaml</code> file.</p><h4 id=init-containers-and-jobs>Init Containers and Jobs</h4><p><a href=https://kubernetes.io/docs/concepts/workloads/pods/init-containers/>Kubernetes init containers</a> are sometimes used to manage pods/services dependencies. For this specific use case, init containers are ugly hacks.
Using systemd to manage container dependency is much more elegant. I need only specify <code>dependsOn</code> in my nix file, e.g. <code>dependsOn = ["postgresql"];</code> above.
I override the <code>ExecStartPost</code> option for systemd units to do initialization job. Kubernetes jobs are just more containers,
while cronjobs are just containers with systemd timer.</p><h4 id=ingress>Ingress</h4><p>See routing.</p><h3 id=ansible>Ansible</h3><p>As much as I love NixOS, I don&rsquo;t use nix for everything. Nix does not work along with some technologies.
I use ansible for two purposes, first setting up cloud resources (like setting up <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/site.yaml#L19-L41>tailscale</a> and <a href=https://github.com/contrun/infra/blob/ac7d148e95d455b2fc64ddfbc8c2c343a19a06f7/site.yaml#L43-L84>envoy</a>), second managing Kubernetes.
Kubernetes is declarative, but using command line to manage Kubernetes is imperative. I use <a href=https://docs.ansible.com/ansible/latest/collections/community/kubernetes/>community.kubernetes</a>.
A pleasant side effect of using ansible to manage Kubernetes is what I did and what I need to do is well-documented.</p><h3 id=kubernetes>Kubernetes</h3><p>My Kubernetes distribution is k3s (provisioned by nix). Each Kubernetes cluster includes exactly one node for the time being.
There are a few edge cases where I can&rsquo;t simply use nix and docker. <a href=https://jupyterhub.readthedocs.io/en/stable/>Jupyterhub</a> and <a href=https://www.eclipse.org/che/>eclipse che</a> are major ones, as they need to manage cluster resources dynamically.
They spawn new containers on user request. This is doable with vanilla docker spawner for jupyter hub. I don&rsquo;t think Che support this natively.
Using Kubernetes is much preferable.</p><h2 id=security>Security</h2><h3 id=authentication-and-authorization>Authentication and Authorization</h3><h4 id=setup>Setup</h4><p>I use <a href=https://github.com/authelia/authelia>authelia</a> for authentication and authorization. I <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L1619>created</a> an <a href=https://doc.traefik.io/traefik/middlewares/forwardauth/>ForwardAuth</a> middleware for traefik, which works like nginx <a href=http://nginx.org/en/docs/http/ngx%5Fhttp%5Fauth%5Frequest%5Fmodule.html>auth_request</a>.
Upon receiving a client request, depending on the routing, traefik may initiate a subrequest to authelia possibly with necessary client crendentials,
if authelia is able to authenticate the user and authorize the request, the client request will be forwarded
to the backend service with some extra headers containing client user information.
There is not such thing as authorization yet. It&rsquo;s only me using my services.</p><h4 id=weakness>Weakness</h4><p>Authelia is not satisfactory in many aspects. First, its policy engine is not flexible enough. Second, it requires a lot of boilerplate in
the configuration, e.g. I need to specify many hard-coded base domain <code>hostname.mydomain.tld</code> instead of hostname. This is not desirable as I have many hostnames,
and currently this configuration is shared by a common sops file.</p><h4 id=strength>Strength</h4><p>What I really like about authelia is its simplicity and easy integration with traefik.</p><h4 id=future>Future</h4><p>I want to use a <a href=https://cloud.google.com/beyondcorp>beyondcorp</a> style <a href=../identity-aware-proxy/>identity-aware proxy</a> with <a href=https://www.openpolicyagent.org/>open policy agent</a> support some other day. The last time I checked <a href=https://www.pomerium.com/>pomerium</a>,
I found envoy was hard to pack and pomerium was too oidc-centric, most of all it did not support ldap or other local user database.</p><h3 id=sso>SSO</h3><p>Authelia just landed <a href=https://github.com/authelia/authelia/issues/189>openid connect support</a>. I haven&rsquo;t tried it yet. One more thing about authelia is that I currently use a single text file as account backend.
I have set up <a href=https://www.openldap.org/>openldap</a> on my machines, but I haven&rsquo;t tried it on authelia yet. I intend to use <a href=https://www.freeipa.org/page/Main%5FPage>freeipa</a> instead, which is much more versatile.</p><h3 id=intrusion-prevention>Intrusion Prevention</h3><p>Because of my distrust to other people&rsquo;s computer, I intentionally made my edge proxy to be as dumb as possible.
There ain&rsquo;t such thing as intrusion detection system yet. Setting up fail2ban is easy, but I need to integrate it with traefik and aioproxy.</p><h2 id=backup>Backup</h2><p>Of all the incremental backup tools, there are two distinctive feature about <a href=https://restic.net/>restic</a>.
First, it supports all <a href=https://rclone.org/>rclone</a> backends, second, I can backup different directories from different hosts to the same endpoint.
Here is my nix configuration.</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-nix data-lang=nix>restic <span>=</span> {
  backups <span style=color:#666>=</span> <span style=color:#007020;font-weight:700>let</span>
    go <span style=color:#666>=</span> name: conf: backend: {
      <span style=color:#4070a0>&#34;</span><span style=color:#70a0d0;font-style:italic>${</span>name<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>-</span><span style=color:#70a0d0;font-style:italic>${</span>backend<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>&#34;</span> <span style=color:#666>=</span> {
	initialize <span style=color:#666>=</span> <span style=color:#60add5>true</span>;
	passwordFile <span style=color:#666>=</span> <span style=color:#4070a0>&#34;/run/secrets/restic-password&#34;</span>;
	repository <span style=color:#666>=</span> <span style=color:#4070a0>&#34;rclone:</span><span style=color:#70a0d0;font-style:italic>${</span>backend<span style=color:#70a0d0;font-style:italic>}</span><span style=color:#4070a0>:restic&#34;</span>;
	rcloneConfigFile <span style=color:#666>=</span> <span style=color:#4070a0>&#34;/run/secrets/rclone-config&#34;</span>;
	timerConfig <span style=color:#666>=</span> {
	  OnCalendar <span style=color:#666>=</span> <span style=color:#4070a0>&#34;00:05&#34;</span>;
	  RandomizedDelaySec <span style=color:#666>=</span> <span style=color:#4070a0>&#34;5h&#34;</span>;
	};
	pruneOpts <span style=color:#666>=</span> [
	  <span style=color:#4070a0>&#34;--keep-daily 7 --keep-weekly 5 --keep-monthly 12 --keep-yearly 75&#34;</span>
	];
      } <span style=color:#666>//</span> conf;
    };
    mkBackup <span style=color:#666>=</span> name: conf:
      go name conf <span style=color:#4070a0>&#34;backup-primary&#34;</span> <span style=color:#666>//</span> go name conf <span style=color:#4070a0>&#34;backup-secondary&#34;</span>;
  <span style=color:#007020;font-weight:700>in</span> mkBackup <span style=color:#4070a0>&#34;vardata&#34;</span> {
    extraBackupArgs <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;--exclude=postgresql&#34;</span> ];
    paths <span style=color:#666>=</span> [ <span style=color:#4070a0>&#34;/var/data&#34;</span> ];
  };
};
</code></pre></div><p>I back up my data everyday to two backend storage.</p><h2 id=proxy>Proxy</h2><p>I use <a href=https://github.com/Dreamacro/clash>clash</a> and iptables for transparent proxy. <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/dot%5Fbin/executable%5Fclash-redir>Here</a> is the script, and <a href=https://github.com/contrun/dotfiles/blob/75d7a0c803f763996f77bfe570c9369b9d32910a/ignored/nix/common.nix#L2013-L2107>here</a> is the systemd unit to run the script and update clash configuration.
The source of truth for my clash configuration lies in cloudflare workers kv. All my machines use the same proxy configuration by periodically downloading a subscription from cloudflare worker.
Although it is straightforward to set up transparent proxy on Linux, There are two complications when I want to proxy docker container traffic transparently.</p><h3 id=transparent-proxy-does-not-work-with-docker-container-in-bridge-network-mode>Transparent proxy does not work with docker container in bridge network mode</h3><p>This is a first world problem. Docker/Kubernetes <a href=https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/#network-plugin-requirements>wants</a> <code>sysctl net.bridge.bridge-nf-call-iptables=1</code>, while libvirt <a href=https://wiki.libvirt.org/page/Net.bridge.bridge-nf-call%5Fand%5Fsysctl.conf>wants</a> <code>sysctl net.bridge.bridge-nf-call-iptables=0</code>.
More explanations can be found <a href=http://ebtables.netfilter.org/misc/brnf-faq.html>here</a>, <a href=https://serverfault.com/questions/963759/docker-breaks-libvirt-bridge-network>here</a> and <a href=https://github.com/kelseyhightower/kubernetes-the-hard-way/issues/561#issue-585446276>here</a>. The following scenery illustrates why docker/Kubernetes insists on enabling <code>bridge-netfilter</code>.</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>docker run -it --rm -p 8081:8081 nicolaka/netshoot socat -v -v -d -d tcp-listen:8081,fork exec:cat

<span style=color:#bb60d5>HOST_IP</span><span style=color:#666>=</span><span style=color:#4070a0>&#34;</span><span style=color:#007020;font-weight:700>$(</span>ip -4 -json addr | jq -r <span style=color:#4070a0>&#39;.[] | .addr_info[] | select(.scope == &#34;global&#34;) | .local&#39;</span> | head -n 1<span style=color:#007020;font-weight:700>)</span><span style=color:#4070a0>&#34;</span>
docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081&#34;</span>
docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081,bind=\$(ip -4 -json addr show dev eth0 | jq -r &#39;.[].addr_info[].local&#39;):8082&#34;</span>
docker run -it --rm -p 8082:8082 nicolaka/netshoot bash -c <span style=color:#4070a0>&#34;echo test | socat - tcp:</span><span style=color:#bb60d5>$HOST_IP</span><span style=color:#4070a0>:8081,bind=127.1.0.1:8082&#34;</span>
</code></pre></div><p>When <code>bridge-netfilter</code> is disabled, the last command would time out, while the other two commands will not.
This kind of hairpinning support is seldom needed on my machine.</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>sysctl net.bridge.bridge-nf-call-iptables<span style=color:#666>=</span><span style=color:#40a070>0</span> net.bridge.bridge-nf-call-ip6tables<span style=color:#666>=</span><span style=color:#40a070>0</span> net.bridge.bridge-nf-call-arptables<span style=color:#666>=</span><span style=color:#40a070>0</span>
</code></pre></div><p>So I disable <code>bridge-netfilter</code>. A further complication is that k3s and docker is so smart as to enable <code>bridge-netfilter</code> on startup.
I <a href=https://github.com/contrun/dotfiles/commit/122bef19579e18fcd9e8ca778a64ec0688b9555f>added</a> a <code>ExecStartPost</code> to disable it.</p><h3 id=transparent-proxy-does-not-work-with-docker-container-when-on-ip-is-missing>Transparent proxy does not work with docker container when on-ip is missing</h3><p>To be more precise, sometimes it does not work. I don&rsquo;t know why. I just banged my head for a few hundreds times and find out <code>--on-ip</code> is a must.</p><div class=highlight><pre style=background-color:#f0f0f0;-moz-tab-size:4;-o-tab-size:4;tab-size:4><code class=language-shell data-lang=shell>iptables -t mangle -A CLASH_EXTERNAL -p tcp -j TPROXY --on-port <span style=color:#40a070>7893</span> --on-ip 127.0.0.1 --tproxy-mark 0x4242/0xffffffff
</code></pre></div><h3 id=alternatives>Alternatives</h3><p>Oh, dear god, iptables is hard. I wish there is an easier way to transparent proxy.</p><ul><li>TUN</li><li>macvlan virtual machine</li></ul><h2 id=next-step>Next Step</h2><h3 id=kubernetes-after-all>Kubernetes after All?</h3><p>I abandoned my plan of using Kubernetes for all. Currently, I refrain my usage of Kubernetes because first I didn&rsquo;t find a satisfactory workflow
for nix and Kubernetes, second I begin to feel Kubernetes is the new c++.
I sincerely hope I can declaratively manange Kubernetes with nix the way I manage docker and traefik with nix.
I find <a href=https://github.com/xtruder/kubenix/issues/26>integrating kustomize and kubenix</a> interesting, but it is not there yet.
Both nix and Kubernetes are too overwhelming. They require you to go all-in. Nix is my daily driver. It is definitely here to stay.
I need some Kubernetes features like node affinity (jupyter hub requires a faster node), proxy traffic received from any node.
As I said, Kubernetes is like c++. It is extremely powerful, but it is also extremely complex and can be easily misused.
I partially agree <a href=https://pythonspeed.com/articles/dont-need-kubernetes/>“Let’s use Kubernetes!” Now you have 8 problems</a>. I find also find <a href=https://github.com/oam-dev/kubevela>kubevela</a> to be interesting. I haven&rsquo;t tried it yet.
I hope it live up to what its promise. Also, <a href=https://mrkaran.dev/posts/home-server-nomad/>Nomad</a> looks interesting, it may well suits che and jupyter hub.</p><h3 id=configuration-database>Configuration Database</h3><p>Nix is great. But it is hard for outside world to learn my nix configuration.</p><h3 id=security-hardening>Security Hardening</h3><h3 id=observablility>Observablility</h3><h3 id=federated-storage>Federated Storage</h3><h3 id=grand-unification>Grand Unification</h3></div></div></div></div><script src=https://wiki.cont.run/js/URI.js type=text/javascript></script><script src=https://wiki.cont.run/js/page.js type=text/javascript></script><script id=MathJax-script async src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script></body></html>